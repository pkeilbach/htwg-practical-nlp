{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Practical Natural Language Processing! \ud83d\udc4b","text":"<p>Welcome to the course Practical Natural Language Processing at HTWG Konstanz.</p> <p>I'm excited to have you here and I hope you will enjoy the course.</p>"},{"location":"#quickstart","title":"Quickstart \ud83d\ude80","text":"<p>If you have Python and Git installed on your system, you can get started right away.</p> <p>After you forked or cloned the repository, execute one of the following commands:</p> <pre><code># regular setup\nmake\n\n# development setup\nmake install-dev\n</code></pre> <p>For more details, check out the getting started guide.</p>"},{"location":"#preparation","title":"Preparation \ud83d\udcda","text":"<p>There are already a few things you can do before our course officially starts:</p> <ul> <li> <p>Enrol in the Moodle course: \ud83d\ude4b\u200d\u2642\ufe0f</p> <p>We use the HTWG Moodlefor internal announcements or anything that is related to the current semester. This is to make sure that nobody misses anything. Also it will give me an estimation of how many students plan to take the course.</p> </li> <li> <p>Check the course profile and prerequisities: \ud83d\udccb</p> <p>Check out the course profile to see if the description matches your expactations.</p> <p>If you decide to take the course, check if you need to catch up on the topics mentioned in the course [prerequisites].</p> </li> <li> <p>Create online accounts: \ud83d\udcbb</p> <p>To get the most out of this course, you should have a GitHub and Mural account. Both services are free to use.</p> <p>Tip</p> <p>You can use your HTWG email address to register for GitHub and Mural. This will make it easier to identify you as a member of this course.</p> </li> <li> <p>Set up your development environment for the course: \ud83d\udee0\ufe0f</p> <p>Go through the getting started guide to set up your development environment for the course.</p> </li> <li> <p>Read through the contribution guide: \ud83d\udc50</p> <p>You can help to improve this course with your contributions and earn bonus points for the exam. The contributing guide contains all information on that.</p> </li> <li> <p>Get started with the assignments: \ud83e\udd13</p> <p>If you already want to get hands-on, you can go ahead and start looking into the assignments. Assignments 0 and 1 are great starting points. This also gives you a better feeling of what is expected on the practical side in this course.</p> </li> </ul>"},{"location":"assignments/","title":"Assignments","text":"<p>The course materials are accompanied by assignments to deepen your understanding of the topics.</p> <p>The assignments are voluntary. But completing the assignment earns you bonus points for the exam!</p> <p>If you have basic programming skills, you should be able to complete the assignments.</p>"},{"location":"assignments/#structure","title":"Structure","text":"<p>Throughout the course, we will work on a Python package called <code>htwgnlp</code>. The package is located in the <code>src</code> directory and is a fully functional and installable Python package. The core sturcture will be provided, and the assignments will be about implementing the missing functionality.</p> <p>To work on an assignment, you will need to locate the <code>TODO ASSIGNMENT-*</code> items in the code.</p> <p>Example</p> <p>For example, to work on the first assignment, use the search functionality of your IDE to find all relevant items:</p> <pre><code>TODO ASSIGNMENT-1\n</code></pre> <p>Tip</p> <p>You should check the unit tests located in the <code>tests</code> directory to see the exact requirements that need to be implemented.</p>"},{"location":"assignments/#testing-your-implementation","title":"Testing your Implementation","text":"<p>Once you implemented everything, you can run the tests to check if everything works as expected.</p> <p>You can run the tests using the provided <code>make assignment-*</code> commands. If all your tests pass, you successfully completed the assignment! \ud83d\ude80</p> <p>Example</p> <p>For example, to test your implementation for the first assignment, you can run:</p> <pre><code>make assignment-1\n</code></pre> <p>Tip</p> <p>If your IDE provides the functionality, you can also run the tests directly from the IDE.</p> <p>Note</p> <p>You can also use the native <code>pytest</code> commands, but then you need to know the exact path to the tests:</p> <pre><code># make sure to have the virtual environment activated\npytest tests/htwgnlp/test_preprocessing.py\n</code></pre> <p>Pytest is a very powerful testing framework and the de-facto standard for testing in Python. You will not need to know all the details, but if you want to learn more, check out the official Pytest documentation.</p>"},{"location":"assignments/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Some of the assignments are accompanied by Jupyter notebooks.</p> <p>If you successfully complete an assignment, you can run the accompanying notebook and see your code in action.</p> <p>See the Getting Started guide for instructions on how to start the Jupyter lab server.</p>"},{"location":"assignments/#submission","title":"Submission","text":"<p>To submit an assignment, you will need to provide screenshots of a successful test run.</p> <p></p> <p>Please submit your screenshots via email to me.</p> <p>When you submit your assignments, please provide a short description of your major learnings or challenges for each assignment.</p> <p>Info</p> <p>Depending on your preference, you can submit all assignments at once, or submit them separately.</p>"},{"location":"assignments/#working-on-your-assignments","title":"Working on your Assignments","text":"<p>How you work on your assignments on your local machine is completely up to you. But following a consistent branching strategy will probably make you life easier.</p> <p>Info</p> <p>This is regardless of whether you use the recommended approach to fork the repository, or just clone it locally.</p> <p>Note</p> <p>Feel free to skip this section if you have basic familiarity with git and branching workflows.</p> <p>Tip</p> <p>If you are new to git or not very experienced, this is a great learing opportunity for you, as git is a very important tool in nowadays software development. You can work on the assignments without getting too much into git, but this is a great way to gain some experience with git.</p>"},{"location":"assignments/#using-a-single-branch","title":"Using a single branch","text":"<p>Generally, it is recommended to work on your assignments in a separte branch:</p> <pre><code>git checkout -b my-assigments\n</code></pre> <p>Then, you can work on your assignments and commit your changes locally:</p> <pre><code>git add .\ngit commit -m \"solution for assignment 1\"\n</code></pre> <p>Tip</p> <p>If you work on a fork, you can also push your changes to your remote repository. This is not possible if you just clone the course repository.</p>"},{"location":"assignments/#using-multiple-branches","title":"Using multiple branches","text":"<p>Similarly, you could create a new branch for each assignment. This can help you to keep your work separated.</p> <pre><code>git checkout -b my-assigments-1\ngit add .\ngit commit -m \"solution for assignment 1\"\n</code></pre>"},{"location":"assignments/#using-your-local-working-tree","title":"Using your local working tree","text":"<p>If you don't want to deal with git, you could also work purely locally without committing anything.</p> <p>Warning</p> <p>This is not recommended, as this bears the risk of losing your progress, or dealing with lots of merge conflicts. When pulling updates, you probably need to stash your changes. But be careful: if not done properly, you may lose your progress! \ud83d\ude31</p>"},{"location":"assignments/#fetching-updates","title":"Fetching Updates","text":"<p>As described in the getting started guide, there will be updates from time to time.</p> <p>It can happen that these updates affect the assignments - just in case you are wondering why your tests suddenly fail \ud83d\ude05).</p> <p>So make sure to always fetch the latest updates before working on your assignments.</p>"},{"location":"assignments/#bonus-points-for-assignments","title":"Bonus Points for Assignments","text":"<p>Each completed assignment earns you one bonus points for the exam. \ud83c\udfc5</p> <p>Furthermore, you can earn an extra two bonus points if you manage to package and publish your code to Test PyPI.</p> <p>Tip</p> <p>Note that the assignment code is organized as a fully functional Python package which you can push to a package index and install it via <code>pip</code>.</p> <p>Find more details on how to package and publish your code here.</p> <p>Acceptance Criteria</p> <p>As soon as I can install your package with the following command, you are eligible for the two bonus points:</p> <pre><code>pip install --index-url https://test.pypi.org/simple/ --no-deps htwgnlp_YOUR_GITHUB_USERNAME\n</code></pre> <p>Please let me know via email when you are ready for me to test your package!</p>"},{"location":"course_profile/","title":"Course Profile","text":""},{"location":"course_profile/#description","title":"Description","text":"<p>This course offers a comprehensive introduction to Natural Language Processing (NLP), focusing on both basic methods and modern techniques. Students will explore essential topics such as preprocessing, feature extraction, and classic algorithms like Logistic Regression and Naive Bayes, used for text classification. Core concepts in language modeling and vector space models will be covered, including Minimum Edit Distance for text similarity.</p> <p>Later in the course, students will delve into advanced NLP approaches, including word embeddings and sequence models, such as recurrent neural networks (RNNs) and their improvements with attention mechanisms. The course balances theory and practical application, preparing students to build and understand NLP systems in real-world contexts. Towards the end of the course, we will also touch on large language models, prompt engineering, and generative AI.</p>"},{"location":"course_profile/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ul> <li>Understand the fundamentals of Natural Language Processing (NLP) and its key applications.</li> <li>Apply various preprocessing techniques and feature extraction methods to text data.</li> <li>Analyze and implement basic models for text classification, such as Logistic Regression and Naive Bayes.</li> <li>Understand and utilize advanced NLP techniques, including word embeddings and sequence models.</li> <li>Explore the concepts behind large language models, prompt engineering, and generative AI.</li> </ul>"},{"location":"course_profile/#assessment","title":"Assessment","text":"<p>The course is graded based on a written 90-minute exam at the end of the semester. To be admitted to the exam, you need to complete the assignmnets and give a presentation. You can earn bonus points for the exam through contributions to the course repository.</p> <ul> <li>Exam: graded</li> <li>Presentation: ungraded, but mandatory to be admitted to the exam</li> <li>Assignments: ungraded, but eligible for bonus points</li> <li>Contributions: ungraded, but eligible for bonus points</li> </ul> <p>You can earn a maximum of 10 bonus points throughout the semester by completing assignments and making contributions.</p>"},{"location":"course_profile/#course-language","title":"Course Language","text":"<p>All course materials are provided in English. Lectures will be delivered in German, but can also be conducted in English if we have international students.</p>"},{"location":"course_profile/#course-format","title":"Course Format","text":"<p>This course will be delivered in a hybrid format, consisting of both in-person and online lectures. Note that in-person lectures will not be streamed, and online lectures will not be recorded.</p>"},{"location":"course_profile/#prerequisites","title":"Prerequisites","text":"<p>The following skills are recommended to participate in the course. Let me know if you have any doubts or questions regarding those prerequisites. My goal is to keep the entry barrier as low as possible!</p> <ul> <li> <p>Basic Programming Skills</p> <p>To complete the course, you will need basic programming skills. If you visited an introduction to programming course, you should be good to go.</p> <p>Info</p> <p>I tried to design the coding exercises in a way that students with little programming experience can solve them. If you know your basics about object oriented programming, you will be well equipped. Any advanced concpets will be explained during the lecture. In the end, we don't want to bother with advanced programming concepts but get excited with NLP! So don't worry if you just started your programming journey, you are still encouraged to take the course!</p> </li> <li> <p>Basic Python Skills</p> <p>The code for this lecture is written in Python, so it is definetely an advantage if you have worked with Python before. However, if you are coming from a different language, you should be able to follow along. I tried to keep the language specific parts to a minimum and will provide explanations where necessary.</p> <p>Tutorial</p> <p>Microsoft provides a nice beginner course that you can take to get up to speed with Python.</p> </li> <li> <p>Knowledge of the Linux Command Line</p> <p>Since the course is designed for a Linux development environment, it is recommended to have some basic skills with the Linux command line, i.e. the bash shell.</p> <p>However, all required commands will be provided in the instructions, so it is not necessary to have extensive Linux command line skills.</p> <p>Tutorial</p> <p>Here is a basic bash tutorial which may help if you are new to Bash and the Linux command line.</p> <p>Info</p> <p>We use <code>make</code> commands to automate the setup process and simplify the commands. You can check out our <code>Makefile</code> to see what is actucally executed. If you have not used a Makefile before, check out this guide.</p> </li> <li> <p>Basic Knowledge of Git</p> <p>To participate in the course you need basic Git knowledge, like cloning a repository, commit and push changes, or pull updates.</p> <p>Since the repository is hosted in GitHub, it is an advantage to be familiar with processes like forking or pull requests.</p> <p>Tutorial</p> <p>If you have not worked a lot with Git before, please check out this Git tutorial</p> <p>If you are new to GitHub (a popular Git hosting service), you might want to check out this module.</p> </li> </ul> <p>Tip</p> <p>In general, Microsoft Learn offers some great tutorials for all kinds of technologies.</p>"},{"location":"course_profile/#literature","title":"Literature","text":"<p>Here is a list of recommended literature for this course:</p> <ul> <li> <p>Alammar, Jay, and Maarten Grootendorst. Hands-On Large Language Models. Sebastopol, CA: O'Reilly Media, 2024. https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/.</p> </li> <li> <p>Bird, Steven, Ewan Klein, and Edward Loper. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. Sebastopol, CA: O'Reilly Media, 2009. https://www.nltk.org/book_1ed/.</p> </li> <li> <p>Bishop, Christopher M. Pattern Recognition and Machine Learning. New York: Springer, 2006. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.</p> </li> <li> <p>Jurafsky, Daniel, and James H. Martin. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 2nd ed. Upper Saddle River, NJ: Prentice Hall, 2008. https://web.stanford.edu/~jurafsky/slp3/.</p> </li> <li> <p>Kamath, U., Keenan, K., Somers, G., and Sorenson, S. Large Language Models: A Deep Dive: Bridging Theory and Practice. 2024. Springer. https://link.springer.com/book/10.1007/978-3-031-65647-7.</p> </li> <li> <p>Raschka, Sebastian. Build a Large Language Model (From Scratch). Shelter Island, NY: Manning, 2024. https://www.manning.com/books/build-a-large-language-model-from-scratch.</p> </li> <li> <p>Vajjala, Sowmya, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana. Practical Natural Language Processing: A Comprehensive Guide to Building Real-world NLP Systems. Sebastopol, CA: O'Reilly Media, 2020. https://www.practicalnlp.ai/.</p> </li> </ul>"},{"location":"exam/","title":"Exam","text":""},{"location":"exam/#basic-information","title":"Basic Information","text":"<p>Here is some basic information about the exam:</p> <ul> <li>The exam will contain a section about each lecture topic.</li> <li>Each lecture will be represented by approximately 5 questions, corresponding to approximately 10 points per lecture.</li> <li>The questions are a mix of multiple choice, true/false, and open questions.</li> <li>Open questions will require only short answers (1-2 sentences).</li> <li>You won't need to write any code in the exam, but you should be able to understand and explain code snippets.</li> <li>You won't need to memorize any formulas. If you need to calculate something, the formula will be provided.</li> <li>You are allowed to use a standard calculator during the exam.</li> <li>You won't need to memorize parts of the script by heart, but you should be able to explain the concepts in your own words.</li> <li>During preparation, try to focus on understanding the key concepts.</li> <li>The examples throughout the script will be helpful for understanding the concepts.</li> </ul>"},{"location":"exam/#grade-scale","title":"Grade Scale","text":"<p>The maximium number of points you can reach in the exam is 100.</p> <p>To calculate your grade points, the bonus points you earned throughout the semester will be added to your exam points:</p> \\[ \\text{Grade Points} = \\text{Exam Points} + \\text{Bonus Points} \\] Grade Points Grade \\(\\geq 95\\) \\(1.0\\) \\(\\geq 90\\) \\(1.3\\) \\(\\geq 85\\) \\(1.7\\) \\(\\geq 80\\) \\(2.0\\) \\(\\geq 75\\) \\(2.3\\) \\(\\geq 70\\) \\(2.7\\) \\(\\geq 65\\) \\(3.0\\) \\(\\geq 60\\) \\(3.3\\) \\(\\geq 55\\) \\(3.7\\) \\(\\geq 50\\) \\(4.0\\) <p>Example</p> <p>Assuming you reach 83 points in the exam, and earned 7 bonus points throughout the semester, your grade points would be 90.</p> <p>This would result in a grade of 1.3 instead of 2.0. \ud83e\udd73</p> <p>Note</p> <p>If you exceed 100 points with your bonus points, the grade points will be capped at 100.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#miscellaneous","title":"Miscellaneous","text":""},{"location":"faq/#is-this-an-online-course","title":"Is this an online course?","text":"<p>No, this is a hybrid course. There will be a few in-person lectures in Konstanz. It will be announced individually which lectures will be held in-person and which will be held online.</p>"},{"location":"faq/#is-it-mandatory-to-attend-the-lectures","title":"Is it mandatory to attend the lectures?","text":"<p>No. Attending the lectures is voluntary - but of course highly recommended and encouraged! If you are more comfortable with self-study, feel free to go through the course materials at your own pace.</p> <p>Note that if you go this way, it is even more important to actively engage in GitHub discussions and exchange with your fellow students.</p> <p>As of now, the only requirement to be admitted to the exam is to give a presentation.</p>"},{"location":"faq/#what-communication-channels-do-we-use","title":"What communication channels do we use?","text":"<p>Everything that is related to the course content should be discussed via GitHub discussions. See the contributing guide for details.</p> <p>Everything that is specific to the current semester will be announced through Moodle. E.g. room changes or organizational matters. This is to make sure that nobody misses anything.</p> <p>For private matters, you can always send me an email or ask me in-person.</p>"},{"location":"faq/#how-do-i-get-access-to-the-course-resources","title":"How do I get access to the course resources?","text":"<p>The course materials are available open source here: https://pkeilbach.github.io/htwg-practical-nlp/</p> <p>To make contributions, you will need a GitHub account.</p> <p>To participate in the kick-off lecture, it would be great f you open a Mural account.</p>"},{"location":"faq/#how-do-i-get-access-to-the-murals","title":"How do I get access to the murals?","text":"<p>In order to access the murals, you will need to create an account.</p> <p>The link to the murals will shared through the Moodle course or during the lecture.</p>"},{"location":"faq/#is-there-a-pdf-version-of-the-script","title":"Is there a PDF version of the script?","text":"<p>The content for this course was created using the MkDocs framework. As the course content is subject to continuous change, there is no PDF version of the script provided. But if you wish, you can convert any page to a PDF file by using the printing function of your browser. MkDocs will render a nice-looking PDF file without any menu or other web items.</p>"},{"location":"faq/#what-are-the-prerequisites-to-take-this-course","title":"What are the prerequisites to take this course?","text":"<p>The prerequisites can be found here.</p> <p>In general, I want to keep the entry barrier as low as possible, but you should bring some basic computer science skills.</p> <p>Let me know if you are unsure if you are unsure whether you can take the course or not!</p>"},{"location":"faq/#can-i-take-the-course-if-i-just-started-programming","title":"Can I take the course if I just started programming?","text":"<p>I would say yes, and definitely encourage you to go for it even if you just started your programming journey! \ud83d\udcaa</p> <p>My goal is to design the course materials in a way that also students with little programming experience can participate. I will try my best to make it clear whenever we have a more advanced or language specific concept that goes beyond basic object oriented programming.</p> <p>However, it is certainly easier for you to follow along and complete the assignments if you bring basic computer science and software engineering skills, like Linux, Git, Python.</p> <p>If you are new to programming, your learning curve is probably steeper, as you may have to deal with some technologies of the ecosystem that may be new to you. But you should keep in mind that this is probably related to a certain technology or framework, and not the concepts of NLP.</p> <p>Please check out the course prerequisites and let me know if you have any doubts.</p>"},{"location":"faq/#assessment","title":"Assessment","text":""},{"location":"faq/#how-is-this-course-graded","title":"How is this course graded?","text":"<p>The course is graded based on a written 90-minute exam at the end of the semester. For more details, see the course profile.</p> <p>The presentation and assignments are ungraded.</p>"},{"location":"faq/#what-are-bonus-points","title":"What are bonus points?","text":"<p>Throughout the semester you can earn bonus points that will be added to your exam score.</p>"},{"location":"faq/#how-can-i-earn-bonus-points","title":"How can I earn bonus points?","text":"<p>You can earn bonus points for the exam through contributions and by completing assignments.</p>"},{"location":"faq/#how-many-bonus-points-can-i-earn","title":"How many bonus points can I earn?","text":"<p>You can earn a maximum of 10 bonus points throughout the semester.</p>"},{"location":"faq/#contributions","title":"Contributions","text":""},{"location":"faq/#how-many-contributions-can-i-make","title":"How many contributions can I make?","text":"<p>There is no limit on that. You can make multiple contributions throughout the semester.</p>"},{"location":"faq/#how-many-bonus-points-do-i-get-for-a-contribution","title":"How many bonus points do I get for a contribution?","text":"<p>Each contribution will be rewarded with bonus points, depending on the size and the scope of the contribution. The value of the contribution will be defined by the instructor.</p>"},{"location":"faq/#what-kind-of-contributions-can-be-rewarded","title":"What kind of contributions can be rewarded?","text":"<p>A rewardable contribution is a code change that is successfully merged to the main branch of the course repository.</p> <p>While participating in discussions is appreciated and encouraged, this a not a rewardable contribution.</p> <p>Find more details in the contributing guide.</p>"},{"location":"faq/#when-can-i-start-with-contributions","title":"When can I start with contributions?","text":"<p>Unless announced otherwise, please wait with your contributions until we officially kick off the semester in the first lecture. There, we will go through the policy and details so that everybody is on the same page. This is to give everybody a fair chance to contribute, and understand the processes and policies for contributions and earning bonus points.</p>"},{"location":"faq/#assignments","title":"Assignments","text":""},{"location":"faq/#when-is-an-assignment-considered-completed","title":"When is an assignment considered completed?","text":"<p>An assignment is considered completed when all of its tests pass.</p>"},{"location":"faq/#how-many-assignmnents-do-i-need-to-complete","title":"How many assignmnents do I need to complete?","text":"<p>All assignments are voluntary. But you can earn bonus points for the exam by completing assignments.</p>"},{"location":"faq/#how-are-the-assignments-graded","title":"How are the assignments graded?","text":"<p>The assignments are ungraded and voluntary.</p>"},{"location":"faq/#can-we-get-the-sample-solutions-for-the-assignments","title":"Can we get the sample solutions for the assignments?","text":"<p>I will show my sample solutions during the lecture but they will not be published or shared.</p>"},{"location":"faq/#presentations","title":"Presentations","text":""},{"location":"faq/#how-is-the-presentation-graded","title":"How is the presentation graded?","text":"<p>The presentation is ungraded but it is mandatory to give a presentation to be admitted to the exam.</p>"},{"location":"faq/#do-i-need-to-submit-any-slides-of-my-presentation","title":"Do I need to submit any slides of my presentation?","text":"<p>No, you don't need to submit any slides.</p>"},{"location":"faq/#what-do-i-need-to-submit","title":"What do I need to submit?","text":"<p>You need to submit the practical part of your presentation as a GitHub repository. For more details, check the presentation submission section.</p>"},{"location":"getting_started/","title":"Development Setup","text":"<p>This page describes how to set up your development environment for this course.</p>"},{"location":"getting_started/#install-python","title":"Install Python","text":"<p>The recommended Python version for this course is 3.12. in a virtual environment.</p>  Linux Mac Windows <pre><code>sudo apt update\nsudo apt install python3.12\nsudo apt install python3.12-venv\n</code></pre> <p>In case this doesn't work, try to add the deadsnakes PPA to your system, and try again.</p> <pre><code>sudo add-apt-repository ppa:deadsnakes/ppa\n</code></pre> <p>On Mac, you can use Homebrew to install Python.</p> <pre><code>brew install python@3.12\n</code></pre> <p>On Windows, it is recommended to use the Windows Subsystem for Linux (WSL). Then you can follow the instructions for Linux.</p> <p>If you are a VS Code user, you need to install the WSL extension.</p> <p>There is currently no setup guide for native Windows, but I'm happy to accept a pull request for this issue. \ud83d\ude09</p> <p>Warning</p> <p>You are free to use another Python version if you wish, but be aware that this may cause problems with the provided code. Also if you are using Python outside a virtual environment or with a distribution like Anaconda, the described setup may not work.</p>"},{"location":"getting_started/#forking-the-repository-optional","title":"Forking the Repository (Optional)","text":"<p>You can decide between forking the course repository, or just clone it:</p> <ul> <li>When forking the repository, you act as a conributor to the course repository. You will go through the full development setup and can use your remote repository to manage your work. Also, you can contribute back changes to earn bonus points for the exam. \ud83c\udfc5</li> <li>When cloning the repository, you act as a user of the course repository. While this is a bit more leightweight, you work fully locally and cannot contribute to the course repository. \ud83d\ude41</li> </ul> <p>Tip</p> <p>Forking is a very common practice in open source developlment. If you are new to open source development and have not forked a repository before, this may be a good learning opportunity for you! \ud83e\udd13</p> <p>If you decide to clone the repository, you can directly continue with the next step.</p> <p>If you decide to fork the repository, you can follow the official GitHub documentation on how to fork a repository.</p> <p>Info</p> <p>If you fork a repository, a copy of the repository will be created in your personal GitHub user space.</p>"},{"location":"getting_started/#clone-the-repository","title":"Clone the Repository","text":"<p>Make sure you have Git installed on your system.</p> <p>Cloning the repository is straightforward, no matter if you work on a fork or not. You only need to watch out where you clone from:</p> <pre><code># When cloning your fork, make sure to clone it from your personal GitHub user space\ngit clone https://github.com/&lt;your-username&gt;/htwg-practical-nlp.git\n\n# cloning the course repository directly\ngit clone https://github.com/pkeilbach/htwg-practical-nlp.git\n</code></pre>"},{"location":"getting_started/#execute-the-setup-script","title":"Execute the Setup Script","text":"<p>The setup script is provided as a <code>Makefile</code>. Change into the repository directory and execute the setup script. This should create a virtual environment and install all required dependencies.</p> <pre><code># go to the project directory\ncd htwg-practical-nlp\n\n# use plain make to install all required dependencies\nmake\n\n# if you plan to contribute, you need to install the dev dependencies\nmake install-dev\n</code></pre> <p>This may take a few minutes. \u2615</p> <p>If everything went well, you should be good to go.</p> <p>Acticate the virtual environment</p> <p>From now, make sure that you have the virtual environment activated. Usually, the IDE should automatically suggest you to activate it (e.g. VSCode). If that is not the case, you can activate the virtual environment with the following command</p> <pre><code># activate the virtual environment manually\nsource .venv/bin/activate\n\n# in case you need to deactivate it\ndeactivate\n</code></pre>"},{"location":"getting_started/#test-your-installation","title":"Test your Installation","text":"<p>You can test your installation by running the tests for the first assignment.</p> <pre><code>make assignment-1\n</code></pre> <p>In your terminal, you should see lots of failed tests. \ud83d\ude28</p> <p>But this is exactly what we want to see, since we haven't implemented anything yet! \ud83e\udd13</p> <p>Info</p> <p>You can find more details on how we handle assignments in the assignments guide.</p> <p>If you came this far, your initial setup was successful and you are ready to go! \ud83d\ude80</p> <p>Now we can take a look at some other components of the repository.</p>"},{"location":"getting_started/#jupyter","title":"Jupyter","text":"<p>Some of the assignments are accompanied by Jupyter notebooks.</p> <p>If your IDE supports it, you can execute the Jupyter notebooks natively in your IDE (e.g. using the VSCode Jupyter extension).</p> <p>If you prefer the web UI, you can start the Jupyter lab server with the following command.</p> <pre><code>make jupyter\n</code></pre> <p>Jupyter is now accessible at http://localhost:8888/.</p>"},{"location":"getting_started/#serve-the-lecture-notes","title":"Serve the Lecture Notes","text":"<p>If you want, you can bring up the lecture notes on your local machine.</p> <pre><code>make mkdocs\n</code></pre> <p>The lecture notes are now accessible at http://localhost:8000/.</p>"},{"location":"getting_started/#fetching-updates","title":"Fetching Updates","text":"<p>During the semester, it is very likely that the course repository will be updated.</p> <p>You can incorporate the updates as follows:</p> <pre><code># if you work on a fork, you need to fetch your updates from the course repository (aka 'upstream')\ngit fetch upstream\ngit checkout main\ngit merge upstream/main\n\n# make sure the correct upstream repository is set:\ngit remote -v\n# if not, add the upstream:\ngit remote add upstream https://github.com/pkeilbach/htwg-practical-nlp.git\n\n# otherwise (if you have cloned the repository), you need to fetch from origin\ngit fetch origin\ngit checkout main\ngit merge origin/main\n</code></pre> <p>Pull updates regularly</p> <p>It is good practice to pull the latest changes from <code>main</code> every now and then (just in case you are wondering why your assignment tests suddenly fail \ud83d\ude05). However, important updates will be announced in the lecture.</p> <p>Note</p> <p>Find more details about syncing a fork in the official GitHub docs.</p>"},{"location":"presentations/","title":"Presentations","text":"<p>During the semester, each student should give a presentation about an NLP-related topic.</p> <p>The presentation is ungraded, but it is mandatory to give a presentation in order to be admitted to the exam.</p>"},{"location":"presentations/#topic","title":"Topic","text":"<p>The presentation can be about a tool, framework, technology, or just anything that you are interested in, as long as it fits into the broader context of NLP.</p> <p>The idea is that you can explore an exciting NLP topic, hack around with some new framework, and share your findings with the class.</p> <p>You can find the list of topics in the GitHub presentation topic pool issue.</p> <p>If you are interested in a topic, please add a comment to the issue, stating the topic you would like to work on.</p> <p>We will discuss the topic assignments in one of the first lectures.</p> <p>Tip</p> <p>You can also suggest your own topic! Just add a comment to the issue with your idea, and we can discuss it.</p> <p>Note</p> <p>If multiple students are interested in the same topic, we will figure out a way to assign the topics fairly. In case of doubt, a first-come-first-serve approach may be applied, so it's probably a good idea to be quick! \ud83d\ude09</p>"},{"location":"presentations/#scope","title":"Scope","text":"<p>The scope of the presentation should not exceed 10 minutes. Ideally, you should include a demo or practical example (but I am aware that probably not all topics are equally suitable for that).</p> <p>For the practical part, try to aim for 25-50% of the time (depending on the topic). Feel free to use any tool you want. You can show something in the browser, use Postman or Jupyter notebooks, or do some live hacking on the command line. \ud83e\udd13</p> <p>Note</p> <p>If you work on a paper, there may be fewer practiical parts as if you work on a tool or framework. That's totally fine!</p> <p>Besides that, there are no strict rules about the presentation format. You can use slides if you want, but it is not mandatory.</p>"},{"location":"presentations/#presentation-dates","title":"Presentation Dates","text":"<p>We will discuss the dates of the presentations in one of the first lectures.</p>"},{"location":"presentations/#submission","title":"Submission","text":"<p>You don't need to submit the presentation slides.</p> <p>For the practical part, please push your code to a GitHub repository and provide a README with instructions on how to run your code.</p> <p>Ideally, your code should be directly executable with a setup that is self-explanatory and follows common conventions (e.g. <code>npm start</code>).</p> <p>Example</p> <p>Here are some examples of what I mean:</p> <ul> <li>If your practical part is a Jupyter Notebook, you could provide a mybinder link.</li> <li>If you build a web service, after I cloned the repository, the service should come up by running <code>make</code> or <code>npm install &amp;&amp; npm start</code>.</li> <li>If you provide a Python script, it is acceptable to do a <code>pip install</code> followed by a <code>python my_script.py</code>, since this is conventional in Python.</li> </ul> <p>Info</p> <p>In case of doubt, feel free to ask in our discussion board or reach out to me directly.</p>"},{"location":"lectures/attention_models/","title":"Attention Models","text":"<p>This lecture will introduce an imprtant concept of state-of-the-art NLP models: attention. Specifically, we will look how the transformer architecture uses attention to process sequences of words.</p>"},{"location":"lectures/attention_models/#attention","title":"Attention","text":"<p>Attention is a mechanism that allows models to focus on specific parts of input sequences when making predictions or generating output sequences. It mimics the human ability to selectively concentrate on relevant information while processing input data. This allows the model to understand and generate more contextually relevant and accurate text.</p> <p>Example</p> <p>Imagine you're reading a long article or a book, and there's a specific word or phrase that is crucial for understanding the overall meaning. Your attention naturally focuses on that important part to comprehend the context better. Attention models work somewhat similarly.</p> <p>In NLP, when a computer is trying to understand or generate text, attention models help to decide which parts of the input text are more important at any given moment. Instead of treating all words equally, an attention model allows the model to focus more on specific words that are relevant to the task at hand.</p> <p>The general attention mechanism makes use of three main components, namely Query (Q), Key (K), and Value (V). They are used to capture the relationships between different words in a sequence, and thus, allow the model to focus on different parts of the input sequence when generating or understanding output.</p> <ul> <li> <p>Key (K): The Key vector is like a unique identifier or key associated with each word in the input sequence. It captures information about the word that is relevant for determining its relationship with other words.</p> </li> <li> <p>Query (Q): The Query vectors represent the word for which we are trying to compute attention weights. The model uses the Query vectors to compare against the Key vectors of all other words in the sequence.</p> </li> <li> <p>Value (V): The Value vector is used to calculate the weighted sum, and it represents the content or meaning of the word. The attention weights determine how much importance should be given to each word's content when computing the final attention representation.</p> </li> </ul> <p>Attention is the mechanism that allows the model to find the best matching keys for a given query, and return the corresponding values.</p> <p>The attention mechanism computes the attention scores by measuring the similarity between the query and keys: the higher the attention score, the more focus the model places on the corresponding values associated with those keys.</p> <p>Here is a simplified visualization of the attention mechanism:</p> <p></p> <p>The input for the attention layer are the Query, Key, and Value vectors. The output is the attention vector, which is the weighted sum of the Value vectors.</p> <p>Example</p> <p>Imagine encoding the following sentence :</p> <p>Vincent van Gogh is a painter, known for his stunning and emotionally expressive artworks.</p> <p>When encoding the query <code>van Gogh</code>, the output may be <code>Vincent van Gogh</code> as the key, with <code>painter</code> as the associated value.</p> <p>The model stores keys and values in a table, which it can then use for future decoding:</p> Key Value Vincent van Gogh painter William Shakespeare playwright Charles Dickens writer <p>Whenever a new sentence is presented, e.g.</p> <p>Shakespeare's work has influenced many movies, mostly thanks to his work as a ...</p> <p>The model can complete the sentence by taking <code>Shakespeare</code> as the query and finding it in the table of keys and values:</p> <p><code>Shakespeare</code> as query is closest to <code>William Shakespeare</code> the key, and thus the associated value <code>playwright</code> is presented as the output.</p> <p>Analogy from Retrieval Systems</p> <p>I found this analogy from retrieval systems quite helpful to understand the intuition behind the Query, Key, Value concept<sup>1</sup>:</p> <p>The Key/Query/Value concept can be seen analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query  (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).</p> <p>In attention models, the Query vector is used to retrieve the most relevant Key vectors, and the Value vectors are used to compute the final attention representation.</p> <p>Suppose we have a sequence of words \\(X = \\{x_1, x_2, ..., x_n\\}\\). Depending on the NLP task (e.g. translation or summarization), we might want to assign different weights to each word based on its relevance to the current task.</p> <ol> <li> <p>Key, Query, and Value:</p> <ul> <li>Each word in the sequence has associated Key \\(K\\), Query \\(Q\\), and Value \\(V\\) vectors.</li> <li>For each word \\(x_i\\), we have corresponding \\(k_i\\), \\(q_i\\), and \\(v_i\\) vectors.</li> </ul> </li> <li> <p>Score Calculation:</p> <ul> <li>The attention score \\(e_{ij}\\) between a query vector \\(q_i\\) and a key vector \\(k_j\\) is calculated using a function, most commonly the dot product:</li> </ul> \\[ e_{ij} = q_i \\cdot k_j \\] </li> <li> <p>Softmax:</p> <ul> <li>The scores are passed through a softmax function to get normalized attention weights:</li> </ul> \\[ a_{ij} = \\frac{e^{e_{ij}}}{\\sum_{k=1}^{n} e^{e_{ik}}} \\] <ul> <li>These weights \\(a_{ij}\\) represent the importance of each word \\(x_j\\) with respect to the query word \\(x_i\\).</li> <li>In other words, the softmax function's output includes which keys are closest to the query.</li> </ul> <p>Softmax</p> <p>The softmax function is a generalization of the logistic function to multiple dimensions. It is used to normalize a vector of arbitrary real values to a probability distribution, that is, the sum of all values is 1.</p> <p>The softmax function is defined as follows:</p> \\[ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} \\] <p>The softmax function is often used in classification problems to convert a vector of real values to a probability distribution over predicted output classes.</p> </li> <li> <p>Weighted Sum:</p> <ul> <li>The final attention vector \\(A_i\\) for a word \\(x_i\\) is the weighted sum of the Value vectors \\(v_j\\) across all words, based on the attention weights:</li> </ul> \\[ A_i = \\sum_{j=1}^{n} a_{ij} \\cdot v_j \\] </li> </ol> <p>When we write the equations above in matrix form, we get the following:</p> \\[ Attention(Q, K, V) = \\text{softmax}(QK^T)V \\] <p>Where:</p> <ul> <li>\\(Q\\) is the matrix of Query vectors.</li> <li>\\(K\\) is the matrix of Key vectors.</li> <li>\\(V\\) is the matrix of Value vectors.</li> <li>\\(d_k\\) is the dimensionality of the Key vectors.</li> </ul> <p>In summary, attention allows the model to dynamically focus on different parts of the input sequence. It is a layer of calculations that let the model focus on the most important parts of the sequence for each step.</p> <p>The attention scores are obtained by measuring the similarity between Query and Key vectors, and the softmax function ensures that the weights are normalized (i.e. sum to 1).</p> <p>The weighted sum of the Value vectors, using these attention weights, gives the final attention vector for a specific word in the sequence.</p> <p>Combining all attention vectors for all words in the sequence gives the final attention matrix.</p> <p>This mechanism allows the model to selectively attend to relevant information during processing.</p> <p>Attention models are used in many NLP tasks, such as machine translation, text summarization, and question answering.</p> <p>Machine Translation</p> <p>The attention mechanism is particularly useful for machine translation as the most relevant words for the output often occur at similar positions in the input sequence.</p> <p>Here is a visualization of the attention matrix of a translation model<sup>2</sup>:</p> <p></p> <p>The x-axis represents the source sentence and the y-axis represents the target sentence. We can see which positions in the source sentence were considered more important when generating the target word.</p> <p>The model correctly translates a phrase <code>European Economic Area</code> into <code>zone economique europ\u00e9en</code>. It was able to correctly align <code>zone</code> with <code>Area</code>, jumping over the two words <code>European</code> and <code>Economic</code>, and then looked one word back at a time to complete the whole phrase <code>zone economique europ\u00e9en</code>.</p> <p>Tip</p> <p>Here is a great interactive visualization of the attention mechanism (make sure to scroll down a bit).</p> <p>Here is the general attention mechanism implemented with SciPy and NumPy<sup>3</sup></p> <pre><code>from numpy as np\nfrom scipy.special import softmax\n\n# encoder representations of four different words\nword_1 = np.array([1, 0, 0])\nword_2 = np.array([0, 1, 0])\nword_3 = np.array([1, 1, 0])\nword_4 = np.array([0, 0, 1])\n\n# stacking the word embeddings into a single array\nwords = np.array([word_1, word_2, word_3, word_4])\n\n# generating the weight matrices\nnp.random.seed(42)\nW_Q = np.random.randint(3, size=(3, 3))\nW_K = np.random.randint(3, size=(3, 3))\nW_V = np.random.randint(3, size=(3, 3))\n\n# generating the queries, keys and values\nQ = words @ W_Q\nK = words @ W_K\nV = words @ W_V\n\n# scoring the query vectors against all key vectors\nscores = Q @ K.transpose()\n\n# computing the weights by a softmax operation\nweights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n\n# computing the attention by a weighted sum of the value vectors\nattention = weights @ V\n\nprint(attention)\n</code></pre> <p>This would output the following:</p> <pre><code>[[0.98522025 1.74174051 0.75652026]\n [0.90965265 1.40965265 0.5       ]\n [0.99851226 1.75849334 0.75998108]\n [0.99560386 1.90407309 0.90846923]]\n</code></pre>"},{"location":"lectures/attention_models/#transformers","title":"Transformers","text":"<p>Now the next step is to port the attention mechanism into a neural network architecture. This is where transformers come into play.</p> <p>Transformers make use of the attention mechanism to process sequences of words.</p> <p>We could also say transformers is a type of neural network architecture that implements the attention mechanism.</p> <p>They are a type of neural network architecture that is used for a wide range of natural language processing tasks, such as machine translation, text summarization, and question answering.</p> <p>The main advantages of Transformers over RNNs are:</p> <ul> <li>\u274c RNNs are slow to train because they process words sequentially and cannot be parallelized.</li> <li>\u274c RNNs cannot capture long-range dependencies effectively.</li> <li>\u274c RNNs are computationally expensive.</li> <li>\u2705 Transformers can capture long-range dependencies more effectively using attention.</li> <li>\u2705 Transformers are parallelizable</li> <li>\u2705 Transformers are faster to train and require less computation.</li> </ul> <p>Since the position of a word and the order of words in a sentence are important to understand the meaning of a text, while still being able to process text in parallel, Transformers use positional encoding to keep track of the sequence.</p> <p>To be more specific, transformers use a variant of the attention mechanism, called multi-head attention, which allows them to capture more complex patterns and relationships between words in the sequence.</p> <p>We will look at both of these concepts in more detail below.</p> <p>Attention Is All You Need</p> <p>The transformer architecture was introduced in the paper \"Attention Is All You Need\" by Vaswani, et al. from 2017.</p> <p>It was a breakthrough in the field of natural language processing, as it outperformed previous state-of-the-art models on a wide range of tasks, including machine translation, text summarization, and question answering<sup>4</sup>:</p> <p>This figure is taken from Google's blog post, accompanying the original paper:</p> <p></p> <p>BLUE Score</p> <p>The BLEU score is a metric for evaluating the quality of machine translation. It is based on the precision of the translation, and it measures how many words in the machine translation match the reference translation. The higher the BLEU score, the better the translation.</p> <p>Large Language Models</p> <p>Large language models (LLMs) are essentially a type of transformer model that are trained on large amounts of text data. The GPT models by OpenAI are examples of large language models.</p>"},{"location":"lectures/attention_models/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Attention mechanisms enable models to focus on different parts of the input sequence when processing each output token, allowing them to capture long-range dependencies effectively.</p> <p>Multi-head attention extends this concept by computing attention from multiple linearly projected \"heads\" in parallel, which are then concatenated and projected again to obtain the final output.</p> <p>Multi-head attention involves the following steps:</p> <ol> <li> <p>Splitting: Multi-head attention involves splitting the query (Q), key (K), and value (V) vectors into multiple smaller vectors, or \"heads.\"</p> </li> <li> <p>Independent Attention Calculation: Attention scores are calculated independently for each head, allowing the model to focus on different parts of the input sequence simultaneously.</p> </li> <li> <p>Weighted Summation: Using the attention scores, a weighted sum of the value vectors is computed for each head.</p> </li> <li> <p>Concatenation and Projection: The results from all heads are concatenated and linearly projected again to obtain the final output of the multi-head attention layer.</p> </li> </ol> <p></p> <p>Not only is this more computationally efficient, but it also enables the model to capture more complex patterns and relationships between words in the sequence. This is because  it enables the model to process and combine multiple aspects of the input data simultaneously, leading to richer and more nuanced representations.</p> <p>Self Attention</p> <p>Traditional attention focuses on different parts of the input sequence (e.g., English sentence) to generate each word in the output sequence (e.g., translated French sentence).</p> <p>Self attention, on the other hand, captures dependencies and relationships within the same sequence (e.g., an English sentence) by allowing each element to attend to all other elements, including itself.</p> <p>It is computed on the fly, meaning that the query, key, and value vectors are all derived from the same input sequence.</p> <p>Multi-head attention can be seen as an extension of self attention, where the input is split into multiple heads, and attention is computed independently for each head.</p> <p>In the case where <code>it</code> refers to the animal, we can observe a different attention than when <code>it</code> refers to the street:</p> <p></p>"},{"location":"lectures/attention_models/#positional-encoding","title":"Positional Encoding","text":"<p>Transformers do not inherently understand the sequential order of the input tokens.</p> <p>This brings huge computational benefits, because if the order doesn't matter, we can process the text in parallel.</p> <p>But the position of a word and the order of words in a sentence are important to understand the meaning of a text.</p> <p>Therefore transformers must still keep track of the order of words in a sentence, and must somehow inject the positional information of the words in the sequence into the model, namely into the input embeddings.</p> <p>This is done by adding positional encoding to the input embeddings.</p> <p>Example</p> <p>In a simple version of positional encoding, we could encode the position of a word in a sentence by using a single number to represent its position in the sequence.</p> <p>Given the sentence:</p> <p>The work of William Shakespeare inspired many movies.</p> <p>We could encode the position of each word in the sentence as follows:</p> Word Index The 0 work 1 of 2 William 3 Shakespeare 4 inspired 5 many 6 movies 7 <p>Mathematically, positional encoding in transformers means adding the positional vectors to the input embeddings (they have the same shape). By doing so, the encoded text includes information about the both the meaning and position of a word in a sentence.</p> <p>Info</p> <p>Word Embeddings + Positional Encoding = Input Embeddings</p> <p>For Transformers, the encoding mechanism is a bit more complex than in the example above. It uses a sine and cosine function to encode the position of a word in a sentence. This allows the model to learn relative positions between words, which is important for understanding the meaning of a sentence.</p>"},{"location":"lectures/attention_models/#encoder-decoder-architecture","title":"Encoder-Decoder Architecture","text":"<p>There are two main components in the transformer architecture:</p> <ol> <li> <p>Encoder: The encoder takes the input sequence and generates a representation of it (i.e. a vector) that captures the meaning of the sequence. This representation is then passed to the decoder.</p> </li> <li> <p>Decoder: The decoder takes the representation generated by the encoder and uses it to generate the output sequence, one word at a time.</p> </li> </ol> <p>The encoder and decoder are both composed of multiple layers, each of which contains a multi-head attention layer and a feed-forward neural network. The encoder and decoder layers are stacked on top of each other, with each layer passing its output to the next layer in the stack.</p> <p></p> <p>The connection between the encoder and decoder is established through the attention mechanism, which allows the decoder to selectively attend to different parts of the input sequence based on their importance for generating the next token in the output sequence.</p> <p>This enables the model to effectively capture dependencies between input and output sequences, making it well-suited for tasks such as machine translation, where the input and output sequences may have complex and non-linear relationships.</p> <p>Tip</p> <p>The original paper shows the transformer architecture in more detail.</p> <p>Info</p> <p>Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sentence word by word while consulting the representation generated by the encoder.</p> <p>The animation below shows the encoder-decoder architecture of the transformer model in action. It is from the transformer blog post by Google:</p> <ul> <li>The Transformer starts by generating initial representations, or embeddings, for each word. These are represented by the unfilled circles.</li> <li>Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations.</li> <li>The decoder operates similarly, but generates one word at a time, from left to right. It attends not only to the other previously generated words, but also to the final representations generated by the encoder.</li> </ul> <p></p>"},{"location":"lectures/attention_models/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Attention models address the limitations of sequence models like RNNs by enabling the model to selectively weigh and focus on specific parts of the input sequence when making predictions or generating output sequences.</li> <li>Attention mimics the human ability to selectively concentrate on relevant information while processing input data.</li> <li>Mathematically, attention is a weighted sum of the values, calculated from Query (Q) and Key (K) and Value (V) vectors.</li> <li>A model architecture that uses attention is called a Transformer. It is a type of neural network architecture that is used for a wide range of natural language processing tasks, such as machine translation, text summarization, and question answering.</li> <li>The transformer architecture was introduced in the paper \"Attention Is All You Need\" by Vaswani, et al. from 2017 and set new standards in natural language processing.</li> <li>Transformers can be parallelized because attention does not require sequential processing of words, making them much faster to train than RNNs. However, to process text in parallel, they still need to maintain the order of words in a sentence, which is achieved by incorporating positional encoding into the input embeddings.</li> <li>Multi-head attention is a variant of the attention mechanism that allows the model to capture more complex patterns and relationships between words in the sequence.</li> <li>The transformer model consists of an encoder and a decoder that are connected through the attention mechanism.</li> </ul> <p>Further Reading</p> <p>The transformer is a sophisticated architecture and attention is a complex concept. If you want to dig deeper into those topics, here are some resources that may be interesting for you:</p> <ul> <li>Attention and Augmented Recurrent Neural Networks</li> <li>The Illustrated Transformer</li> <li>The Illustrated GPT-2 (Visualizing Transformer Language Models)</li> <li>https://jalammar.github.io/how-gpt3-works-visualizations-animations/</li> <li>The Annotated Transformer</li> <li>Microsoft Learn: Understand the transformer architecture used for NLP</li> <li>The Transformer Family</li> </ul> <ol> <li> <p>https://stats.stackexchange.com/a/424127 \u21a9</p> </li> <li> <p>Neural Machine Translation by Jointly Learning to Align and Translate \u21a9</p> </li> <li> <p>https://machinelearningmastery.com/the-attention-mechanism-from-scratch/ \u21a9</p> </li> <li> <p>https://blog.research.google/2017/08/transformer-novel-neural-network.html \u21a9</p> </li> </ol>"},{"location":"lectures/feature_extraction/","title":"Feature Extraction","text":""},{"location":"lectures/feature_extraction/#vocabulary","title":"Vocabulary","text":"<p>The first step to represent a text as a vector is to build a vocabulary.</p> <p>In NLP, a vocabulary is a set of unique words in a corpus.</p> <p>The vocabulary is build after the preprocessing step.</p> \\[ V = \\{w_1, w_2, \\ldots, w_n\\} \\] <pre><code>&gt;&gt;&gt; corpus = [\n... \"This is the first document\",\n... \"This document is the second document\",\n... \"And this is the third one\",\n... \"Here is yet another document\"\n... ]\n&gt;&gt;&gt; words = [word.lower() for document in corpus for word in document.split()]\n&gt;&gt;&gt; vocabulary = set(words)\n&gt;&gt;&gt; vocabulary\n{\"document\", \"this\", \"here\", \"one\", \"is\", \"yet\", \"another\", \"third\", \"second\", \"and\", \"the\", \"first\"}\n</code></pre>"},{"location":"lectures/feature_extraction/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>In a one-hot encoded vector, each word in the vocabulary \\(V\\) is assigned a unique index. Each element in the vector is binary, indicating the presence (1) or absence (0) of the corresponding word in the document.</p> <p>The dimension of a feature vector \\(x\\) is equal to the size of the vocabulary \\(|V|\\):</p> \\[ dim(x) = |V| \\] <p>Here is a coding example:</p> <pre><code>&gt;&gt;&gt; vocabulary = {\"document\", \"this\", \"here\", \"one\", \"is\", \"yet\", \"another\", \"third\", \"second\", \"and\", \"the\", \"first\"}\n&gt;&gt;&gt; document = [\"this\", \"document\", \"is\", \"the\", \"second\", \"cool\", \"document\"]\n&gt;&gt;&gt; np.array([1 if word in document else 0 for word in vocabulary])\narray([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0])\n</code></pre> <p>Sparse Representation</p> <p>This type of representation of a vector is also known as sparse representation. In sparse representation, only a small number of elements in the representation have non-zero values, while the rest are explicitly or implicitly considered zero.</p> <p>Limitations</p> <p>One-hot encoding does not capture the frequency, or more complex properties like grammar or word order. It only indicates whether a word is present or not.</p>"},{"location":"lectures/feature_extraction/#bag-of-words","title":"Bag of Words","text":"<p>Bag of Words (BoW) is a similar representation to one-hot encoding. The difference is that BoW captures the frequency of words in a document, which is a more informative representation than one-hot encoding.</p> <pre><code>&gt;&gt;&gt; vocabulary = {\"document\", \"this\", \"here\", \"one\", \"is\", \"yet\", \"another\", \"third\", \"second\", \"and\", \"the\", \"first\"}\n&gt;&gt;&gt; document = [\"this\", \"document\", \"is\", \"the\", \"second\", \"cool\", \"document\"]\n&gt;&gt;&gt; np.array([document.count(word) for word in vocabulary])\narray([2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0])\n</code></pre> <p>Info</p> <p>In BoW, the values in the vector can be integers representing word counts or real numbers representing TF-IDF weights.</p> <p>Dimensionality</p> <p>With sparse representation, because of \\(dim(x) = |V|\\), as the vocabulary size increases, the number of features increases as well. For the model, this means we have to fit more parameters, which negatively affects the model's performance.</p>"},{"location":"lectures/feature_extraction/#positive-and-negative-frequencies","title":"Positive and Negative Frequencies","text":"<p>To avoid the problem of dimensionality, we can simplify the feature vector by using only the frequencies per class.</p> <p>Let's assume we have a binary classification problem with two classes: positive and negative.</p> <p>When using only the positive and negative frequencies as features, we will only have three features:</p> <ul> <li>The bias unit</li> <li>The sum of positive frequencies of the words in the document</li> <li>The sum of negative frequencies of the words in the document</li> </ul> <p>Compared to a sparse representation, this will significantly reduce the number of features and thus, improve the model's performance, because we have fewer parameters to fit.</p> <p>Bias Unit</p> <p>The bias unit (also known as the intercept or offset) in logistic regression serves to shift the decision boundary away from the origin (0,0) in the feature space.</p> <p>Without the bias term, the decision boundary would always pass through the origin, but this is barely the case in real-world data.</p> <p>While the weights associated with the features determine the slope of the decision boundary, the bias term influences where the decision boundary is positioned on the y-axis.</p> <p>Note</p> <p>Note that this simplification also means that we lose some information, since we are not considering the frequency of each word in the document, but only the sum of positive and negative frequencies.</p> <p>Here is an example corpus and its corresponding labels:</p> <pre><code>corpus = [\n    \"I am happy\",\n    \"I am sad\",\n    \"I am happy because I love the weather\",\n    \"I am sad because I hate the weather\",\n]\n\nlabels = [1, 0, 1, 0]\n</code></pre> <p>We want to build a data structure that shows us the word frequencies per class.</p> <ul> <li>To get the positive frequency of any word in the vocabulary, we have to count how often it appears in the positive class.</li> <li>To get the negative frequency of any word in the vocabulary, we have to count how often it appears in the negative class.</li> </ul> <p>Given the corpus and the labels above, we can build the following table:</p> \\(V\\) \\(n_{pos}\\) \\(n_{neg}\\) I 3 3 am 2 2 happy 2 0 sad 0 2 because 1 1 love 1 0 hate 0 1 the 1 1 weather 1 1 <p>We can observe that some words take clear sides, like <code>happy</code> and <code>sad</code>, while others are neutral, like <code>am</code> and <code>the</code>.</p> <p>Example</p> <ul> <li>The word <code>happy</code> appears twice in the positive class and zero times in the negative class.</li> <li>The word <code>sad</code> appears zero times in the positive class and twice in the negative class.</li> <li>The word <code>because</code> appears once in the positive class and once in the negative class.</li> <li>The word <code>I</code> appears three times in the positive class and three times in the negative class.</li> </ul> <p>Info</p> <p>In practice, there are multiple ways to implement such a table.</p> <ul> <li>Pandas DataFrame</li> <li>Python dictionary</li> <li>Numpy array</li> <li>...</li> </ul> <p>Based on this table, we can build the feature vector for a document \\(i\\), by summing up the positive and negative frequencies of each word in the document.</p> <p>Considering the bias unit as the first feature, the feature vector \\(x_i\\) looks like this:</p> \\[ x_i = [1, \\sum_{j=1}^{m} n_{pos}(w_j), \\sum_{j=1}^{m} n\\_{neg}(w_j)] \\] <p>Example</p> <p>Given the table of frequencies above, let's consider the following document:</p> \\[ \\text{I am happy because the sun is shining} \\] <p>To get the feature vector, we sum up the frequencies of every word in the document per class:</p> <ul> <li>\\(n_{pos} = 3 + 2 + 2 + 1 + 1 = 9\\)</li> <li>\\(n_{neg} = 3 + 2 + 0 + 1 + 1 = 7\\)</li> </ul> <p>Considering the bias unit as the first feature, the feature vector \\(x_i\\) for this document is:</p> \\[ x_i = [1, 9, 7] \\] <p>Question</p> <p>How would you treat the words that are not in the vocabulary?</p>"},{"location":"lectures/further_reading/","title":"Where to go from here?","text":"<p>This a collection of resources that may be interesting for you if you want to dive deeper into NLP, LLMs and related topics.</p>"},{"location":"lectures/further_reading/#deeplearningai","title":"DeepLearning.AI","text":"<p>DeepLearning.AI offers a variety of courses on deep learning. The courses are taught by Andrew Ng, one of the most influential researchers in the field of machine learning and an Adjunct Professor at Stanford University\u2019s Computer Science Department. The courses are available on Coursera and deeplearning.ai.</p> <p>Their specializations are a great way to deep dive into a specific topic and consist of one or more courses. They require a time commitment of 2-4 months and cost around 40-80\u20ac per month. Here are some of the specializations that may be interesting for you:</p> <ul> <li>Natural Language Processing</li> <li>Generative AI with LLMs</li> </ul> <p>They also offer some easily digestable short courses on specific topics. Here are some that some especially interesting as a follow-up to this course:</p> <ul> <li>ChatGPT Prompt Engineering for Developers</li> <li>Building Systems with the ChatGPT API</li> <li>LangChain: Chat with Your Data</li> <li>LangChain for LLM Application Development</li> <li>Finetuning Large Language Models</li> <li>LLMOps</li> <li>Automated Testing for LLMOps</li> </ul>"},{"location":"lectures/further_reading/#microsoft","title":"Microsoft","text":"<p>Microsoft offers a variety of courses on any technology related topic. They also offer some courses on AI and LLMs, mostly related to their own products (e.g. Azure). The courses are available on Microsoft Learn. Here are some of the courses that may be interesting for you:</p> <ul> <li>Coursera: Natural Language Processing in Microsoft Azure</li> <li>Understand the Transformer architecture and explore large language models in Azure Machine Learning</li> <li>Get started with prompt flow to develop Large Language Model (LLM) apps</li> </ul>"},{"location":"lectures/further_reading/#google","title":"Google","text":"<p>Also Google has great learning resources available on Google Cloud. Their Generative AI for Developers Learning Path sounds especially interesting.</p>"},{"location":"lectures/further_reading/#podcasts","title":"Podcasts","text":"<ul> <li>Lex Fridman's Podcast is a podcast about AI, deep learning, science, and technology. He interviews some of the most interesting people in the field of AI and machine learning, like Sam Altman, Andrew Ng, or Yann LeCun.</li> <li>In his episode with Dario Amodei (CEO of Anthropic, the company that created Claude, one of the best AI systems in the world) they talk about scaling, safety, regulation, and a lot of technical details about the present and future of AI and humanity.</li> </ul>"},{"location":"lectures/further_reading/#related-fields-of-study","title":"Related fields of study","text":"<ul> <li> <p>Search and Information Retrieval: Search is a very interesting field of study that is closely related to NLP. It is about finding relevant information in a large corpus of documents. The Introduction to Information Retrieval book written by Stanford University professors is a great resource to learn more about this topic.</p> </li> <li> <p>Healthcare: NLP is also used in healthcare to extract information from medical records, like diagnoses, symptoms, or medications. This information can be used to improve patient care, reduce costs, or predict diseases. The Natural Language Processing in Healthcare course on Coursera is a great resource to learn more about this topic, or more generally, the AI for Medicine Specialization by DeepLearning.AI.</p> </li> <li> <p>Recommendation Systems: Recommendation systems are used to recommend products, movies, or music to users. They are based on the idea that users with similar preferences will also like similar products. The Recommender Systems Specialization by the University of Minnesota is a great resource to learn more about this topic.</p> </li> <li> <p>Legal: NLP is also used in the legal domain to extract information from legal documents, like contracts, or to predict the outcome of a case. Here is a short article about this topic.</p> </li> </ul>"},{"location":"lectures/further_reading/#talks-and-conferences","title":"Talks and Conferences","text":"<ul> <li>TED Talk: How AI could empower any business by Andrew Ng</li> </ul> <p>...to be continued.</p>"},{"location":"lectures/introduction/","title":"Introduction to Natural Language Processing","text":"<p>The first lecture will give you a basic understanding of what Natural Language Processing (NLP) is about, the problems we deal with and the challenges that come with it. The goal is to get a good intuition of what it means to work with language from a programmatic perspective.</p> <p>Natural Language Processing</p> <p>Natural Language Processing (NLP) is a branch of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. By combining computational linguistics with machine learning and deep learning, NLP allows machines to process and analyze vast amounts of natural language data. This capability is fundamental for applications such as speech recognition, text analysis, and language translation.<sup>1</sup></p>"},{"location":"lectures/introduction/#nlp-in-our-everyday-lives","title":"NLP in our everyday lives","text":"<p>In our daily digital lives, we interact with a lot of apps and tools that are based on NLP technologies. The following figure gives an overview of real-world NLP applications along with popular examples<sup>2</sup>.</p> <p></p> <ul> <li>Voice-based assistants like Alexa or Siri utilize a variety of NLP techniques to interact with its users, i.e. they need to understand user commands and questions, and reply accordingly using Text-to-Speech (TTS).</li> <li>Modern search engines like Google or Bing make heavy use of NLP for a variety of subtasks, such as understanding the user query, autocompletion, or ranking and grouping the query results.</li> <li>Social media platforms make use of NLP, e.g. to analyse user comments, or make suggestions to their users.   Also platforms like Youtube or conference tools are able to automatically create transcriptions on the fly using \"Speech-to-text\" (SST) technologies.</li> <li>In popular email applications like Gmail or Outlook, NLP is used for tasks like spam filtering, calendar event extraction or smart replies.</li> <li>Text processing apps offer auto-correction or auto-completion features to their users, i.e. spell-checking or grammar correction.   Grammarly is a nice example here, as well as any smartphone keyboard.</li> <li>More and more companies offer chatbots (aka conversational agents) on their websites for their clients.   With ChatGPT, conversational agents (and NLP in general) reached a new level just recently.</li> </ul> <p>Question</p> <p>Based on your working experience, where have you already encountered, or can think of, potential NLP applications?</p>"},{"location":"lectures/introduction/#a-brief-history-about-nlp","title":"A brief history about NLP","text":"<p>IMHO, every lecture should provide a little historical background in the beginning<sup>3</sup>: \ud83d\udcdc</p> <ul> <li> <p>1950s\u20131990s   Initial attempts are made to map hard rules around languages and follow logical steps to accomplish tasks like translating a sentence from one language to another.   While this works sometimes, strictly defined rules only work for concrete, well-defined tasks that the system has knowledge about.</p> <p>Noam Chomsky</p> <p>Renowned linguist and cognitive scientist Noam Chomsky introduced the theory of generative grammar, which revolutionized the understanding of language structure and acquisition. His work laid the theoretical groundwork for studying language processing and understanding the innate structures that govern language use. Chomsky's theories on syntax and the nature of language have influenced computational linguistics and the development of algorithms for natural language processing, informing how machines understand and generate human language. His critiques of AI's ability to fully understand language have also sparked important discussions on the limitations of current language models.</p> </li> <li> <p>1990s   Language models begin evolving into statistical models and language patterns start being analyzed, but larger-scale projects are limited by computing power.</p> </li> <li> <p>2000s   Advancements in machine learning increase the complexity of language models, and the wide adoption of the internet sees an enormous increase in available training data.</p> <p>Geoffrey Hinton</p> <p>Often referred to as one of the \"Godfathers of Deep Learning,\" Geoffrey Hinton has made significant contributions to the field of neural networks and deep learning, which are foundational to modern language models. His work on backpropagation, neural networks, and the development of the dropout technique has greatly influenced the training of deep learning models. Hinton's research laid the groundwork for the advancement of architectures like BERT and GPT, significantly impacting NLP and machine learning as a whole.</p> <p>Together with John Hopfield, he was awarded the 2024 Nobel Prize in Physics.</p> </li> <li> <p>2012   Advancements in deep learning architectures and larger data sets lead to the development of GPT (Generative Pre-trained Transformer).</p> </li> <li> <p>2018   Google introduces BERT (Bidirectional Encoder Representations from Transformers), which is a big leap in architecture and paves the way for future large language models.</p> </li> <li> <p>2020   OpenAI releases GPT-3, which becomes the largest model at 175B parameters and sets a new performance benchmark for language-related tasks.</p> <p>Ilya Sutskever</p> <p>Co-founder and Chief Scientist at OpenAI, Ilya Sutskever has played a pivotal role in the development of groundbreaking language models, including GPT-2 and GPT-3. His work in deep learning and reinforcement learning has advanced the understanding and capabilities of generative models. Sutskever's contributions to the architecture and training of these models have led to significant improvements in their performance on a wide range of language-related tasks. His leadership at OpenAI has also driven the broader adoption and awareness of large language models and their potential applications.</p> </li> <li> <p>2022   ChatGPT is launched, which turns GPT-3 and similar models into a service that is widely accessible to users through a web interface and kicks off a huge increase in public awareness of LLMs and generative AI.</p> </li> <li> <p>2023   Open source LLMs begin showing increasingly impressive results with releases such as Dolly 2.0, LLaMA, Alpaca, and Vicuna. GPT-4 is also released, setting a new benchmark for both parameter size and performance.</p> </li> </ul>"},{"location":"lectures/introduction/#tasks-that-can-be-solved-by-nlp","title":"Tasks that can be solved by NLP","text":"<p>NLP is used for a wide variety of language-related tasks, including answering questions, classifying text in a variety of ways, and conversing with users.</p> <p>If you design an NLP system, you may make use of multiple tasks.</p> <p>Here are some tasks that can be solved by NLP<sup>4</sup>:</p>"},{"location":"lectures/introduction/#sentiment-analysis","title":"Sentiment analysis","text":"<ul> <li>Sentiment analysis is a text classification problem and it classifies the emotional intent of text.</li> <li>Input is a text, and the output is the probability of the sentiment being positive, negative, or neutral.</li> <li>Features used may include hand-generated features, word n-grams, TF-IDF, or deep learning models for capturing text dependencies.</li> <li>Common applications include classifying customer reviews and identifying signs of mental illness in online comments.</li> </ul> <p>Example</p> <p>Here is a fictional example of customer reviews of a hotel booking service:</p> <ul> <li>\ud83d\ude03 Great service for an affordable price. We will definitely be booking again!</li> <li>\ud83d\ude10 Just booked two nights at this hotel.</li> <li>\ud83d\ude21 Horrible service. The room was dirty and unpleasant. Not worth the money.</li> </ul> <p>Email spam detection is another typical example of a binary text classification problem in NLP.</p> <p>Toxicity classification</p> <p>Toxicity classification, a branch of sentiment analysis, categorizes hostile content like threats, insults, obscenities, and hate speech. It's used to moderate online conversations by detecting and silencing offensive comments or scanning for defamation.</p> <p>Text classification with Naive Bayes</p> <p>A very good starting algorithm for text classification tasks in Naive Bayes algorithm. This is primarily because it is relatively simple to understand and implement, and very fast to train and run. We will cover the Naive Bayes algorithm later in the course, as well as in one of the assignments.</p>"},{"location":"lectures/introduction/#machine-translation","title":"Machine translation","text":"<ul> <li>Machine translation automates the process of translating text between different languages.</li> <li>The input is text in a source language, and the output is text in a target language.</li> <li>Machine translation improves communication on platforms like Facebook and Skype.</li> <li>Effective models can distinguish between words with similar meanings.</li> <li>Some systems also identify the language of the input text.</li> </ul> <p>Example</p> <ul> <li>DeepL is a well-known example of this application.</li> </ul>"},{"location":"lectures/introduction/#named-entity-recognition","title":"Named entity recognition","text":"<ul> <li>NER extracts entities from text and classifies them into predefined categories (e.g., names, persons, organizations, locations, quantities, book title, etc.).</li> <li>The input is text, and the output includes the identified entities and their positions in the text.</li> <li>NER is valuable for applications like summarizing news articles and fighting disinformation.</li> </ul> <p>Example</p> <p>The spaCy library is an open-source Python library for advanced Natural Language Processing that provides efficient tools for tasks such as tokenization, POS tagging, NER, and dependency parsing.</p> <p>Here is an example of NER with spaCy</p> <pre><code>import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n</code></pre> Text Start End Label Description Apple 0 5 ORG Companies, agencies, institutions. U.K. 27 31 GPE Geopolitical entity, i.e. countries, cities, states. $1 billion 44 54 MONEY Monetary values, including unit. <p>It can be visualized as follows:</p> <p></p>"},{"location":"lectures/introduction/#part-of-speech-pos-tagging","title":"Part-of-speech (POS) tagging","text":"<ul> <li>In NLP, the anatomy of a sentence is commonly described using POS tags, which involves assigning grammatical categories (e.g., noun, verb, adjective) to each word in the sentence.</li> <li>The input is a sequence of words, and the output is a tagged sequence indicating the POS for each word.</li> <li>POS tagging is essential for various NLP tasks, such as syntactic parsing, machine translation, and information retrieval.</li> </ul> <p>POS-tagging with the Python spaCy library</p> <p>Here is an example of POS-tagging with spaCy</p> <pre><code>import spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\ndisplacy.serve(doc, style=\"dep\")\n</code></pre> <p>Output:</p> <p></p> <p>POS-tagsets</p> <p>A tagset is a predefined list of grammatical categories used in Natural Language Processing to classify words during tasks like POS tagging.</p> <p>A popular tagging convention is Penn Treebank tagset which is also used by default by the Python Natural Language Toolkit library.</p> <p>A simplified universal tagset by Google Research maps the Penn Treebank tagset to only 12 tags.</p>"},{"location":"lectures/introduction/#topic-modelling","title":"Topic modelling","text":"<ul> <li>Topic modeling is an unsupervised text mining task that identifies abstract topics in a collection of documents.</li> <li>The input is a corpus of documents, and the output is a list of topics, with each topic consisting of relevant words and proportions assigned to documents.</li> <li>Topic modeling has commercial applications, such as helping lawyers find evidence in legal documents.</li> </ul> <p>Latent Dirichlet Allocation</p> <p>Latent Dirichlet Allocation is a popular technique that views documents as mixtures of topics, and topics as mixtures of words.</p>"},{"location":"lectures/introduction/#text-generation","title":"Text generation","text":"<ul> <li>Text generation (aka natural language generation) produces text that mimics human-written content.</li> <li>Models can be fine-tuned to generate text in various formats, like tweets, blogs, and computer code.</li> <li>Common applications include autocomplete and chatbots.</li> </ul> <p>Autocomplete</p> <p>Autocomplete predicts what word comes next, and autocomplete systems of varying complexity are used in chat applications like WhatsApp. Google uses autocomplete to predict search queries. One of the most famous models for autocomplete is GPT-2, which has been used to write articles, song lyrics, and much more.</p> <p>Grammatical error correction</p> <p>Grammatical error correction models use grammatical rules to fix text. This is typically approached as a sequence-to-sequence task, where the model learns to transform an ungrammatical sentence into a correct one. Tools like Grammarly and Microsoft Word implement these models to enhance writing, while schools use them to grade student essays.</p>"},{"location":"lectures/introduction/#information-retrieval","title":"Information Retrieval","text":"<ul> <li>Information retrieval aims to find the most relevant documents for a given query.</li> <li>It is essential in search and recommendation systems and focuses on retrieving a relevant set of documents, not directly answering the query.</li> <li>These systems handle massive collections of documents (millions), retrieving the most relevant ones.</li> <li>Two main processes involved are indexing (typically done using vector space models and Two-Tower Networks) and matching (based on similarity or distance scores).</li> <li>Advanced systems, such as Google's, use multimodal information retrieval, working across text, image, and video data.</li> </ul> <p>Elasticsearch</p> <p>A popular example for an information retrieval system is Elasticsearch. It is an open-source search and analytics engine built on top of Apache Lucene. It is designed for fast, scalable search capabilities and is widely used for a variety of applications, including full-text search, structured data retrieval, and real-time analytics.</p>"},{"location":"lectures/introduction/#summarization","title":"Summarization","text":"<ul> <li>Summarization aims to shorten text while emphasizing the most relevant information.</li> <li>Some summarizers are able to evaluate factual consistency for accurate output.</li> <li>Extractive summarization: Selects and combines the most important sentences from the input text.</li> <li>Abstractive summarization: Creates a summary by paraphrasing the input text, resulting in an output that may contain new words and sentences not present in the original. This is modeled as a sequence-to-sequence task.</li> </ul>"},{"location":"lectures/introduction/#question-answering","title":"Question Answering","text":"<ul> <li>Question answering involves providing answers to human-posed questions in natural language.</li> <li>Notable example: Watson won the game show Jeopardy against human champions in 2011.</li> <li>Multiple choice: A question is paired with a set of possible answers, and the task is to select the correct answer.</li> <li>Open domain: The model answers questions in natural language without provided options, often by querying a large number of texts.</li> </ul> <p>Question</p> <p>In a modern email application, which of the above NLP tasks could be utilized to achieve a great user experience?</p> Answer <p>We can find many NLP tasks in a modern email client, such as:</p> <ul> <li>information extraction: suggest calendar events</li> <li>spam classification: spam or not spam</li> <li>summarization: a potential plugin for long and annoying emails</li> <li>information retrieval: email search capabilities</li> <li>text generation: suggest answers or sophisticated autocompletion</li> <li>...</li> </ul>"},{"location":"lectures/introduction/#building-blocks-of-language","title":"Building Blocks of Language","text":"<p>While linguistics is the systematic study of language, NLP tries to make language processable by computer systems.</p> <p>To study NLP, we don't need to be linguistic experts, but it is important to understand some core linguistic concepts. In this section, we will dissect language into its building blocks.</p> <p>For the scope of this course, we think of human language as composed of four major building blocks: phonemes, morphemes and lexemes, syntax, and context. In linguistics, each of the building blocks described in this section is a research area for itself.</p> <p></p>"},{"location":"lectures/introduction/#phonemes","title":"Phonemes","text":"<p>Description:</p> <ul> <li>Layer of speech and sound</li> <li>Smallest units of speech sound in a language</li> <li>Vary across languages, i.e. each language has its own set of phonemes.</li> <li>Standard English has 44 phonemes (i.e. 24 consonant phonemes, and 20 vowel phonemes)</li> <li>German: 46 phonemes</li> <li>Spanish: 24 phonemes</li> <li>Usually denoted in slashes using the symbols of the International Phonetic Alphabeth (IPA)</li> <li>May or may not have a meaning by themselves, but can induce meaning, e.g. <code>/s/</code> for plural or <code>/\u026a/</code> for adjective</li> <li>A nice list of examples can be found here</li> </ul> <p>Important for:</p> <ul> <li>Speech recognition</li> <li>Speech-to-text</li> <li>Text-to-speech</li> </ul> <p>Examples</p> <p>Consonant phonemes:</p> <ul> <li><code>/b/</code> as in \"bat\" \ud83e\udd87</li> <li><code>/d\u0292/</code> as in \"giraffe\" \ud83e\udd92</li> </ul> <p>Vowel phonemes:</p> <ul> <li><code>/i\u02d0/</code> as in \"bee\" \ud83d\udc1d</li> <li><code>/\u00e6/</code> as in \"cat\" \ud83d\udc08</li> </ul>"},{"location":"lectures/introduction/#morphemes-and-lexemes","title":"Morphemes and Lexemes","text":"<p>The layer of words consists of morphemes and lexemes.</p> <p>Morphemes:</p> <ul> <li>Smallest unit of language that has a meaning</li> <li>Formed by a combination of phonemes</li> <li>All prefixes or suffixes are morphemes, e.g. <code>multi</code> as in <code>multimedia</code></li> <li>When breaking down words into their morphemes, variations may occur</li> </ul> <p>Lexemes:</p> <ul> <li>Consists of one or more morphemes</li> <li>Multiple words can go back to the same lexeme</li> <li>Not synonymous with the term \"word\", rather comparable to an entry in a dictionary or an encyclopedia</li> <li>Different verb forms go back to the same lexeme</li> </ul> <p>Important for:</p> <ul> <li>Tokenization</li> <li>Stemming</li> <li>POS-tagging</li> <li>Word embeddings</li> </ul> <p>Examples</p> <p>Breaking down a word into its morphemes:</p> <ul> <li>no variation: \\(unbreakable\\) \ud83d\udc49 \\(un + break + able\\)</li> <li>with variation: \\(unreliability\\) \ud83d\udc49 \\(un + rely + able + ity\\)</li> </ul> <p>Rooting back words on its lexemes:</p> <ul> <li>\\(\\{jump, jumps, jumping, ...\\}\\) \ud83d\udc49 \\(jump\\)</li> <li>\\(\\{buy, bought, buying, ...\\}\\) \ud83d\udc49 \\(buy\\)</li> </ul> <p>Differentiation of words, lexemes, and morphemes:</p> <ul> <li>word \\(football\\) (noun) \ud83d\udc49 lexeme: \\(football\\) \ud83d\udc49 morphemes: \\(foot + ball\\)</li> <li>word \\(cats\\) (noun) \ud83d\udc49 lexeme: \\(cat\\) \ud83d\udc49 morphemes: \\(cat + s\\)</li> <li>word \\(tumbling\\) (verb) \ud83d\udc49 lexeme: \\(tumble\\) \ud83d\udc49 morphemes: \\(tumble + ing\\)</li> <li>word \\(happening\\) (verb) \ud83d\udc49 lexeme: \\(happen\\) \ud83d\udc49 morphemes: \\(happen + ing\\)</li> </ul> <p>Tip</p> <p>A lexeme is not synonymous to a word, but can be thought of as an abstract word: if we use lexemes, they become words</p> <p>Warning</p> <p>There is some linguistic debate on morphemes and lexemes, but we leave that to linguistics experts. For the scope of this course, the differentiation as given above is sufficient.</p>"},{"location":"lectures/introduction/#syntax","title":"Syntax","text":"<ul> <li>Layer of phrases and sentences</li> <li>Set of rules to construct grammatically correct sentences out of words</li> <li>The syntactic structure can be described in many different ways, e.g. grammar</li> <li>In NLP, it is common to describe the anatomy of a sentence using POS tags</li> </ul> <p>The following snippet shows POS-tagging in Python using the NLTK library and the universal tagset:</p> <pre><code>&gt;&gt;&gt; from nltk import pos_tag, word_tokenize\n&gt;&gt;&gt; pos_tag(word_tokenize(\"The girl plays tennis.\"), tagset=\"universal\")\n[\n    ('The', 'DET'),\n    ('girl', 'NOUN'),\n    ('plays', 'VERB'),\n    ('tennis', 'NOUN'),\n    ('.', '.')\n]\n</code></pre> <p>Example</p> <p>Consider the following sentence:</p> <p>\"The girl plays tennis.\"</p> <p>Grammatical analysis \ud83d\udc49 The girl (subject) plays (verb) tennis (object).</p> <p>POS-tagging using the universal tagset: \ud83d\udc49 The (DET) girl (NOUN) plays (VERB) tennis (NOUN) .(.)</p> <p>Question</p> <p>Imagine you need to analyze a set of tweets and the sentence above changes to:</p> <p>\"The girl \ud83d\udc67 plays tennis \ud83c\udfbe.\"</p> <p>Given the code above, how would you expect the POS-tagger to handle emojis?</p> <p>Find some further information here.</p> Answer <p>It would depend on the use case, there are situations where the emoji is a crucial part of the sentence, to indicate emotions (e.g. social media posts). If it is just about the conveyed information, it is probably OK to remove them during pre-processing.</p>"},{"location":"lectures/introduction/#context","title":"Context","text":"<ul> <li>Layer of meaning</li> <li>Describes how the various parts of language come together and convey a particular meaning</li> <li>Includes world knowledge and common sense</li> <li>Depending on the context, a word or sentence may change its meaning, as in the case of ambiguity</li> <li>Consists of semantics and pragmatics (external context)</li> <li>Semantics refers to the direct meaning of words or sentences, without external context</li> <li>Pragmatics add world knowledge and enables us to infer implied meaning</li> </ul> <p>Important for:</p> <ul> <li>Text summarization</li> <li>Conversational agents</li> <li>Sentiment analysis</li> </ul> <p>Example</p> <p>Consider the following sentence:</p> <p>I'm so hungry, I could eat a horse!</p> <p>Semantics would assume that this person wants to eat a horse. \ud83d\udc0e</p> <p>Pragmatics applies world knowledge and infers that this person is very hungry. \ud83c\udf74</p> <p>Info</p> <p>Being a linguistic expert is not necessary to master NLP, but by understanding the basic building blocks of language, we can use the right NLP tools in real-world projects.</p>"},{"location":"lectures/introduction/#challenges-in-nlp","title":"Challenges in NLP","text":""},{"location":"lectures/introduction/#ambiguity","title":"Ambiguity","text":"<ul> <li>Occurs when a word, phrase, or sentence has more than one plausible interpretation</li> <li>The intended meaning often depends on the context or world knowledge</li> <li>Related to vagueness and uncertainty</li> <li>Most languages are inherently ambiguous</li> <li>Humans use it sometimes on purpose</li> </ul> <p>Example</p> <p>Consider the following sentence:</p> <p>The boy ate the cookies on the couch \ud83c\udf6a\ud83d\udecb</p> <ol> <li>The boy ate the cookies that were lying on around on the couch</li> <li>The boy was sitting on the couch while eating cookies</li> </ol> <p>Question</p> <p>Which language is supposed to be the least ambiguous of all languages?</p> Answer <p>The language of mathematics is unambiguous by design. A mathematical term is means the same everywhere.</p> <p>Also programming languages are designed to be unambiguous and deterministic.</p>"},{"location":"lectures/introduction/#world-knowledge","title":"World Knowledge","text":"<ul> <li>Set of all facts that most humans, or a targeted group of humans, are aware of</li> <li>Generally assumed that these facts are known, hence not explicitly mentioned</li> <li>Can change over time, e.g. heliocentric model</li> </ul> <p>Example</p> <p>Consider the following 2 sentences:</p> <ol> <li>Man bit dog \ud83d\udc68\ud83d\udc36</li> <li>Dog bit man \ud83d\udc36\ud83d\udc68</li> </ol> <p>As humans, we know that only the second sentence is plausible since we assume that dogs are known to bite humans. Whereas for a machine, if not further specified, both sentences are equally plausible.</p> <p>Info</p> <p>A nice collection of examples of how ambiguity and world knowledge play together are Winograd schemas.</p> <p>Here is an example:</p> <p>The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.</p> <ol> <li>If the word is feared, then they presumably refers to the city council.</li> <li>If it is advocated, then they presumably refers to the demonstrators.</li> </ol> <p>Examples like this one are easily disambiguated by the human reader (sometimes we don't even notice that there is ambiguity), mostly because of world knowledge, but they pose challenges to most NLP techniques.</p> <p>The official collection of 150 Winograd schemas can be found here.</p> <p>In 2016, there was a challenge announced, where the highest score achieved was 58%.</p> <p>Nowadays, NLP systems achieve more than 90% accuracy. A review of approaches as of April 2020 can be found here.</p>"},{"location":"lectures/introduction/#creativity","title":"Creativity","text":"<ul> <li>Language not only consists of rules, there are also a lot of creative aspects to it</li> <li>Language contains a lot of variety, e.g. styles, genres, dialects, aesthetics</li> <li>There is not \"one correct way\"</li> <li>Understanding creativity is a hard difficult problem, not just in NLP, but in AI in general (and probably also a philosophical problem)</li> </ul> <p>Quote</p> <p>Writing beautiful novels probably requires a high degree of world knowledge and creativity:</p> <p>He stepped down, trying not to look long at her, as if she were the sun, yet he saw her, like the sun, even without looking.</p> <p>Leo Tolstoy, Anna Karenina</p>"},{"location":"lectures/introduction/#diversity","title":"Diversity","text":"<ul> <li>Many times, no direct mapping between the vocabularies of any two languages</li> <li>Makes adapting NLP solutions to other languages very hard</li> <li>Build a language-agnostic system, or build separate solutions for each language</li> </ul> <p>Example</p> <p>The German word Heimat has no exact English equivalent.</p>"},{"location":"lectures/introduction/#nlp-in-the-era-of-artificial-intelligence","title":"NLP in the era of Artificial Intelligence","text":"<p>In this section, we will look at NLP and how it is related to articial intelligence (AI).</p> <p>In general, NLP aims to give computers the ability to understand text and spoken words in much the same way human beings can.</p> <p>It combines computational linguistics with programming, statistics, machine learning, and deep learning. The field has greatly benefit from recent advancements in ML and DL and is one of the fast-growing research domains in AI.</p> <p>However, in its simplest forms, NLP is not necessarily based on AI, and we can also solve some of the simpler tasks with rule-based apporaches.</p> <p></p> <p>Artificial Intelligence (AI), Oxford English Dictionary</p> <p>The theory and development of computer systems able to perform tasks normally requiring human intelligence.</p> <p>Info</p> <p>A good read on how AI, ML, and DL are related is this article by IBM. This article gives more details about the role of NLP in that context.</p>"},{"location":"lectures/introduction/#rule-based-nlp","title":"Rule-based NLP","text":"<ul> <li>Defining rules for the task to be solved</li> <li>Utilize lexical resources</li> <li>Requires developers to have a certain degree of domain expertise</li> </ul> <p>Regular expressions (regex)</p> <p>A very common rule-based tool for text analysis are regular expressions, where we define search patterns to find substrings in a text. Regexes operate deterministically, i.e. it is either a match or not. Here is a guide using regular expressions with Python, which will be useful throughout the course (as well as your entire journey as a Python developer).</p> <p>WordNet</p> <p>WordNet is a database of words and their semantic relationship:</p> <ul> <li>Synonyms: <code>car</code> and <code>automobile</code> \ud83d\ude97 (interchangeable)</li> <li>Hyponyms: both <code>baseball</code> \u26be and <code>tennis</code> \ud83c\udfbe are both <code>sports</code> (aka \"super-subordinate\" relation)</li> <li>Meronyms: <code>arm</code> \ud83d\udcaa and <code>foot</code> \ud83e\uddb6 are <code>body</code> parts (aka \"part-whole\" relation)</li> </ul> <p>Question</p> <p>What NLP tasks do you think could be solved using rule-based approaches? What are the advantages and limitations of such approaches?</p> Answer <p>Examples would be sentiment analysis based on word count, or a regex to map tracking numbers to shipping companies.</p> <ul> <li>advantages: light-weight, quick &amp; easy compared to DL models, are deterministic</li> <li>limitations: hard to solve more complex NLP tasks</li> </ul> <p>We can use rule based approaches to bridge gaps between NLP systems, e.g. eliminate mistakes made by AI with rule-based approaches</p>"},{"location":"lectures/introduction/#machine-learning-for-nlp","title":"Machine Learning for NLP","text":"<ul> <li>Branch of AI</li> <li>Algorithms that can learn to perform tasks based on a large number of examples</li> <li>No explicit instructions required, algorithm learns patterns</li> <li>Requires numeric representation (aka \"features\") of the training data</li> <li>ML can be applied to textual data similar to other forms of data</li> <li>A special focus needs to be put on pre-processing and feature extraction</li> <li>training a model is then \"business as usual\"</li> </ul>"},{"location":"lectures/introduction/#supervised-learning","title":"Supervised learning","text":"<ul> <li>learn mapping function from input to output</li> <li>requires a large number of labeled training data, i.e. known input-output pairs</li> </ul> <p>Example</p> <p>An email spam filter is a common example where supervised learning is used in NLP.</p>"},{"location":"lectures/introduction/#unsupervised-learning","title":"Unsupervised learning","text":"<ul> <li>Aims to find hidden patterns in given input data</li> <li>Output is unknown, i.e. works with unlabeled data</li> </ul> <p>Example</p> <p>An example where unsupervised learning is used in NLP is topic modeling, where we try to identify topics in a large collection of text data, e.g. news articles, without prior knowledge of these topics. A simple approach to topic modeling is a tag cloud.</p>"},{"location":"lectures/introduction/#deep-learning-for-nlp","title":"Deep Learning for NLP","text":"<p>When we speak of using deep learning for NLP, this encompasses all algorithms based on artifical neural networks. Deep learing approaches in NLP led to significant advances in the recent year in the field.</p> <p>This section gives a brief overview of the terms that are used in this context.</p>"},{"location":"lectures/introduction/#language-models","title":"Language Models","text":"<ul> <li>System that is trained to understand and generate human-like text</li> <li>Designed to predict and generate sequences of words or characters based on the input it receives</li> <li>Learns patterns, structures, and relationships within a given language by being exposed to large amounts of text data</li> <li>They can be based on various architectures, including RNNs, LSTMs, CNNs, and more recently, transformers</li> </ul> <p>Example</p> <p>GPT-3, based on the transformer architecture, is an example of a powerful language model.</p>"},{"location":"lectures/introduction/#convolutional-neural-networks-cnns","title":"Convolutional Neural Networks (CNNs)","text":"<ul> <li>Adapted from computer vision tasks</li> <li>Require word embeddings to build sentences matrices, which can be treated analogously to images</li> </ul>"},{"location":"lectures/introduction/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNNs)","text":"<ul> <li>language is sequential by nature, e.g. text flows from one direction to another</li> <li>RNNs are a type of neural network designed for sequential data, making them suitable for tasks where the order of the input matters</li> <li>Can remember what they have processed so far, but cannot remember long contexts</li> </ul>"},{"location":"lectures/introduction/#long-short-term-memory-lstm","title":"Long Short-Term Memory (LSTM)","text":"<ul> <li>LSTMs are a specific type of RNN designed to address the vanishing gradient problem, enabling better learning of long-range dependencies in sequential data.</li> <li>Let go of irrelevant context, and only remember the context that is required to solve the task at hand</li> </ul>"},{"location":"lectures/introduction/#transformers","title":"Transformers","text":"<ul> <li>Type of architecture that has gained prominence in NLP</li> <li>Use attention mechanisms to capture relationships between different parts of a sequence simultaneously, making them effective for processing sequential data, including language</li> <li>Look at surrounding words to derive context (e.g. bank as a river bank or financial institution)</li> </ul>"},{"location":"lectures/introduction/#transfer-learning","title":"Transfer Learning","text":"<ul> <li>Transfer learning is a machine learning paradigm where a model trained on one task is adapted or fine-tuned for a different but related task</li> <li>Often used to leverage pre-trained models for specific applications</li> <li>The model is able to transfer the pre-trained knowledge for downstream tasks</li> </ul>"},{"location":"lectures/introduction/#foundation-models","title":"Foundation Models","text":"<ul> <li>The term foundation model refers to large-scale language models based on the transformer architecture</li> <li>They serve as a starting point for various NLP tasks</li> <li>It emphasizes the idea that a pre-trained model forms the foundation and can be adapted for various tasks</li> </ul> <p>Info</p> <p>GPT-3 is known to be trained on 45 TB of text data and the model has about 175 billion parameters.</p>"},{"location":"lectures/introduction/#attention","title":"Attention","text":"<ul> <li>The attention mechanism is a key component of the transformer model architecture and plays a crucial role in capturing contextual information across sequences.</li> <li>Attention mechanisms, particularly self-attention in the context of transformers, allow models to focus on different parts of the input sequence when making predictions.</li> <li>Especially beneficial for capturing long-range dependencies.</li> <li>In general, attention is the ability to focus on important things and ignore irrelevant things, as certain parts of a sentence are more important than others</li> </ul>"},{"location":"lectures/introduction/#limitations-of-ai-in-nlp","title":"Limitations of AI in NLP","text":"<p>DL has brought NLP to the next level, and powerful transformer models have become SOTA in most NLP tasks. However, DL is not the silver bullet for all NLP tasks, especially when it comes to industrial applications<sup>2</sup>:</p> <ul> <li> <p>Overfitting on small datasets:   DL models need more training data to fit all their parameters, but many times, sufficient training data is not available.   Hence, they overfit on small datasets and have poor generalization capabilities.   Also, consider Occam's razor: given that all other conditions are equal, the simpler solution should be preferred.</p> </li> <li> <p>Domain adaption:   Most DL models are trained on common domains (e.g. news articles).   If we try to use them for a different domain (e.g. social media), we may observe poor performance.   This is because language is domain-specific (think of all the emojis and all the slang words in social media posts).   Therefore, a well-designed rule-based domain-specific that encodes all required knowledge may outperform a complex DL model.</p> </li> <li> <p>Explainable models:   Most of the time, DL models work like a black box, but in many use cases, industries demand interpretable results that can be explained to end users.   In such cases, traditional approaches might be more useful.   For example, with Naive Bayes for sentiment analysis, the effect of positive vs. negative words can be easily explained.   While for computer vision, many techniques exist to explain model predictions, this is not as common in NLP.</p> </li> <li> <p>Common sense and world knowledge   As mentioned earlier, teaching NLP models common sense, and world knowledge remains a challenge.   Also, logical reasoning falls into this category, and understanding events and understand their consequences are complex tasks for machines.</p> </li> <li> <p>Cost   Many cost factors apply to complex NLP models, especially when DL-based.   They require a lot of (labeled) data, and training not only takes a lot of time but also consumes a lot of hardware resources.   In addition, such large models may have latency issues.</p> </li> <li> <p>Deployment   In some use cases, some deployment constraints apply.   For example, the NLP system needs to run on an embedded device with limited resources, or even offline, rather than in the cloud.</p> </li> </ul> <p>Example</p> <p>Consider the following sentence:</p> <p>John walks out of the house into his garden.</p> <p>As humans, we immediately reason that John's current location is the garden, and his previous location was inside his house, but this kind of reasoning is hard to incorporate into machines.</p> <p>Is AI dumber than a cat?</p> <p>Read in this Wall Street Journal article why Yann LeCun, Meta's Chief AI Scientist, thinks that AI is dumber than a cat. \ud83d\udc31</p> <p>Resource consumption of ChatGPT</p> <p>An interesting Twitter thread from Professor Tom Goldstein gives some easily digestible figures about the resources consumed by ChatGPT. This is not what you can invest in your everyday NLP system!</p> <p>AI is not always the best answer</p> <p>DL has led to significant advances in NLP, but be aware that DL is not always the go-to solution for NLP projects. Most of the SOTA models are trained on common and very clean datasets, which does not apply to many industry use cases.</p> <p>Therefore it is even more important to understand the fundamentals of NLP and apply the right method for the right use case, as cutting-edge solutions may not always be the best choice.</p> <p>In many real-world scenarios, the focus is more on the data pipeline side.</p> <p>Question</p> <p>In a hate speech detection project on tweets, a lesson learned was that a simple Naive Bayes prototype performed similarly, if not better, than a fine-tuned BERT model. Why do you think that is?</p> Answer <ul> <li>Tweets are very short snippets of text and contain a lot of slang or informal words, as well as emojis.</li> <li>Models like BERT are trained on rather \"clean\" corpora, like news articles.</li> <li>We can adapt that, but for short text like tweets, BERT probably provides an overhead, and we found that a simple approach tailored to that exact use case leads to similar if not better results, with much less effort.</li> </ul> <p>Start simple and improve iteratively</p> <p>In many cases, it makes sense to start with a simple baseline model and adjust and improve model complexity iteratively.</p> <p>This is useful advice in many areas of software development. \ud83d\ude03</p> <p></p>"},{"location":"lectures/introduction/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Our modern lives are full of NLP applications, and we encounter them every day.</li> <li>When building an NLP system or application, we need to understand the problem we are trying to solve and the NLP tasks that are required to solve it.</li> <li>To work on NLP problems, we need to have a basic understanding of how we can approach language programmatically.</li> <li>It is also important to understand if we are looking at a problem from a research or engineering perspective.</li> <li>The basic building blocks of language are phonemes, morphemes &amp; lexemes, syntax, and context.</li> <li>Trying to map language to a computer system poses several challenges, as language is inherently ambiguous, requires world knowledge, and involves creativity and diversity.</li> <li>It is very common to make use of AI to solve NLP problems, but not every NLP problem requires AI. Depending on the use case, rule-based approaches may be sufficient.</li> <li>Recent advancement in NLP is mainly driven by deep learning and transformer models and their ability to learn from large amounts of data. This is the beginning of a new era in NLP.</li> </ul> <ol> <li> <p>https://www.ibm.com/think/topics/natural-language-processing \u21a9</p> </li> <li> <p>Vajjala, Sowmya, S.V. Bharathi, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana. Practical Natural Language Processing: A Comprehensive Guide to Building Real-world NLP Systems. Sebastopol, CA: O'Reilly Media, 2020. https://www.practicalnlp.ai/.\u00a0\u21a9\u21a9</p> </li> <li> <p>https://www.databricks.com/sites/default/files/2023-06/compact-guide-to-large-language-models.pdf \u21a9</p> </li> <li> <p>https://www.deeplearning.ai/resources/natural-language-processing/ \u21a9</p> </li> </ol>"},{"location":"lectures/language_models/","title":"Language Models","text":"<p>This lecture is about language models. A language model is able to calculate the probability of a sequence of words, such as a sentence. They can also estimate the probability of an upcoming word given a history of previous words.</p> <p>A popular application of language models is auto-complete.</p> <p></p> <p>Example</p> <p>Given the sentence:</p> <p>\"I want to eat an ...\"</p> <p>A language model can predict the next word:</p> <ul> <li>apple \ud83c\udf4f</li> <li>orange \ud83c\udf4a</li> </ul>"},{"location":"lectures/language_models/#applications-of-language-models","title":"Applications of Language Models","text":"<p>N-gram Language Models can be used for almost any task that involves predicting the next word in a sentence:</p> <ul> <li>Auto-complete: predict the next word in a sentence</li> <li>Summarization: generate a summary of a long text</li> <li>Spelling correction: correct typos in a word</li> <li>Speech recognition: convert speech to text</li> <li>Machine translation: convert text in one language to another language</li> </ul> <p>Example</p> <p>Speech recognition:</p> <ul> <li>\"I need to water \ud83d\udca6 a dozen red noses \ud83d\udc43 for my adversary.\" \u274c</li> <li>\"I need to order \ud83d\uded2 a dozen red roses \ud83c\udf39 for my anniversary.\" \u2705</li> </ul> <p>Spell correction:</p> <ul> <li>\"He entered the ship \ud83d\udea2 and bought a bannaa \ud83c\udf4c.\" \u274c</li> <li>\"He entered the shop \ud83d\uded2 and bought a banana \ud83c\udf4c.\" \u2705</li> </ul>"},{"location":"lectures/language_models/#text-corpus","title":"Text Corpus","text":"<p>A text corpus is a large and structured set of texts, such as:</p> <ul> <li>Wikipedia</li> <li>News articles</li> <li>Books</li> <li>Blog posts</li> <li>Tweets</li> </ul> <p>A corpus can be general, such as Wikipedia or news articles, or it can be domain specific, such as medical texts or legal documents.</p> <p>Vocabulary size vs. corpus size</p> <p>Note that the vocabulary size \\(|V|\\) is the number of unique words in the corpus, whereas the corpus size \\(|C|\\) is the total number of words in the corpus.</p> <p></p>"},{"location":"lectures/language_models/#sequence-of-words","title":"Sequence of Words","text":"<p>When building language models, we make use of sequence of words in a text corpus.</p> <p>Given a corpus \\(C\\) and a sequence of words \\(w_1, w_2, \\ldots, w_n\\) with \\(w_i \\in C\\), we can denote the sequence as:</p> \\[ w_1^n = w_1, w_2, \\ldots, w_n \\] <p>Example</p> <p>Given a corpus \\(C\\) of size \\(n\\), we can denote the first three words as:</p> \\[ w_1^3 = w_1, w_2, w_3 \\] <p>The last three words are:</p> \\[ w_{n-2}^n = w_{n-2}, w_{n-1}, w_n \\]"},{"location":"lectures/language_models/#n-gram","title":"N-gram","text":"<p>In general, the term N-gram refers to a sequence of \\(N\\) items.</p> <p>In the context of NLP, an N-gram is a sequence of \\(N\\) words:</p> <ul> <li>Unigram (1 word): set of all unique single words in a text corpus</li> <li>Bigram (2 words): set of all unique pairs of words in a text corpus</li> <li>Trigram (3 words): set of all unique triplets of words in a text corpus</li> <li>N-gram (\\(N\\) words): set of all unique sequences of \\(N\\) words in a text corpus</li> </ul> <p></p> <p>Punctuation is usually treated like words.</p> <p>N-grams refer to unique sequences of words. So if a sentence contains the same N-gram multiple times, it appears only once in the set of N-grams.</p> <p>Example</p> <p>The sentence \"I want to eat an apple.\" \ud83c\udf4f contains the following N-grams:</p> <ul> <li>unigrams: \\(\\{\\text{I}, \\text{want}, \\text{to}, \\text{eat}, \\text{an}, \\text{apple}, \\text{.}\\}\\)</li> <li>bigrams: \\(\\{\\text{I want}, \\text{want to}, \\text{to eat}, \\text{eat an}, \\text{an apple}, \\text{apple .}\\}\\)</li> <li>trigrams: \\(\\{\\text{I want to}, \\text{want to eat}, \\text{to eat an}, \\text{eat an apple}, \\text{an apple .}\\}\\)</li> </ul> <p>The order is important. Only words that appear next to each other in the text can form an N-gram.</p> <p>Example</p> <p>Given the example above, the words \"I\" and \"eat\" do not form a bigram \u274c</p> <p>Collocations</p> <p>Words that appear next to each other frequently are called collocations.</p> <p>An N-gram language model can learn and leverage these collocations to improve its predictive accuracy and generate more coherent and contextually relevant language. They play a key role in understanding the nuances of a language.</p> <p>Here are some examples:</p> <ul> <li>\"ice cream\" \ud83c\udf66</li> <li>\"machine learning\" \ud83e\udd16</li> <li>\"New York\" \ud83d\uddfd</li> <li>\"want to\"</li> </ul>"},{"location":"lectures/language_models/#n-gram-probabilities","title":"N-gram Probabilities","text":"<p>Now we will learn how to calculate the probability of an N-gram. We will start with the unigram probability, which is simply the probability of a word appearing in the corpus, and then generalize to bigrams, trigrams, and N-grams.</p>"},{"location":"lectures/language_models/#unigram-probability","title":"Unigram Probability","text":"<p>The unigram probability of a word \\(w\\) is the probability of the word appearing in the corpus \\(C\\):</p> \\[ P(w) = \\frac{\\text{freq}(w)}{|C|} \\] <p>where</p> <ul> <li>\\(\\text{freq}(w)\\) is the number of times the word \\(w\\) appears in the corpus \\(C\\)</li> <li>\\(|C|\\) is the size of the corpus \\(C\\), i.e. the total number of words in the corpus</li> </ul> <p>Unigram Probability</p> <p>Given the corpus:</p> <p>I am happy because I am learning</p> <p>The unigram probability of the word \"I\" is given by:</p> \\[ P(\\text{I}) = \\frac{\\text{freq}(\\text{I})}{|C|} = \\frac{2}{7} \\] <p>And the unigram probability of the word \"happy\" is given by:</p> \\[ P(\\text{happy}) = \\frac{\\text{freq}(\\text{happy})}{|C|} = \\frac{1}{7} \\]"},{"location":"lectures/language_models/#bigram-probability","title":"Bigram Probability","text":"<p>The general formula for the bigram probability of a word \\(w_i\\) given the previous word \\(w_{i-1}\\) is given by:</p> \\[ P(w_i | w_{i-1}) = \\frac{\\text{freq}(w_{i-1},w_i)}{\\text{freq}(w_{i-1})} \\] <p>where</p> <ul> <li>\\(\\text{freq}(w_{i-1}, w_i)\\) is the number of times the bigram \\(w_{i-1}, w_i\\) appears in the corpus \\(C\\)</li> <li>\\(\\text{freq}(w_{i-1})\\) is the number of times the word \\(w_{i-1}\\) appears in the corpus \\(C\\)</li> </ul> <p>Bigram vs. Conditional Probability</p> <p>Note that the bigram probability is the same as the conditional probability of the word \\(w_i\\) given the previous word \\(w_{i-1}\\).</p> <p>Bigram Probability</p> <p>Given the corpus:</p> <p>I am happy because I am learning</p> <p>The bigram probability of the \"I am\", i.e. the probability of the word \"am\" following the word \"I\", is given by:</p> \\[ P(\\text{am} | \\text{I}) = \\frac{\\text{freq}(\\text{I am})}{\\text{freq}(\\text{I})} = \\frac{2}{2} = 1 \\] <p>The probability of the bigram \"I happy\" is given by:</p> \\[ P(\\text{happy} | \\text{I}) = \\frac{\\text{freq}(\\text{I happy})}{\\text{freq}(\\text{I})} = \\frac{0}{2} = 0 \\] <p>The probability of the bigram \"am learning\" is given by:</p> \\[ P(\\text{learning} | \\text{am}) = \\frac{\\text{freq}(\\text{am learning})}{\\text{freq}(\\text{am})} = \\frac{1}{2} = 0.5 \\]"},{"location":"lectures/language_models/#trigram-probability","title":"Trigram Probability","text":"<p>The general formula for the trigram probability of a word \\(w_i\\) given the previous two words \\(w_{i-2}\\) and \\(w_{i-1}\\) is given by:</p> \\[ P(w_i | w_{i-2}, w_{i-1}) = \\frac{\\text{freq}(w_{i-2},w_{i-1},w_i)}{\\text{freq}(w_{i-2},w_{i-1})} \\] <p>where</p> <ul> <li>\\(\\text{freq}(w_{i-2},w_{i-1},w_i)\\) is the number of times the trigram \\(w_{i-2},w_{i-1},w_i\\) appears in the corpus \\(C\\)</li> <li>\\(\\text{freq}(w_{i-2},w_{i-1})\\) is the number of times the bigram \\(w_{i-2},w_{i-1}\\) appears in the corpus \\(C\\)</li> </ul> <p>Note that we can think of a trigram as a bigram followed by a unigram. From the sequence notation shown above, we can rewrite a trigram as:</p> \\[ w_{i-2},w_{i-1},w_i = w_{i-2}^{i-1},w_i = w_{i-2}^i \\] <p>The trigram probability can then be rewritten as:</p> \\[ P(w_i | w_{i-2}^{i-1}) = \\frac{\\text{freq}(w_{i-2}^{i-1},w_i)}{\\text{freq}(w_{i-2}^{i-1})} \\] <p>The formula states that the conditional probability of the third word given the previous two words is the count of all three words appearing divided by the count of the previous two words appearing in the correct sequence.</p> <p>Trigram Probability</p> <p>The probability of the trigram \\(w_{i-2},w_{i-1},w_i\\) is the probability of the word \\(w_i\\) given the bigram \\(w_{i-2},w_{i-1}\\) has already occurred.</p> <p>We can also say it is the conditional probabity of the third word, given that the previous two words have already occurred.</p> <p>Example</p> <p>Given the corpus:</p> <p>I am happy because I am learning</p> <p>The trigram probability of the \"I am happy\", i.e. the probability of the word \"happy\" following the bigram \"I am\", is given by:</p> \\[ P(\\text{happy} | \\text{I am}) = \\frac{\\text{freq}(\\text{I am happy})}{\\text{freq}(\\text{I am})} = \\frac{1}{2} = 0.5 \\]"},{"location":"lectures/language_models/#n-gram-probability","title":"N-gram Probability","text":"<p>To generalize the formula to N-grams for any \\(N\\), we can write the N-gram probability of a word \\(w_i\\) given the previous \\(N-1\\) words \\(w_{i-N+1}, \\ldots, w_{i-1}\\) as:</p> \\[ P(w_i | w_{i-N+1}^{i-1}) = \\frac{\\text{freq}(w_{i-N+1}^{i-1},w_i)}{\\text{freq}(w_{i-N+1}^{i-1})} \\] <p>where</p> <ul> <li>\\(\\text{freq}(w_{i-N+1}^{i-1},w_i)\\) is the number of times the N-gram \\(w_{i-N+1}^{i-1},w_i\\) appears in the corpus \\(C\\)</li> <li>\\(\\text{freq}(w_{i-N+1}^{i-1})\\) is the number of times the (N-1)-gram \\(w_{i-N+1}^{i-1}\\) appears in the corpus \\(C\\)</li> </ul> <p>Note that we can think of an N-gram as an (N-1)-gram followed by a unigram:</p> \\[ w_{i-N+1}^{i-1},w_i = w_{i-N+1}^i \\] <p>N-gram Probability</p> <p>The probability of the N-gram \\(w_{i-N+1}^{i-1},w_i\\) is the probability of the word \\(w_i\\) given the (N-1)-gram \\(w_{i-N+1}^{i-1}\\) has already occurred.</p> <p>We can also say it is the conditional probabity of the \\(i\\)-th word, given that the previous \\(N-1\\) words have already occurred.</p> <p>Example</p> <p>Given the corpus:</p> <p>In every place of great resort the monster was the fashion. They sang of it in the cafes, ridiculed it in the papers, and represented it on the stage</p> <p>Jules Verne, Twenty Thousand Leagues under the Sea</p> <p>The probability of word \"papers\" following the phrase \"it in the\" is given by:</p> \\[ P(\\text{papers} | \\text{it in the}) = \\frac{\\text{count}(\\text{it in the papers})}{\\text{count}(\\text{it in the})} = \\frac{1}{2} \\]"},{"location":"lectures/language_models/#sequence-probabilities","title":"Sequence Probabilities","text":"<p>If we think of a sentence as a sequence of words, how can we calculate the probability of the sentence?</p> <p>Example</p> \\[P(\\text{The teacher drinks tea}) = ?\\] <p>Recall the conditional probability of \\(A\\) given \\(B\\):</p> \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] <p>From Bayes rule, we know that:</p> \\[ P(A \\cap B) = P(A) \\cdot P(B | A) \\] <p>This means that the probability of \\(A\\) and \\(B\\) is the probability of \\(A\\) times the probability of \\(B\\) given \\(A\\).</p> <p>We can generalize this to a sequence of events \\(A, B, C, D\\) as follows:</p> \\[ P(A, B, C, D) = P(A) \\cdot P(B | A) \\cdot P(C | A, B) \\cdot P(D | A, B, C) \\] <p>Example</p> <p>Let's take a look at a simple example, to get a better understanding of the formula.</p> \\[ P(\\text{The teacher drinks tea}) = P(\\text{The}) \\cdot P(\\text{teacher} | \\text{The}) \\cdot P(\\text{drinks} | \\text{The teacher}) \\cdot P(\\text{tea} | \\text{The teacher drinks}) \\] <p>This means that the probability of the sentence \"The teacher drinks tea\" is</p> <ul> <li>the probability of the word \"The\"</li> <li>times the probability of the word \"teacher\" given the word \"The\"</li> <li>times the probability of the word \"drinks\" given the words \"The teacher\"</li> <li>times the probability of the word \"tea\" given the words \"The teacher drinks\".</li> </ul> <p>From this example, we can see that the longer the sentence gets, the more unlikely it is to occur in the corpus.</p> <p>We can also see this by looking at the probability of a sentence vs. the probability of a single word. From the intersection of probabilities, we know that:</p> \\[ P(A \\cap B) \\leq P(A) \\] <p>And thus, a sequence of events \\(A, B, C, D\\) is less likely to occur than the first event \\(A\\):</p> \\[ P(A, B, C, D) &lt; P(A) \\] <p>Example</p> <p>In a regular corpus, we can assume that the probability of the word \"drinks\" is higher than the probability of the sentence \"The teacher drinks tea\", because the word \"drinks\" will appear in other sentences as well.</p> \\[ P(\\text{The teacher drinks tea}) &lt; P(\\text{drinks}) \\] <p>Note that the probability of a sentence can be zero, because not every possible sentence will appear in the corpus.</p> <p>In fact, a corpus almost never contains the exact sentence that we want to calculate the probability for.</p> <p>The longer the sentence, the more unlikely it is to occur in the corpus.</p> <p>Example</p> <p>Consider the following sentence:</p> <p>The teacher drinks tea</p> <p>The probability of this sentence is given by:</p> \\[ P(\\text{The teacher drinks tea}) = P(\\text{The}) \\cdot P(\\text{teacher} | \\text{The}) \\cdot P(\\text{drinks} | \\text{The teacher}) \\cdot P(\\text{tea} | \\text{The teacher drinks}) \\] <p>If we look at the probability of the word \"tea\" or the word \"drinks\", we can imagine that those words occur regularly in a regular corpus.</p> <p>However, if we look at the last part of the equation, which is the probability of the word \"tea\" given the words \"The teacher drinks\", we can imagine that they do not occur very often in a regular corpus, and thus, the probability of the sentence is very low.</p> \\[ P(\\text{tea} | \\text{The teacher drinks}) = \\frac{\\text{freq}(\\text{The teacher drinks tea})}{\\text{freq}(\\text{The teacher drinks})} \\] <p>We can easily construct sentences that are very unlikely to occur in a regular corpus.</p> <p>The teacher drinks tea \ud83c\udf75 and eats pizza \ud83c\udf55 and afterwards the teacher builds a house \ud83c\udfe0</p> <p>In such cases, the nominator and the denominator of the formula will be zero.</p>"},{"location":"lectures/language_models/#markov-assumption","title":"Markov Assumption","text":"<p>To overcome this problem, we can use the Markov assumption.</p> <p>In general, the Markov assumption says that the probability of the next event only depends on the previous event. So we only look at the previous event, and ignore the rest.</p> \\[ P(D | A, B, C) \\approx P(D | C) \\] <p>Transferring this to sentences, i.e. sequences of words, the Markov assumption says that the probability of the next word only depends on the previous word.</p> \\[ P(w_i | w_{i-1}, w_{i-2}, \\ldots, w_1) \\approx P(w_i | w_{i-1}) \\] <p>That means, we can only look at the previous word, and ignore the rest.</p> <p>Generalizing this to N-grams, we can write the Markov assumption for N-grams as:</p> \\[ P(w_i | w_{i-1}, w_{i-2}, \\ldots, w_1) \\approx P(w_i | w_{i-N+1}^{i-1}) \\] <p>Example</p> \\[ P(\\text{The teacher drinks tea}) \\approx P(\\text{The}) \\cdot P(\\text{teacher} | \\text{The}) \\cdot P(\\text{drinks} | \\text{teacher}) \\cdot P(\\text{tea} | \\text{drinks}) \\] <p>Tip</p> <p>The Markov assumption is a simplifying assumption that allows us to estimate the probability of a word given the previous \\(N\\) words, without having to consider the entire history of the sentence.</p> <p>Andrey Markov</p> <p>The Markov assumption is named after the Russian mathematician Andrey Markov (1856-1922).</p> <p></p> <p>He was the first to study the theory of stochastic processes, and he discovered the Markov chains, which are a special case of the Markov assumption.</p> <p>Markov chains are used in many applications, such as:</p> <ul> <li>Google PageRank</li> <li>Hidden Markov Models</li> <li>Markov Decision Processes</li> </ul> <p>Using the Markov assumption for bigrams, only the previous word is considered:</p> \\[ P(w_i | w_{i-1}, w_{i-2}, \\ldots, w_1) \\approx P(w_i | w_{i-1}) \\] <p>For N-grams, only the previous \\(N-1\\) words are considered:</p> \\[ P(w_i | w_{i-1}, w_{i-2}, \\ldots, w_1) \\approx P(w_i | w_{i-N+1}^{i-1}) \\] <p>Notation</p> <p>Recall the notation for a sequence of words:</p> \\[ P(w_i | w_{i-1}, w_{i-2}, \\ldots, w_1) = P(w_i | w_{1}^{i-1}) \\] <p>Using the Markov assumption, we can rewrite the probability of a sequence of \\(n\\) words as the product of conditional probabilities of the words and their immediate predecessors:</p> \\[ \\begin{align} P(w_1^n) &amp;= P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_2) \\cdot \\ldots \\cdot P(w_n | w_{n-1}) \\\\ &amp;= \\prod_{i=1}^n P(w_i | w_{i-1}) \\end{align} \\] <p>For N-grams, the probability of a sequence of \\(n\\) words is the product of conditional probabilities of the words and their \\(N-1\\) immediate predecessors:</p> \\[ \\begin{align} P(w_1^n) &amp;= P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1^2) \\cdot \\ldots \\cdot P(w_n | w_{n-N+1}^{n-1}) \\\\ &amp;= \\prod_{i=1}^n P(w_i | w_{i-N+1}^{i-1}) \\end{align} \\] <p>Notation</p> <p>Note that</p> <ul> <li>\\(n\\) is the number of words in the sequence, while</li> <li>\\(N\\) is the number of words in the N-gram.</li> </ul> <p>Markov Assumption</p> <p>Whenn applying the Markov assumption to N-grams, we can say that the probability of a word only depends on its \\(N-1\\) immediate predecessors. That means, only the last \\(N\\) words are relevant, and the rest can be ignored.</p> <p>Naive Bayes vs. Markov Assumption</p> <p>As you may recall, this is in contrast with Naive Bayes where we approximated sentence probability without considering any word history.</p> <p>Example</p> <p>Given the folloing probabilities:</p> N-gram Probability \\(P(\\text{Mary})\\) 0.1 \\(P(\\text{likes})\\) 0.2 \\(P(\\text{cats})\\) 0.3 \\(P(\\text{Mary} \\vert \\text{likes})\\) 0.2 \\(P(\\text{likes} \\vert \\text{Mary})\\) 0.3 \\(P(\\text{cats} \\vert \\text{likes})\\) 0.1 \\(P(\\text{likes} \\vert \\text{cats})\\) 0.4 <p>We can calculate the approximated probability of the sentence</p> <p>Mary likes cats</p> <p>using a bigram language model as follows:</p> \\[ \\begin{align} P(\\text{Mary likes cats}) &amp;= P(\\text{Mary}) \\cdot P(\\text{likes} | \\text{Mary}) \\cdot P(\\text{cats} | \\text{likes}) \\\\ &amp;= 0.1 \\cdot 0.3 \\cdot 0.1 \\\\ &amp;= 0.003 \\end{align} \\]"},{"location":"lectures/language_models/#start-and-end-of-sequences","title":"Start and End of Sequences","text":"<p>When we calculate the probability of a sentence using N-grams, we need to take a closer look at the first and last word of the sentence.</p> <p>At the beginning of a sentence, we do not have any context, so we cannot calculate the N-gram probability of the first word in a sentence.</p> <p>To overcome this problem, we can add \\(N-1\\) start tokens &lt;s&gt; at the beginning of the sentence.</p> <p>Example</p> <p>Given the sentence:</p> <p>The teacher drinks tea</p> <p>If we want to calculate the bigram probability of the sentence, we can add a start token &lt;s&gt; at the beginning of the sentence:</p> <p>&lt;s&gt; The teacher drinks tea</p> <p>We can then calculate the bigram probability of the sentence as follows:</p> \\[ P(\\text{&lt;s&gt; The teacher drinks tea}) = P(\\text{The} | \\text{&lt;s&gt;}) \\cdot P(\\text{teacher} | \\text{The}) \\cdot P(\\text{drinks} | \\text{teacher}) \\cdot P(\\text{tea} | \\text{drinks}) \\] <p>If we want to calculate the trigram probability of the sentence, we can add two start tokens &lt;s&gt; at the beginning of the sentence:</p> <p>&lt;s&gt; &lt;s&gt; The teacher drinks tea</p> <p>We can then calculate the trigram probability of the sentence as follows:</p> \\[ P(\\text{&lt;s&gt; &lt;s&gt; The teacher drinks tea}) = P(\\text{The} | \\text{&lt;s&gt; &lt;s&gt;}) \\cdot P(\\text{teacher} | \\text{&lt;s&gt; The}) \\cdot P(\\text{drinks} | \\text{The teacher}) \\cdot P(\\text{tea} | \\text{teacher drinks}) \\] <p>Similarly we need to add an end token &lt;/s&gt; at the end of the sentence.</p> <p>However, we only need to add a single end token &lt;/s&gt; at the end of the sentence, because we only need to calculate the probability of the sentence ending given its last \\(N-1\\) words.</p> <p>Example</p> <p>Given the sentence:</p> <p>The teacher drinks tea</p> <p>If we want to calculate the bigram probability of the sentence, we can add one start token &lt;s&gt; at the beginning of the sentence, and one end token &lt;/s&gt; at the end of the sentence:</p> <p>&lt;s&gt; The teacher drinks tea &lt;/s&gt;</p> <p>Since the last word in the sentence is \"tea\", we only need to calculate the probability of the word \"tea\" being the last word in the sentence, which is given by:</p> \\[ P(\\text{&lt;/s&gt;} | \\text{tea}) \\] <p>So we can calculate the bigram probability of the sentence as follows:</p> \\[ P(\\text{&lt;s&gt; The teacher drinks tea &lt;/s&gt;}) = P(\\text{The} | \\text{&lt;s&gt;}) \\cdot P(\\text{teacher} | \\text{The}) \\cdot P(\\text{drinks} | \\text{teacher}) \\cdot P(\\text{tea} | \\text{drinks}) \\cdot P(\\text{&lt;/s&gt;} | \\text{tea}) \\] <p>Start and End Tokens</p> <p>To calculate N-gram probabilities at the beginning and end of sequences, we need to add \\(N-1\\) start tokens &lt;s&gt; at the beginning of the sentence, and a single end token &lt;/s&gt; at the end of the sentence.</p>"},{"location":"lectures/language_models/#count-matrix","title":"Count Matrix","text":"<p>The count matrix captures the counts of all N-grams in the corpus.</p> <ul> <li>the rows represent the unique (N-1)-grams in the corpus</li> <li>the columns represent the unique words in the corpus</li> </ul> <p>Info</p> <p>Recap the formula for the N-gram probability, where the counts of an N-gram is repesented by the numerator:</p> \\[ \\text{freq}(w_{i-N+1}^{i-1},w_i) \\] <p>Tip</p> <p>The count matrix tells us how often a word occurs after a certain (N-1)-gram. It is a variant of the co-occurrence matrix we learned in the lecture about vector space models.</p> <p>You can also imagine sliding a window of size \\(N\\) over the whole corpus, and incrementing the counts in the matrix every time the window moves.</p> <p>Bigram Count Matrix</p> <p>Given the corpus:</p> <p>I study I learn</p> <p>If we look at the bigram \"study I\", we can see that it appears once in the corpus.</p> <p>That means, for the row \"study\", that is the (N-1)-gram, and for the column \"I\", that is the word that follows the (N-1)-gram, the count is one.</p> <p>The complete count matrix for bigrams looks as follows (note that we need to add the start and end tokens):</p> &lt;s&gt; I study learn &lt;/s&gt; &lt;s&gt; 0 1 0 0 0 I 0 0 1 1 0 study 0 1 0 0 0 learn 0 0 0 0 1 &lt;/s&gt; 0 0 0 0 0 <p>Trigram Count Matrix</p> <p>Given the corpus:</p> <p>I study I learn</p> <p>If we look at the trigram \"study I learn\", we can see that it appears once in the corpus.</p> <p>That means, for the row \"study I\", that is the (N-1)-gram, and for the column \"learn\", that is the word that follows the (N-1)-gram, the count is one.</p> <p>The complete count matrix for trigrams looks as follows (note that we need to add the start and end tokens):</p> &lt;s&gt; I study learn &lt;/s&gt; &lt;s&gt; &lt;s&gt; 0 1 0 0 0 &lt;s&gt; I 0 0 1 0 0 I study 0 1 0 0 0 study I 0 0 0 1 0 I learn 0 0 0 0 1 learn &lt;/s&gt; 0 0 0 0 0"},{"location":"lectures/language_models/#probability-matrix","title":"Probability Matrix","text":"<p>To get the probabilities, we need to divide the counts by the total number of occurrences of the (N-1)-gram.</p> <p>This is the same as dividing each row by the sum of the row.</p> <p>Info</p> <p>Recall the formula for the N-gram probability, where the counts of an (N-1)-gram is represented by the denominator:</p> \\[ \\text{freq}(w_{i-N+1}^{i-1}) \\] <p>Note that we want to know the probability of the word \\(w_i\\) given the (N-1)-gram \\(w_{i-N+1}^{i-1}\\) has already occurred.</p> <p>Example</p> <p>Given the corpus:</p> <p>I study I learn</p> <p>The count matrix for bigrams looks as follows:</p> &lt;s&gt; I study learn &lt;/s&gt; \\(\\sum\\) &lt;s&gt; 0 1 0 0 0 1 I 0 0 1 1 0 2 study 0 1 0 0 0 1 learn 0 0 0 0 1 1 &lt;/s&gt; 0 0 0 0 0 0 <p>Note that last column, where we sum up the counts of each row.</p> <p>We can now calculate the probability matrix by dividing each cell by the sum of the row:</p> &lt;s&gt; I study learn &lt;/s&gt; &lt;s&gt; 0 1 0 0 0 I 0 0 0.5 0.5 0 study 0 1 0 0 0 learn 0 0 0 0 1 &lt;/s&gt; 0 0 0 0 0"},{"location":"lectures/language_models/#language-model","title":"Language Model","text":"<p>A language model is a model that assigns a probability to a sequence of words. It can be used to predict the next word in a sentence, or to calculate the probability of a sentence.</p> <p>Tip</p> <p>In a simple form, a language model can be as simple as a script that operates on the probability matrix.</p>"},{"location":"lectures/language_models/#predicting-the-next-word","title":"Predicting the Next Word","text":"<p>Predicting the next word of a sentence, as in auto-complete, works as follows:</p> <ul> <li>extract the last (N-1)-gram from the sentence</li> <li>find the word with the highest probability for the given the (N-1)-gram in the probability matrix</li> </ul> <p>Question</p> <p>Given the bigram probability matrix from above, what is the most likely element to follow the word \"study\"?</p> <p>Coding Example</p> <p>Given a probability matrix as a pandas dataframe, where the index is the (N-1)-gram, and the columns are the words, in a trigram language model, we could predict the next word as follows:</p> <pre><code>def predict_next_word(sentence):\n    # extract the last (N-1)-gram from the sentence\n    last_n_gram = sentence.split()[-2:]\n    # find the word with the highest probability for the given the (N-1)-gram in the probability matrix\n    return probability_matrix.loc[last_n_gram].idxmax()\n</code></pre>"},{"location":"lectures/language_models/#sentence-probability","title":"Sentence Probability","text":"<p>To predict the probility of a sentence:</p> <ul> <li>split sentence into N-grams and extract the probability of each N-gram from the probability matrix</li> <li>multiply the probabilities of the N-grams to get the probability of the sentence</li> </ul> <p>Example</p> <p>Let's assume we want to calculate the probability of the sentence \"I learn\" using the bigram probability matrix from above:</p> \\[ \\begin{align} P(\\text{&lt;s&gt; I learn &lt;/s&gt;}) &amp;= P(\\text{I} | \\text{&lt;s&gt;}) \\cdot P(\\text{learn} | \\text{I}) \\cdot P(\\text{&lt;/s&gt;} | \\text{learn}) \\\\ &amp;= 1 \\cdot 0.5 \\cdot 1 \\\\ &amp;= 0.5 \\end{align} \\]"},{"location":"lectures/language_models/#text-generation","title":"Text Generation","text":"<p>Using the probability matrix, we can also generate text from scratch or by providing a small hint.</p> <ul> <li>Choose a sentence start (or provide a hint)</li> <li>Choose the next word based on the previous (N-1)-gram</li> <li>Repeat until the end of the sentence &lt;/s&gt; is reached</li> </ul> <p>Example</p> <p>Given the following corpus:</p> <p>&lt;s&gt; Lyn drinks chocolate &lt;/s&gt; &lt;s&gt; John drinks tea &lt;/s&gt; &lt;s&gt; Lyn eats chocolate &lt;/s&gt;</p> <p>A generated text could be:</p> <p>Lyn drinks tea</p> <p>because:</p> <ol> <li>(&lt;s&gt; Lyn) or (&lt;s&gt; John)</li> <li>(Lyn eats) or (Lyn drinks)</li> <li>(drinks tea) or (drinks chocolate)</li> <li>(tea &lt;/s&gt;)</li> </ol>"},{"location":"lectures/language_models/#log-probability","title":"Log Probability","text":"<p>When we calculate the probability of a sentence, we multiply the probabilities of the N-grams:</p> \\[ P(w_1^n) = \\prod_{i=1}^n P(w_i | w_{i-N+1}^{i-1}) \\] <p>Since probability values are in the range \\([0,1]\\), the product of many probabilities can become very small, which can lead to numerical underflow.</p> <p>To avoid this problem, we can use the log probability instead:</p> \\[ \\log P(w_1^n) = \\sum_{i=1}^n \\log P(w_i | w_{i-N+1}^{i-1}) \\] <p>Tip</p> <p>Recall the logarithmic property:</p> \\[ \\log(ab) = \\log(a) + \\log(b) \\]"},{"location":"lectures/language_models/#perplexity","title":"Perplexity","text":"<p>In theory, we could evaluate our language model by calculating the probability of all words in the test set, i.e. the probability of the test set. In practice, a variation of this is used, called perplexity.</p> <p>Perplexity is a commonly used metric to evaluate language models.</p> <p>You can interpret perplexity as a measure of</p> <ul> <li>how complex a text is</li> <li>how well a language model predicts a text</li> <li>if a text looks like it was written by a human or by a computer.</li> </ul> <p>A text with a low perplexity will sound more natural to a human than a text with a high perplexity.</p> <p>The more information the N-gram gives us about the word sequence, the lower the perplexity score, as the following data based on an experiment by Jurafsky &amp; Martin shows:</p> N-gram Perplexity Unigram 962 Bigram 170 Trigram 109 <p>Example</p> <p>The following table shows three sentences randomly generated from three n-gram models computed from 40 million words of the Wall Street Journal.</p> N-gram Generated Text Unigram Months the my and issue of year foreign new exchange\u2019s september were recession exchange new endorsed a acquire to six executives Bigram Last December through the way to preserve the Hudson corporation N. B. E. C. Taylor would seem to complete the major central planners one point five percent of U. S. E. has already old M. X. corporation of living on information such as more frequently fishing to keep her Trigram They also point to ninety nine point six billion dollars from two hundred four oh six three percent of the rates of interest stores as Mexico and Brazil on market conditions <p>Example from Speech and Language Processing, 3rd ed. draft, by Jurafsky &amp; Martin, 2023.</p> <p>The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words.</p> <p>For a unigram model, the perplexity of a test set \\(W\\) that consists of \\(m\\) words, is defined as:</p> \\[ \\begin{align} \\text{perplexity}(W) &amp;= P(w_1^m)^{-\\frac{1}{m}} \\\\ &amp;= \\sqrt[m]{\\frac{1}{P(w_1^m)}} \\\\ \\end{align} \\] <p>Generalizing this to N-grams, the perplexity of a test set \\(W\\) that consists of \\(m\\) words, is defined as:</p> \\[ \\text{perplexity}(W) = \\sqrt[m]{\\prod_{i=1}^m \\frac{1}{P(w_i | w_{i-N+1}^{i-1})}} \\] <p>Since the perplexity is the inverse probability of the set set, the lower the perplexity, the better the model.</p> <p>Example</p> <p>Imagine a test set with 100 words, and a really good language model that assigns a probability of \\(P(W)=0.9\\) to the test set.</p> \\[ \\text{perplexity}(W) = 0.9^{-\\frac{1}{100}} \\approx 1.001054 \\] <p>Now, imagine a language model with a poor performance of only \\(P(W)=10^{-250}\\). The perplexity of this model is:</p> \\[ \\text{perplexity}(W) = (10^{-250})^{-\\frac{1}{100}} \\approx 316.227766 \\] <p>Note</p> <p>Sometimes the log perplexity is used instead:</p> \\[ \\text{log perplexity} = \\frac{1}{m} \\sum_{i=1}^m \\log P(w_i | w_{i-N+1}^{i-1}) \\] <p>Info</p> <p>The perplexity score of GPT-2 is reported to be 18.34 on the WikiText-2. There is no official perplexity score published by OpenAI for later versions of GPT, but according to this source, GPT-3.5 achieves a perplexity score of 4.5 while GPT-4 achieves a perplexity score of 2.6.</p>"},{"location":"lectures/language_models/#out-of-vocabulary-oov-words","title":"Out of Vocabulary (OOV) Words","text":"<p>In some cases we need to deal with words that we haven't seen before. Such words are called OOV words, and are usually replaced by a special token <code>&lt;unk&gt;</code>.</p> <p>We need to think about how to make predictions for words that we have not seen in the training corpus. What would be the N-gram probability of a word that is not in the corpus?</p> <p>Example</p> <p>Let's assume we have a trigram language model, and we want to predict the next word for the two given words <code>(I sip)</code>. Furthermore we assume that we have seen the bigram <code>(I drink)</code> but not the bigram <code>(I sip)</code>.</p> <p>Instead of predicting the next word for the bigram <code>(I sip)</code>, we would predict the next word for the bigram <code>(I &lt;unk&gt;)</code>.</p> <p>Ultimately, the use of the <code>&lt;unk&gt;</code> token depends on the vocabulary size. If there are limitations regarding the vocabulary size, we could use the following strategies:</p> <ul> <li>Minimum word frequency: only add words to the vocabulary that occur at least \\(n\\) times in the corpus</li> <li>Maximum vocabulary size: only add the \\(n\\) most frequent words to the vocabulary</li> </ul> <p>Any other words would be replaced by <code>&lt;unk&gt;</code>.</p> <p>Open vs. Closed Vocabulary</p> <p>A closed vocabulary (aka logical vocabulary) consists of a fixed set of words, whereas an open vocabulary (aka non-logical vocabulary) means we may encounter words that we have not seen before.</p> <p>Warning</p> <p>Using a lot of OOV words can influence the perplexity score. If there are a lot of OOV words in the test set, the model will predict them with a high probability, which will result in a low perplexity score.</p> <p>This means the model with generate sentences that contain a lot of <code>&lt;unk&gt;</code> tokens.</p> <p>For this reason, perplexities should only be compared across language models with the same vocabularies.</p>"},{"location":"lectures/language_models/#smoothing","title":"Smoothing","text":"<p>In the previous section, we saw how to address missing words. However, there is another problem of missing information that we need to address: missing N-grams.</p> <p>In other words, there can be N-grams that are made from words that occur in the corpus, but the N-gram itself has not been seen in the corpus.</p> <p>The probability of such N-grams would be zero, and thus, considered \"impossible\" by the language model.</p> <p>Example</p> <p>Given the corpus:</p> <p>&lt;s&gt; Lyn drinks chocolate &lt;/s&gt; &lt;s&gt; John drinks tea &lt;/s&gt; &lt;s&gt; Lyn eats chocolate &lt;/s&gt;</p> <p>The words \"John\" and \"eats\" occur in the corpus, but the bigram \"John eats\" has not been seen in the corpus.</p> <p>Thus, the frequency of the bigram \"John eats\" would be zero, and its probability would be zero as well.</p>"},{"location":"lectures/language_models/#laplacian-smoothing","title":"Laplacian Smoothing","text":"<p>We have already seen Laplacian smoothing previously. If we apply it to N-grams, the formula looks as follows:</p> \\[ P(w_i | w_{i-N+1}^{i-1}) = \\frac{\\text{freq}(w_{i-N+1}^{i-1},w_i) + 1}{\\text{freq}(w_{i-N+1}^{i-1}) + V} \\] <p>Info</p> <p>Since there are \\(V\\) words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra \\(V\\) observations.</p> <p>Or in its generalized form:</p> \\[ P(w_i | w_{i-N+1}^{i-1}) = \\frac{\\text{freq}(w_{i-N+1}^{i-1},w_i) + k}{\\text{freq}(w_{i-N+1}^{i-1}) + k \\cdot V} \\] <p>Info</p> <p>The Laplacian smoothing is also called add-one smoothing. In its general form, it is called add-k smoothing.</p> <p>Hyperparameter \\(k\\)</p> <p>Since we are dealing with small values, adding \\(k=1\\) can be too much and negatively affect the probability estimates.</p> <p>In practice, this means we need to find a good value for \\(k\\). As a hyperparameter, it needs to be learned from the validation set.</p> <p>Other Smoothing Techniques</p> <p>There are a couple of advanced smoothing techniques, such as Good-Turing smoothing or Kneser-Ney smoothing.</p>"},{"location":"lectures/language_models/#backoff","title":"Backoff","text":"<p>Another smoothing approach to deal with N-grams that do not occur in the corpus is to use information about lower order N-grams. This is called backoff.</p> <p>If the N-gram we need has zero counts, we can approximate it by backing off to the (N-1)-gram. If the (N-1)-gram has zero counts, we can approximate it by backing off to the (N-2)-gram, and so on. We continue backing off until we reach something that has non-zero counts.</p> <p>Info</p> <p>In other words, sometimes using less context is a good thing, helping to generalize more for contexts that the model hasn\u2019t learned much about.</p> <p>Here is the general formula for backoff:</p> \\[ P_{bo}(w_i | w_{i-N+1}^{i-1}) = \\begin{cases} P(w_i | w_{i-N+1}^{i-1}) &amp; \\text{if } \\text{freq}(w_{i-N+1}^{i}) &gt; 0 \\\\ \\alpha P(w_i | w_{i-N+2}^{i-1}) &amp; \\text{otherwise} \\end{cases} \\] <p>Discount</p> <p>If we replace an unseen N-gram which has zero probability with a lower order N-gram, we would be adding probability mass, and the total probability assigned to all possible strings by the language model would be greater than 1!</p> <p>To account for this, we need to discount the probability mass from the higher order N-gram and redistribute it to the lower order N-grams.</p> <p>This is expressed by the parameter \\(\\alpha\\) in the formula above. There are different ways to set the value of \\(\\alpha\\).</p> <p>Katz Backoff</p> <p>A popular smoothing technique is the Katz backoff by Katz (1987). It involves a quite detailed computation for estimating the \\(\\alpha\\) parameter.</p> <p>Stupid Backoff</p> <p>A simple yet efficient approach is the Stupid Backoff by Brants et al. (2007), where the \\(\\alpha\\) parameter was heuristically set to 0.4.</p> <p>Originally, the authors thought that such simple approach cannot possibly be good, but it turned out to be very effective and inexpensive.</p> <p>Example</p> <p>Given the corpus:</p> <p>&lt;s&gt; Lyn drinks chocolate &lt;/s&gt; &lt;s&gt; John drinks tea &lt;/s&gt; &lt;s&gt; Lyn eats chocolate &lt;/s&gt;</p> <p>The probability of the sentence \"John drinks chocolate\" cannot be directly estimated:</p> \\[ P(\\text{chocholate} | \\text{John drinks}) = ? \\] <p>However, we can use the bigram probability of \\(P(\\text{chocholate} | \\text{drinks})\\) to estimate the probability of the sentence.</p> <p>Using stupid backoff, we can estimate the probability as follows:</p> \\[ P(\\text{chocholate} | \\text{John drinks}) = 0.4 \\cdot P(\\text{chocholate} | \\text{drinks}) \\]"},{"location":"lectures/language_models/#interpolation","title":"Interpolation","text":"<p>We can even make use of backoff in our model and always mix the probability of the higher order N-gram with the lower order N-gram, even if the higher order N-gram has non-zero counts.</p> <p>This is called interpolation.</p> <p>Question</p> <p>What does this mean for building such a model?</p> <p>Here is the general formula for linear interpolation, where each N-gram is weighted by a parameter \\(\\lambda\\):</p> \\[ P_{inter}(w_i | w_{i-N+1}^{i-1}) = \\lambda_1 P(w_i | w_{i-N+1}^{i-1}) + \\lambda_2 P(w_i | w_{i-N+2}^{i-1}) + \\ldots + \\lambda_n P(w_i) \\] <p>such that</p> \\[ \\sum_{i=1}^n \\lambda_i = 1 \\] <p>Note</p> <p>The \\(\\lambda\\) parameters are hyperparameters that need to be learned from the validation set.</p> <p>Example</p> <p>Given the corpus:</p> <p>&lt;s&gt; Lyn drinks chocolate &lt;/s&gt; &lt;s&gt; John drinks tea &lt;/s&gt; &lt;s&gt; Lyn eats chocolate &lt;/s&gt;</p> <p>Let's assume we have an interpolation model and trained the following \\(\\lambda\\) parameters:</p> N-gram \\(\\lambda\\) Unigram 0.1 Bigram 0.2 Trigram 0.7 <p>Using an interpolation model, we would estimate the probability of the sentence \"Lyn drinks chocolate\" as follows:</p> \\[ \\begin{align} P(\\text{Lyn drinks chocolate}) &amp;= \\lambda_1 P(\\text{chocolate} | \\text{Lyn drinks}) + \\lambda_2 P(\\text{chocolate} | \\text{drinks}) + \\lambda_3 P(\\text{chocolate}) \\\\ &amp;= 0.7 \\cdot P(\\text{chocolate} | \\text{Lyn drinks}) + 0.2 \\cdot P(\\text{chocolate} | \\text{drinks}) + 0.1 \\cdot \\lambda_3 P(\\text{chocolate}) \\\\ \\end{align} \\]"},{"location":"lectures/language_models/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Language models are able to assign a probability to a sequence of words, or to predict the next word in a sentence. This enables many intersting NLP applications, such as auto-complete, text generation, or machine translation.</li> <li>this lecture specifically looked at N-gram language models. An N-gram is a sequence of \\(N\\) words, and instead of estimating the probability of a word given the entire history of the sentence, the N-gram language model only considers the last \\(N-1\\) words.</li> <li>N-gram language models are based on the Markov assumption, which says that the probability of a word only depends on its \\(N-1\\) immediate predecessors.</li> <li>Building the language model means filling the probability matrix, where the rows represent the unique (N-1)-grams in the corpus, and the columns represent the unique words in the corpus.</li> <li>When building the probability matrix, we need to add \\(N-1\\) start tokens &lt;s&gt; at the beginning of the sentence, and a single end token &lt;/s&gt; at the end of the sentence.</li> <li>Having the probability matrix available, we can already predict the next word in a sentence, or calculate the probability of a sentence, simply by performing several lookups in the probability matrix.</li> <li>Also here, we usually make use of the log probability to avoid numerical underflow.</li> <li>To evaluate the performance of a language model, we can calculate the perplexity score. The lower the perplexity, the better the model.</li> <li>To deal with missing information, we introduce a special token for OOV words. For missing N-grams, we can make use of smoothing techniques, such as Laplacian smoothing, backoff, or interpolation.</li> </ul>"},{"location":"lectures/logistic_regression/","title":"Logistic Regression","text":"<p>Logistic regression is a supervised learning algorithm that can be used for classification.</p> <p>In classification, the goal is to predict a discrete class label, such as \"spam\" or \"not spam\".</p> <p>Info</p> <p>In the assignment, we will conduct a sentiment analysis on tweets using logistic regression, and try to predict whether a tweet has an overall positive or negative meaning.</p>"},{"location":"lectures/logistic_regression/#supervised-learning","title":"Supervised Learning","text":"<p>In supervised learning, the goal is to learn a function, i.e. the parameters of a function, that maps an input to an output based on example input-output pairs.</p> <p>When working with text data, we need to make sure to apply the required preprocessing steps in order to extract the required features from the text data.</p> <p></p>"},{"location":"lectures/logistic_regression/#the-sigmoid-function","title":"The Sigmoid Function","text":"<p>The prediction function in logistic regression is a sigmoid function that transforms the linear combination of input features \\(z\\) into a probability value between 0 and 1.</p> <p>The linear combination \\(z\\) of the input features of a given sample \\(x\\) and their weights \\(\\theta\\) is given as</p> \\[ z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n, \\] <p>or in its vectorized form</p> \\[ z = \\theta^T x, \\] <p>where</p> <ul> <li>\\(\\theta\\) is the vector of weights, and</li> <li>\\(x\\) is the vector of input features of a given sample.</li> </ul> <p>When applying the sigmoid function to \\(z\\), the output will be a probability value between 0 and 1.</p> \\[ h(z) = \\frac{1}{1 + e^{-z}} \\] <p>As \\(z\\) approaches \\(-\\infty\\), the denominator of the sigmoid function gets larger and larger and as a result, \\(h(z)\\) approaches 0.</p> <p>On the other hand, as \\(z\\) approaches \\(\\infty\\), the denominator of the sigmoid function gets closer to one and \\(h(z)\\) approaches 1.</p> <p></p> <p>Note</p> <p>The term \\(z\\) is also called the log-odds or logit. It represent the logarithm of the odds of the event occurring.</p>"},{"location":"lectures/logistic_regression/#training","title":"Training","text":"<p>In logistic regression, the goal is typically binary classification, where the algorithm learns to classify input data into one of two classes. During the training phase, the algorithm adjusts its parameters (weights and bias) based on the input data in order to minimize the difference between its predictions and the actual labels in the training dataset.</p> <p>The process involves using an optimization algorithm (usually gradient descent) to find the optimal values for the parameters that minimize a cost function. The cost function measures the difference between the predicted outputs and the true labels. The training process continues iteratively until the algorithm converges to a set of parameters that yield satisfactory predictions on the training data.</p>"},{"location":"lectures/logistic_regression/#gradient-descent","title":"Gradient Descent","text":"<p>Training in logistic regression is based on the gradient descent algorithm. Here is a brief overview of the steps involved:</p> <p></p> <ol> <li> <p>Initialize Parameters: Set the initial values for the weights \\(\\theta\\), considering the bias unit.     This is the starting point for the optimization process.</p> <p>In numpy, this can be done with the following code:</p> <pre><code>theta = np.zeros((n_features + 1, 1))\n</code></pre> </li> <li> <p>Calculate Predictions: Use the current parameter values to make predictions for each data point in the training set using the sigmoid function \\(h(z)\\).     This step produces the predicted probabilities for each example.</p> </li> <li> <p>Compute the Cost Function: Calculate the cost function \\(J(\\theta)\\), which measures the difference between the predicted probabilities and the actual class labels.</p> </li> <li> <p>Compute Gradients: Calculate the partial derivatives of the cost function with respect to each parameter.     These gradients indicate the direction and magnitude of the steepest increase in the cost function.</p> </li> <li> <p>Update Weights: Adjust the parameters in the direction opposite to the gradients to minimize the cost function.     This involves multiplying the gradients by a learning rate \\(\\alpha\\) and subtracting the result from the current parameter values.</p> </li> <li> <p>Repeat: Iterate steps 2-5 until the convergence criteria are met, such as reaching a maximum number of iterations or achieving a sufficiently small change in the cost function.</p> </li> </ol> <p>These steps represent the core of the gradient descent algorithm in the context of logistic regression.</p> <p>The goal is to iteratively update the parameters in the direction that minimizes the cost function, eventually reaching a set of parameters that optimally fit the training data.</p> <p>Learning Rate</p> <p>The learning rate \\(\\alpha\\) is a hyperparameter that controls the step size at each iteration while moving toward a minimum of the cost function. It is a crucial parameter in optimization algorithms that are used to train machine learning models.</p> <p>If the learning rate is too small, the algorithm may take a long time to converge or may get stuck in a local minimum. On the other hand, if the learning rate is too large, the algorithm may overshoot the minimum and fail to converge</p> <p>Info</p> <p>The terms \"cost function\" and \"loss function\" are often used interchangeably. In most contexts, they refer to the same concept.</p>"},{"location":"lectures/logistic_regression/#cost-function","title":"Cost Function","text":"<p>The cost function measures the difference between the predicted labels and the actual class labels (aka cost).</p> <p>The cost function \\(J(\\theta)\\) in logistic regression is given as</p> \\[ J(\\theta) = - \\frac{1}{m}(\\mathbf{y}^T \\log(\\mathbf{h}) + (1 - \\mathbf{y})^T \\log(1 - \\mathbf{h})) \\] <p>where</p> <ul> <li>\\(m\\) is the number of training samples,</li> <li>\\(\\mathbf{y}\\) is the vector of training labels, and</li> <li>\\(\\mathbf{h}\\) is the vector of outputs of the sigmoid function for all samples.</li> </ul> <p>A low cost means good predictions, so the goal is to minimize the cost function \\(J(\\theta)\\).</p> <p>When performing gradient descent, the cost should decrease with every iteration.</p> <p></p> <p>Info</p> <p>For our purposes, we do not bother the derivation of the cost function. However, if you are interested, you can read more about it here.</p>"},{"location":"lectures/logistic_regression/#compute-gradient-and-update-weights","title":"Compute Gradient and Update Weights","text":"<p>Computing the gradient and updating the weights can happen in one step.</p> <p>We want to find the direction of the steepest increase of the cost function \\(J(\\theta)\\), this is why we need to calculate the gradient of the cost function \\(J(\\theta)\\).</p> <p>To do that, we need the partial derivative of the cost function \\(J(\\theta)\\) with respect to the weights \\(\\theta\\).</p> <p>A vectorized form of the gradient of the cost function \\(J(\\theta)\\) and the weight updates is given as</p> \\[ \\theta = \\theta - \\frac{\\alpha}{m} \\mathbf{X}^T (\\mathbf{h - y}) \\] <p>where</p> <ul> <li>\\(m\\) is the number of training samples,</li> <li>\\(\\alpha\\) is the learning rate,</li> <li>\\(\\mathbf{X}\\) is the matrix of input features of all samples, and</li> <li>\\(\\mathbf{h}\\) is the vector of outputs of the sigmoid function for all samples.</li> <li>\\(\\mathbf{y}\\) is the vector of training labels.</li> </ul> <p>So the term \\(\\mathbf{h - y}\\) is essentially the vector of errors, representing the difference between the predicted values and the actual values. For the equation to work, we need to transpose the matrix of input features \\(\\mathbf{X}\\).</p>"},{"location":"lectures/logistic_regression/#testing","title":"Testing","text":"<p>When we reached testing phase, we have already trained the logistic regression model and have the weights \\(\\theta\\) available.</p> <p>To test a sample, we need to extract the features \\(x\\) from the sample, and then pass it to the prediction function of the logistic regression model.</p> <p>To test the logistic regression model, for all samples in the test set, the following steps are performed:</p> <ol> <li> <p>Calculate the linear combination \\(z\\) of the input features \\(x\\) and the weights \\(\\theta\\).</p> \\[ z = \\theta^T x \\] </li> <li> <p>Calculate the sigmoid function \\(h(z)\\).</p> </li> <li> <p>Determine the predicted label \\(\\hat{y}\\) based on the value of the sigmoid function \\(h(z)\\).</p> \\[ \\hat{y} = \\begin{cases} 1 &amp; \\text{if } h(z) \\geq 0.5 \\\\ 0 &amp; \\text{if } h(z) &lt; 0.5 \\end{cases} \\] </li> <li> <p>Compare the predicted labels with the test labels \\(y\\).</p> \\[ \\hat{y} == y \\] </li> <li> <p>Calculate performance metrics, e.g. accuracy.</p> </li> </ol> <p>The figure in the supervised learning section also indicates the steps involved in testing process.</p> <p>Warning</p> <p>Before extracting features from a test sample, we need to apply the same preprocessing steps as we did during training.</p>"},{"location":"lectures/logistic_regression/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Logistic regression is a supervised learning algorithm that can be used for classification.</li> <li>The prediction function in logistic regression is a sigmoid function that outputs a probability value between 0 and 1.</li> <li>The cost function measures the difference between the predicted labels and the actual class labels.</li> <li>The goal is to minimize the cost function.</li> <li>Training in logistic regression is based on the gradient descent algorithm.</li> </ul>"},{"location":"lectures/minimum_edit_distance/","title":"Minimum Edit Distance","text":"<p>Minimum edit distance is another similarity measure that can be used to find strings that are close to a given string.</p> <p>A popular use in NLP is in autocorrection systems, where it is used to suggest corrections for misspelled words. Autocorrect is a feature that is built into many devices and applications, such as smartphones, email clients, and text processors.</p> <p>It does not always work as intended, and sometimes it can even be quite funny. You have probably seen examples like the following:</p> <p>Tip</p> <p>Find more funny autocorrect examples here and here.</p>"},{"location":"lectures/minimum_edit_distance/#edit-distance","title":"Edit Distance","text":"<p>Edit distance, also known as Levenshtein distance, is a measure of the similarity between two strings by calculating the minimum number of single-character edits required to change one string into the other. It provides a quantitative measure of how different or similar two strings are.</p> <p>The operations that can be performed are:</p> <ul> <li>Insertion</li> <li>Deletion</li> <li>Replacement</li> </ul> <p>For the minimum edit distance, we want to find the minimum number of operations that are needed to transform one string into another string.</p> <p>Minimum Edit Distance</p> <p>Given a string, the minimum edit distance is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.</p> <p>Info</p> <p>The term is named after the Soviet mathematician Vladimir Levenshtein, who first introduced the concept in 1965.</p> <p></p> <p>Note that there are other algorithms for calculating the edit distance, including the Wagner-Fischer algorithm, the Needleman-Wunsch algorithm, or the Jaro-Winkler algorithm.</p> <p>Also see NLTK's edit distance module for more information.</p> <p>Example</p> <p>Consider the words <code>kitten</code> \ud83d\udc08 and <code>sitting</code> \ud83e\uddd8. The edit distance between them is 3 because the following three operations can transform one into the other:</p> <ul> <li>Replace <code>k</code> with <code>s</code></li> <li>Replace <code>e</code> with <code>i</code></li> <li>Insert <code>g</code> at the end</li> </ul> <p>For this intuitive example, we do not consider the cost of each operation. We will look at that later.</p>"},{"location":"lectures/minimum_edit_distance/#applications","title":"Applications","text":"<p>The edit distance is used in various NLP applications, including:</p> <ul> <li> <p>Autocorrection / Spell checking: Minimum Edit Distance is often used in spell-checking algorithms to suggest corrections for misspelled words. By calculating the minimum edit distance between a misspelled word and candidate words in a dictionary, a spell checker can identify potential corrections.</p> </li> <li> <p>Information Retrieval: Minimum Edit Distance can be used in information retrieval systems to find similar words or phrases in a database. This is particularly useful in search engines when dealing with queries that may contain typos or slight variations.</p> </li> <li> <p>Plagiarism Detection: Minimum Edit Distance can be applied to compare and analyze text documents for plagiarism detection. By measuring the similarity between documents in terms of edit operations, it becomes possible to identify instances of copied or closely paraphrased content.</p> </li> <li> <p>OCR (Optical Character Recognition): OCR systems may use Minimum Edit Distance to recognize and correct errors in the recognized text. It helps improve the accuracy of converting scanned or photographed documents into machine-readable text.</p> </li> </ul> <p>But also in other fields, such as:</p> <ul> <li> <p>DNA Sequencing: In bioinformatics, Minimum Edit Distance is applied to compare DNA or RNA sequences. It helps identify the evolutionary relationships between different genetic sequences by quantifying the number of mutations needed to transform one sequence into another.</p> </li> <li> <p>Genealogy and Historical Linguistics: Minimum Edit Distance is used to study language evolution and historical relationships between languages by comparing words and phrases across different time periods.</p> </li> </ul> <p></p> <p>For this lecture, we will specifally look at how the Minimum Edit Distance can be used in autocorrection systems.</p> <p>Info</p> <p>The notebook associated with this lecture is the <code>minimum_edit_distance.ipynb</code> notebook in the <code>notebooks</code> folder.</p>"},{"location":"lectures/minimum_edit_distance/#autocorrect-process","title":"Autocorrect Process","text":"<p>The autocorrect process can be broken down into four steps:</p> <ol> <li>Identify an incorrect word</li> <li>Find strings \\(n\\) edit distance away</li> <li>Filter candidate words</li> <li>Calculate the word probabilities</li> </ol> <p></p> <p>Example</p> <p>Consider the following sentence:</p> <p>\"my deah friend\" \u274c</p> <ol> <li> <p>We can easily see that the word <code>deah</code> is incorrect. The intended word is probably <code>dear</code>.</p> <ul> <li>my dear friend \u2705</li> </ul> </li> <li> <p>We can find words that are 1 edit distance away from <code>deah</code>:</p> <p>Note that we do not care if the words are valid words or not. We just want to find words that are 1 edit distance away from <code>deah</code>.</p> <ul> <li>Replacements, e.g. <code>dear</code>, <code>dead</code>, <code>deas</code>, <code>yeah</code></li> <li>Insertions, e.g. <code>deahs</code>, <code>deaht</code>, <code>deahh</code></li> <li>Deletions, e.g. <code>dea</code>, <code>deh</code>, <code>eah</code></li> </ul> </li> <li> <p>We can filter out words that are not in the vocabulary.</p> <ul> <li><code>dear</code> \u2705</li> <li><code>dead</code> \u2705</li> <li><code>deal</code> \u2705</li> <li><code>deas</code> \u274c</li> <li>...</li> </ul> </li> <li> <p>We can calculate the word probabilities to find the most likely word.</p> <p>In our case, we can intuitively assume that <code>dear</code> is more likely than <code>dead</code>.</p> <ul> <li>My dear friend \u2705</li> <li>My dead friend \ud83d\ude31\ud83e\udddf</li> </ul> </li> </ol>"},{"location":"lectures/minimum_edit_distance/#identifying-incorrect-words","title":"Identifying Incorrect Words","text":"<p>For humans, it is usually quite easy to identify incorrect words. But for a computer, we need to find a way to identify incorrect words.</p> <p>The simplified assumption we can in the lecture is that if a word is not in the vocabulary, then it is probably a typo.</p> <pre><code>def identify_incorrect_words(words, vocab):\n    incorrect_words = []\n    for word in words:\n        if word not in vocab:\n            incorrect_words.append(word)\n    return incorrect_words\n</code></pre> <p>Note that the vocabulary is a set of words, i.e. it does not contain duplicates.</p> <pre><code>&gt;&gt;&gt; words = [\"happy\", \"sad\", \"excited\", \"happy\", \"sad\", \"happy\", \"cool\", \"funny\"]\n&gt;&gt;&gt; vocab = set(words)\n&gt;&gt;&gt; vocab\n{'happy', 'sad', 'excited', 'funny', 'cool'}\n</code></pre> <p>Info</p> <p>There are much more sophisticated techniques for identifying words that are probably incorrect by looking at the context of the word. However, in this lecture, we will only look at spelling errors, not grammatical or contextual errors.</p> <p>Example</p> <p>Consider the following sentence:</p> <ul> <li>Happy birthday to you, my dear friend \u2705</li> <li>Happy birthday to you, my deah friend \u274c</li> </ul> <p>This is as expected, since <code>deah</code> is not in the vocabulary. But since we can expect the word <code>deer</code> to be in the vocabulary, we would not identify the following sentence as incorrect:</p> <ul> <li>Happy birthday to you, my deer friend \u2705\ud83e\udd8c</li> </ul>"},{"location":"lectures/minimum_edit_distance/#finding-strings-within-edit-distance","title":"Finding Strings within Edit Distance","text":"<p>Next, we want to find strings that are within a certain edit distance \\(n\\) from a given string.</p> <p>An edit is a type of operation that we can perform on a string to change it into another string, and ss we've already learned, there are three types of edits:</p> <ul> <li>Insertion</li> <li>Deletion</li> <li>Replacement</li> </ul> <p>So the edit distance \\(n\\) tells us, how many operations one string is away from another string. We can combine these operations in any order.</p> <p>By combining these edits, we can find a list of all possible strings that are \\(n\\) edit distances away from a given string, regardless of wheter those strings are valid words or not.</p> <p>Info</p> <p>For autocorrect, we want to find strings that are close to the incorrect word. For autocorrect, a good guess would be to find strings that are not more than 3 edit distances away from the incorrect word.</p> <p>To implement the following operations, Python index slicing comes in handy. It allows you to access a subset of a sequence by specifying a range of indices.</p> <pre><code>&gt;&gt;&gt; word = \"play\"\n&gt;&gt;&gt; word[:2]\n'pl'\n&gt;&gt;&gt; word[2:]\n'ay'\n</code></pre>"},{"location":"lectures/minimum_edit_distance/#insertion","title":"Insertion","text":"<p>An insertion is when we insert a character into a string. It does not matter where we insert the character.</p> <pre><code>def insert_char(word, i, char):\n    return word[:i] + char + word[i:]\n</code></pre> <p>Example</p> <p>For the word <code>to</code></p> <ul> <li>if we do an insertion of the character <code>p</code>, we get the word <code>top</code>.</li> <li>if we do an insertion of the character <code>o</code>, we get the word <code>too</code>.</li> <li>if we do an insertion of the character <code>w</code>, we get the word <code>two</code>.</li> </ul>"},{"location":"lectures/minimum_edit_distance/#deletion","title":"Deletion","text":"<p>A deletion is when we remove a character from a string. It does not matter which character we delete.</p> <pre><code>def delete_char(word, i):\n    return word[:i] + word[i+1:]\n</code></pre> <p>Example</p> <p>For the word <code>hat</code></p> <ul> <li>if we remove the character <code>h</code>, we get the word <code>at</code>.</li> <li>if we remove the character <code>a</code>, we get the word <code>ht</code>.</li> <li>if we remove the character <code>t</code>, we get the word <code>ha</code>.</li> </ul>"},{"location":"lectures/minimum_edit_distance/#replacement","title":"Replacement","text":"<p>A replacement is when we replace a character in a string with another character. It does not matter which character we replace.</p> <pre><code>def replace_char(word, i, char):\n    return word[:i] + char + word[i+1:]\n</code></pre> <p>Example</p> <p>For the word <code>jaw</code></p> <ul> <li>if we replace the character <code>j</code> with <code>s</code>, we get the word <code>saw</code>.</li> <li>if we replace the character <code>w</code> with <code>r</code>, we get the word <code>jar</code>.</li> <li>if we replace the character <code>j</code> with <code>p</code>, we get the word <code>paw</code>.</li> </ul>"},{"location":"lectures/minimum_edit_distance/#filtering-candidate-words","title":"Filtering Candidate Words","text":"<p>Many words returned by the edit distance algorithm are not valid words. We need to filter these words out.</p> <p>To do this, we can also check if the word is in the vocabulary. If it is, then we can keep it.</p> <pre><code>def filter_candidates(words, vocab):\n    valid_words = []\n    for word in words:\n        if word in vocab:\n            valid_words.append(word)\n    return valid_words\n</code></pre>"},{"location":"lectures/minimum_edit_distance/#calculating-word-probabilities","title":"Calculating Word Probabilities","text":"<p>If we have the candidate words available, the next step is to find the word that is most likely to be the correct word (or the \\(n\\) most likely words).</p> <p>Example</p> <p>Consider the following sentence:</p> <p>\"I like apples \ud83c\udf4e\ud83c\udf4f ans bananas\" \ud83c\udf4c\ud83c\udf4c</p> <p>The word <code>ans</code> is supposedly a typo of the word <code>and</code>.</p> <p>If we look at two candidate words, <code>and</code> and <code>ant</code>, we can observe that both are 1 edit distance away from <code>ans</code>. So how can we tell that <code>and</code> is more likely to be the correct word than <code>ant</code>? \ud83d\udc1c</p> <p>Usually, we can assume that the word <code>and</code> is more frequent than the word <code>ant</code> \ud83d\udc1c in any given text, so based on word frequencies, the model should suggest the word <code>and</code> as the correct word. \ud83d\udca1</p> <p>To do this, we can use the word probabilities. The word probabilities tell us how likely it is for a word to appear in a given text.</p> <p>The word probabilities can be calculated by counting the number of times a word appears in a text, and dividing it by the total number of words in the text.</p> \\[ P(w) = \\frac{N(w)}{N} \\] <p>Where</p> <ul> <li>\\(P(w)\\) is the probability of the word \\(w\\) appearing in a text</li> <li>\\(N(w)\\) is the frequency of the word \\(w\\) in the text</li> <li>\\(N\\) is the total count of all words in the text</li> </ul> <p>Info</p> <p>This is quite similar as from the lecture on feature extraction, where we calculated the word frequencies per class. The difference here is that we are now calculating the word frequencies for the entire text, not per class. This gives us the probability of a word appearing in a text.</p> <p>Example</p> <p>Consider the following corpus:</p> <pre><code>corpus = [\n    \"I like apples and bananas\",\n    \"I like apples and oranges\"\n]\n</code></pre> <p>By counting the number of times each word appears in the corpus, we get the word frequencies. Since the total number of words \\(N\\) in the corpus is 10, we can calculate the word probabilities, and end up with the following table:</p> <p>If we build the word frequency table for this corpus, we get the following:</p> Word Frequency Probability I 2 0.2 like 2 0.2 apples 2 0.2 and 2 0.2 bananas 1 0.1 oranges 1 0.1 total 10 1.0 <p>Note that in Python, we can utilize the <code>Counter</code> class from the <code>collections</code> module to count the number of times each word appears in the corpus.</p> <pre><code>&gt;&gt;&gt; from collections import Counter\n&gt;&gt;&gt; freqs = Counter()\n&gt;&gt;&gt; for text in corpus:\n&gt;&gt;&gt;     freqs.update(text.split())\n&gt;&gt;&gt; freqs\nCounter({'I': 2, 'like': 2, 'apples': 2, 'and': 2, 'bananas': 1, 'oranges': 1})\n</code></pre> <p>From there, we can calculate the word probabilities as follows:</p> <pre><code>&gt;&gt;&gt; total_words = sum(freqs.values())\n&gt;&gt;&gt; {word: freq / total_words for word, freq in freqs.items()}\n{'I': 0.2, 'like': 0.2, 'apples': 0.2, 'and': 0.2, 'bananas': 0.1, 'oranges': 0.1}\n</code></pre> <p>Info</p> <p>The code snippet above shows a Python dict comprehension.</p> <p>They can be used to create dictionaries from arbitrary key and value expressions, like so:</p> <pre><code>&gt;&gt;&gt; {x: x**2 for x in (2, 4, 6)}\n{2: 4, 4: 16, 6: 36}\n</code></pre> <p>This is equivalent to the following code:</p> <pre><code>data = {}\nfor x in (2, 4, 6):\n    data[x] = x**2\n</code></pre>"},{"location":"lectures/minimum_edit_distance/#the-levenshtein-distance-algorithm","title":"The Levenshtein Distance Algorithm","text":"<p>Now we take a look at a specific algorithm for calculating the minimum edit distance between two strings.</p> <p>In NLP, one of the most common algorithms for calculating the minimum edit distance is the Levenshtein distance algorithm.</p> <p>The Levenshtein distance algorithm is a dynamic programming algorithm that calculates the minimum edit distance between two strings.</p> <p>Dynamic Programming</p> <p>Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It is often used when the subproblems are overlapping, i.e. when subproblems share subproblems.</p> <p>The Levenshtein distance algorithm is a dynamic programming algorithm because it breaks down the problem of calculating the minimum edit distance into smaller subproblems, and it uses the results of those subproblems to solve the larger problem of calculating the minimum edit distance.</p> <p>We can often solve dynamic programming problems by using recursion.</p> <p>Here is a nice blog post that explains dynamic programming in more detail: The complete beginners guide to dynamic programming</p> <p>When computing the minimum edit distance, you would start with a source word and transform it into the target word. The algorithm would then return the minimum cost of transforming the source word into the target word.</p> <p>We can do that by created a distance matrix \\(D\\) where the source word is shown on the vertical axis, and the target word is shown on the horizontal axis.</p> <p>Example</p> <p>To explain the algorithm in the following sections, we will use the following example words:</p> <ul> <li>Source: <code>play</code></li> <li>Target: <code>stay</code></li> </ul>"},{"location":"lectures/minimum_edit_distance/#edit-cost","title":"Edit Cost","text":"<p>In the Minimum Edit Distance algorithm, each edit operation has a cost associated with it.</p> <p>For the Levenshtein distance algorithm, the cost of each edit operation is as follows:</p> Operation Cost Insertion 1 Deletion 1 Replace 2 <p>Info</p> <p>Since a replacement can be seen as a deletion followed by an insertion, it makes sense that the cost of a replacement is given as 2.</p> <p>If we transform one string into another string, we can calculate the total edit cost by adding up the costs of all edit operations. This is referred to as the total edit cost, and this is what we are trying to minimize.</p> <p>We can think of the cost as the distance between two words. The lower the cost, the closer the words are, and the more similar they are.</p> <p>Example</p> <p>Consider our two example words:</p> <ul> <li>Source: <code>play</code></li> <li>Target: <code>stay</code></li> </ul> <p>To go from <code>play</code> to <code>stay</code>, we need to do the following operations:</p> <ul> <li>Replace <code>p</code> with <code>s</code></li> <li>Replace <code>l</code> with <code>t</code></li> </ul> <p>Since a replacement has a cost of 2, the total cost of changing <code>play</code> into <code>stay</code> is calculated by:</p> \\[ 2 + 2 = 4 \\] <p>So the edit distance between <code>play</code> and <code>stay</code> is 4.</p>"},{"location":"lectures/minimum_edit_distance/#edit-paths","title":"Edit Paths","text":"<p>Sometimes there are multiple ways to transform the source word into the target word. Each possible sequence of edit operations is called an edit path.</p> <p>Example</p> <p>If we want to transform the letter <code>p</code> into the letter <code>s</code>, we can do this via the following edit paths:</p> <ul> <li>Insert <code>s</code> and delete <code>p</code>: <code>p</code> -&gt; <code>ps</code> -&gt; <code>s</code> (cost: 1 + 1 = 2)</li> <li>Delete <code>p</code> and insert <code>s</code>: <code>p</code> -&gt; <code>#</code> -&gt; <code>s</code> (cost: 1 + 1 = 2)</li> <li>Replace <code>p</code> with <code>s</code>: <code>p</code> -&gt; <code>s</code> (cost: 2)</li> </ul>"},{"location":"lectures/minimum_edit_distance/#creating-the-distance-matrix","title":"Creating the Distance Matrix","text":"<p>We start by creating the distance matrix \\(D\\) where the source word <code>play</code> is shown on the vertical axis, and the target word <code>stay</code> is shown on the horizontal axis.</p> <p></p> <p>Note that each letter is assigned an index.</p> <p>Also note that we also add an empty character <code>#</code> to the top and left of the table. This is because we need to be able to transform the source word into an empty string, and the empty string into the target word.</p> <p>Our goal is to fill each cell <code>D[i,j]</code> with the correct edit distance values, where:</p> \\[ D[i,j] = source[:i] \\rightarrow target[:j] \\] <p>We can find the edit distance between the source word <code>play</code> and the target word <code>stay</code> in the bottom right corner of the table, which represents the transformation <code>play -&gt; stay</code>.</p> <p>Example</p> <p>The element <code>D[2,3]</code> represents the transformation <code>pl -&gt; sta</code>.</p> <pre><code>&gt;&gt;&gt; source = \"play\"\n&gt;&gt;&gt; target = \"stay\"\n&gt;&gt;&gt; source[:2]\n'pl'\n&gt;&gt;&gt; target[:3]\n'sta'\n</code></pre>"},{"location":"lectures/minimum_edit_distance/#initializing-the-matrix","title":"Initializing the Matrix","text":"<p>Let's start in the top left corner of the table, which represents the transformation <code># -&gt; #</code>, that is, transforming an empty string into an empty string. This is a special case, since we do not need to do any edits to transform an empty string into an empty string. So the cost of this transformation is zero.</p> <ul> <li>The cell <code>D[1,0]</code> represents the path <code>p -&gt; #</code>, that is, deleting <code>p</code> so that we have an empty string, which has a cost of 1.</li> <li>The cell <code>D[0,1]</code> represents the path <code># -&gt; s</code>, that is, inserting <code>s</code> into an empty string, which has a cost of 1.</li> <li>The cell <code>D[0,0]</code> represents the path <code># -&gt; #</code>, that is, transforming an empty string into an empty string, which has a cost of 0.</li> </ul> <p></p> <p>Now, consider the cell <code>D[1,1]</code>, which represents the transformation <code>p -&gt; s</code>. As we've seen before, there are three paths to do that:</p> Path Operations Cost <code>p -&gt; ps -&gt; s</code> Insert <code>s</code> and delete <code>p</code> 1 + 1 = 2 <code>p -&gt; # -&gt; s</code> Delete <code>p</code> and insert <code>s</code> 1 + 1 = 2 <code>p -&gt; s</code> Replace <code>p</code> with <code>s</code> 2 <ol> <li> <p>First insert <code>s</code>, then delete <code>p</code></p> <p>In cell <code>D[0,1]</code>, we already calculated the cost of inserting <code>s</code>, so we can reuse that cost, and add another deletion cost for <code>p</code>:</p> \\[ D[i-1,j] + \\text{delete cost} \\] </li> <li> <p>First delete <code>p</code>, then insert <code>s</code></p> <p>In cell <code>D[1,0]</code>, we already calculated the cost of deleting <code>p</code>, so we can reuse that cost, and add another insertion cost for <code>s</code>:</p> \\[ D[i,j-1] + \\text{insert cost} \\] </li> <li> <p>Replace <code>p</code> with <code>s</code></p> <p>Going directly from <code>p -&gt; s</code> is considered a replacement and has a cost of 2. In our case <code>p</code> and <code>s</code> are different. If they were the same, we would not need to replace them, and thus, the cost would be zero. So we need to differentiate those two cases here:</p> \\[ D[i-1,j-1] + \\begin{cases}     0 &amp; \\text{if } source[i] = target[j] \\\\     2 &amp; \\text{otherwise} \\end{cases} \\] <p>Tip</p> <p>You can think of a replace operation as moving diagonal in the table.</p> </li> </ol> <p>In this example, all edit paths lead to a cost of 2. But remeber that we always want to find the minimum cost for the given transformation. So if one of those costs would be smaller than the other ones, this would be the cost to enter in <code>D[1,1]</code>.</p>"},{"location":"lectures/minimum_edit_distance/#deriving-the-algorithm","title":"Deriving the Algorithm","text":"<p>Considering all of the above cases, we can formalize the algorithm as follows:</p> \\[ D[i,j] = \\text{min} \\begin{cases}         D[i-1,j] + \\text{delete cost} \\\\         D[i,j-1] + \\text{insert cost} \\\\         D[i-1,j-1] + \\begin{cases}             0 &amp; \\text{if } source[i] = target[j] \\\\             2 &amp; \\text{otherwise}         \\end{cases}     \\end{cases} \\]"},{"location":"lectures/minimum_edit_distance/#completing-the-first-rows","title":"Completing the first rows","text":"<p>Now let's try to complete the first row horizontally and vertically.</p> <p>Let's start with the first row vertically. This means, we want to transform the source word <code>play</code> into an empty string <code>#</code>:</p> Path Operations Cost <code>p -&gt; #</code> Delete <code>p</code> 1 <code>pl -&gt; #</code> Delete <code>p</code> and <code>l</code> 1 + 1 = 2 <code>pla -&gt; #</code> Delete <code>p</code>, <code>l</code>, and <code>a</code> 1 + 1 + 1 = 3 <code>play -&gt; #</code> Delete <code>p</code>, <code>l</code>, <code>a</code>, and <code>y</code> 1 + 1 + 1 + 1 = 4 <p>Intuitively, we can just delete each of the four letters one by one, which gives us a cost of 4. That means, the minimum edit distance between <code>play</code> and <code>#</code> is 4.</p> <p>Tip</p> <p>We can also think of this as adding one additional deletion for each letter to the already known cost of deleting the previous letters.</p> <p>This is exactly what is reflected in the algorithm:</p> \\[ D[i-1,j] + \\text{delete cost} \\] <p>The same applies to the first row horizontally. This means, we want to transform the empty string <code>#</code> into the target word <code>stay</code>. This time, instead of deleting characters, we need to insert characters:</p> Path Operations Cost <code># -&gt; s</code> Insert <code>s</code> 1 <code># -&gt; st</code> Insert <code>s</code> and <code>t</code> 1 + 1 = 2 <code># -&gt; sta</code> Insert <code>s</code>, <code>t</code>, and <code>a</code> 1 + 1 + 1 = 3 <code># -&gt; stay</code> Insert <code>s</code>, <code>t</code>, <code>a</code>, and <code>y</code> 1 + 1 + 1 + 1 = 4 <p>Intuitively, we can just insert each of the four letters one by one, which gives us a cost of 4. That means, the minimum edit distance between <code>#</code> and <code>stay</code> is 4.</p> <p>Tip</p> <p>We can also think of this as adding one additional insertion for each letter to the already known cost of inserting the previous letters.</p> <p>This is exactly what is reflected in the algorithm:</p> \\[ D[i,j-1] + \\text{insert cost} \\] <p>So far, our distance matrix looks like this:</p> <p></p>"},{"location":"lectures/minimum_edit_distance/#completing-the-remaining-cells","title":"Completing the remaining cells","text":"<p>Applying the algorithm to the remaining cells, we get the following table:</p> <p></p> <p>Example</p> <p>Lets take a look at cell <code>D[3,3]</code> which represents the transformation <code>pla -&gt; sta</code>.</p> <ol> <li> <p>Coming from the cell above:</p> \\[ D[i-1,j] + \\text{delete cost} = D[2,3] + 1 = 5 + 1 = 6 \\] </li> <li> <p>Coming from the cell on the left:</p> \\[ D[i,j-1] + \\text{insert cost} = D[3,2] + 1 = 5 + 1 = 6 \\] </li> <li> <p>Coming from the cell diagonally:</p> <p>Remember that</p> \\[ D[i-1,j-1] + \\begin{cases}     0 &amp; \\text{if } source[i] = target[j] \\\\     2 &amp; \\text{otherwise} \\end{cases} \\] <p>In our case, since <code>a</code> and <code>a</code> are the same, we can add a cost of 0:</p> \\[ D[i-1,j-1] + 0 = D[2,2] + 0 = 4 + 0 = 4 \\] </li> </ol> <p>So the minimum cost of transforming <code>pla</code> into <code>sta</code> is 4, and hence:</p> \\[ D[3,3] = 4 \\] <p>Finally, we can find the minimum edit distance between <code>play</code> and <code>stay</code> in the bottom right corner of the table, which is 4.</p> <p>Note</p> <p>Since there are no more edits needed from <code>pl</code> to <code>st</code>, the value of 4 is propagated to the bottom right corner.</p> <p>Backtrace</p> <p>Now that we have the distance matrix, we can find the edit path that leads to the minimum edit distance.</p> <p>We can do this by starting in the bottom right corner, and then backtracing to the top left corner.</p> <p>The backtrace represents the optimal sequence of edit operations that transform the source word into the target word.</p> <p>This sequence provides insights into how two words are related to each other, and can be used to improve the autocorrect process and suggest better corrections.</p>"},{"location":"lectures/minimum_edit_distance/#limitations","title":"Limitations","text":"<p>While the Levenshtein distance algorithm is a very popular algorithm for calculating the minimum edit distance, it has some limitations. Specfically, it does not take into account the following:</p> <ul> <li>Context: It only looks at the words themselves, not the words around them.</li> <li>Part of speech: It treats all words equally, regardless of their POS.</li> <li>Word order: The position in the sentence is not considered</li> <li>Frequency: Generally, the algorithm does not take frequency into account. To do that, we would need to calculate the word frequencies, as indicated above.</li> <li>Long strings: It does not work well with long strings, since the number of possible edit paths increases exponentially with the length of the strings.</li> </ul>"},{"location":"lectures/minimum_edit_distance/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>The minimum edit distance is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.</li> <li>In NLP, a common use case for the minimum edit distance is in autocorrection systems, where it is used to suggest corrections for misspelled words.</li> <li>The Levenshtein distance algorithm is a common algorithm for calculating the minimum edit distance between two strings. Each edit operation has a cost associated with it.</li> <li>We want to find the minimum cost of transforming one string into another string. To do this we can create a distance matrix \\(D\\) where the source word is shown on the vertical axis, and the target word is shown on the horizontal axis.</li> <li>After we have filled the distance matrix, we can find the minimum edit distance in the bottom right corner of the table.</li> </ul>"},{"location":"lectures/naive_bayes/","title":"Naive Bayes","text":"<p>In this lecture, we will learn about the Naive Bayes classifier for binary classification.</p> <p>Naive Bayes is a simple but powerful classifier that doesn't require to find any hyperparameters.</p> <p>It is very fast to train and predict, and can perform surprisingly well.</p>"},{"location":"lectures/naive_bayes/#tldr","title":"TL;DR","text":"<p>When using Naive Bayes for binary classification, we need to calculate the likelihood of a tweet being positive or negative.</p> <p>To do this, we need to build the log ratio of probabilities for each word in the vocabulary.</p> <p>For example, if the word \"happy\" appears 20 times in positive tweets and 5 times in negative tweets, then the ratio of probabilities is \\(20/5=4\\). This means that the word \"happy\" is more likely to appear in a positive tweet. If the ratio would be less than 1, then the word is more likely to appear in a negative tweet.</p> <p>Taking the logarithm is a mathematical trick to avoid numerical underflow and simplify the calculations.</p> <p>Using the log ratio of probabilities, we can calculate the log likelihood of a tweet being positive or negative by summing up the log ratio of probabilities for each word in the tweet, and thus, predict the class of the tweet.</p>"},{"location":"lectures/naive_bayes/#probability-recap","title":"Probability Recap","text":"<p>Let</p> <ul> <li>\\(A\\) be the event that a tweet being labeled positive</li> <li>\\(N_{pos}\\) be the number of positive tweets</li> <li>\\(N\\) be the total number of tweets</li> </ul> <p>Then the probability \\(P(A)\\) of a tweet being positive is the number of positive tweets divided by the total number of tweets:</p> \\[ P(A) = \\frac{N_{pos}}{N} \\] <p>Note</p> <p>For binary classification, if a tweet can only be either positive or negative, then the probability of a tweet being negative is \\(1-P(A)\\).</p> <p>Example</p> <p>If there are 35 positive tweets and 100 tweets in total, then the probability \\(P(A)\\) of a tweet being positive is calculated as</p> \\[ P(A)=35/100=0.35. \\] <p>The probability of the tweet being negative is then calculated as \\(1-P(A) = 0.65\\).</p> <p>Let \\(B\\) be the event that a tweet contains the word \"amazing\". Then the probability \\(P(B)\\) of a tweet containing the word \"amazing\" is the number of tweets containing the word divided by the total number of tweets:</p> \\[ P(B) = \\frac{N_{amazing}}{N} \\] <p>Example</p> <p>If there are 5 tweets containing the word \"amazing\" and 100 tweets in total, then the probability \\(P(B)\\) of a tweet containing the word \"amazing\" is calculated as</p> \\[ P(B) = 5/100 = 0.05 \\]"},{"location":"lectures/naive_bayes/#intersection-of-two-events","title":"Intersection of Two Events","text":"<p>Let \\(A \\cap B\\) be the event that a tweet is positive and contains the word \"amazing\".</p> <p>Then the probability \\(P(A \\cap B)\\) is calculated as the number of tweets that are positive and contain the word \"amazing\" divided by the total number of tweets:</p> \\[ P(A \\cap B) = \\frac{N_{pos \\cap amazing}}{N} \\] <p>The following Venn diagram illustrates this:</p> <p></p> <p>Example</p> <p>Let's assume a corpus of 100 tweets:</p> <ul> <li>35 tweets are positive</li> <li>65 tweets are negative</li> <li>5 tweets contain the word \"amazing\", but one of them is negative (e.g. \"I thought this movie was amazing, but it was actually terrible!\")</li> </ul> <p>Then the probability \\(P(A \\cap B)\\) of a tweet being positive and containing the word \"amazing\" is calculated as</p> \\[ P(A \\cap B) = \\frac{4}{100} = 0.04 \\]"},{"location":"lectures/naive_bayes/#conditional-probability","title":"Conditional Probability","text":"<p>Continuing the example from above, let's assume we want to calculate the probability of a tweet being positive, but knowing that the tweet contains the word \"amazing\".</p> <p>Looking at the diagram from above, this means we only consider the blue circle.</p> <p>In our example, this is the probability of the intersection of the tweets being positive and containing the word \"amazing\" divided by the probability of all tweets containing the word \"amazing\".</p> <p>This is called the conditional probability of a tweet being positive, given that it contains the word \"amazing\".</p> <p>Conditional Probability</p> <p>Generally speaking, the conditional probability \\(P(A|B)\\) is the probability of event \\(A\\) given that event \\(B\\) has already occurred.</p> <p>It is calculated as the probability of both events occuring divided by the probability of the event that has already occurred:</p> \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] <p>Example</p> <p>Let's continue the example from above, where we have 5 tweets containing the word \"amazing\" and 4 of them are positive.</p> <p>Then the probability \\(P(positive|amazing)\\) of a tweet being positive, given that it contains the word \"amazing\" is calculated as</p> \\[ P(positive|amazing) = \\frac{P(positive \\cap amazing)}{P(amazing)} = \\frac{4/100}{5/100} = \\frac{4}{5} = 0.8 \\] <p>Now let's turn it around and calculate the probability of a tweet containing the word \"amazing\", given that it is positive.</p> <p>This is calculated as follows:</p> \\[ P(amazing|positive) = \\frac{P(positive \\cap amazing)}{P(positive)} = \\frac{4/100}{35/100} = \\frac{4}{35} = 0.1143 \\]"},{"location":"lectures/naive_bayes/#bayes-rule","title":"Bayes Rule","text":"<p>Now we can derive Bayes Rule, which is based on conditional probabilities.</p> <p>We know that the conditional probability \\(P(A|B)\\) is calculated as follows:</p> \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] <p>and we also know that the conditional probability \\(P(B|A)\\) is calculated as follows:</p> \\[ P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\] <p>We can rewrite this as:</p> \\[ P(B \\cap A) = P(B|A)P(A) \\] <p>Given that</p> \\[ P(A \\cap B) = P(B \\cap A) \\] <p>we can plug in the previous equation and get:</p> \\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\] <p>With that, we have derived Bayes Rule.</p> <p>Bayes Rule</p> <p>Bayes Rule is a way to calculate the conditional probability \\(P(A|B)\\), given that we know \\(P(B|A)\\).</p> <p>It is calculated as follows:</p> \\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\] <p>In other words: Out of all the times \\(B\\) happens, what fraction are due to \\(A\\)?</p> <p>Example</p> <p>Suppose that in your dataset, 25% of the positive tweets contain the word \"amazing\". You also know that a total of 13% of the tweets in your dataset contain the word \"amazing\", and that 40% of the total number of tweets are positive. Given the tweet \"amazing to be here\". What is the probability that this tweet is positive?</p> <p>Let \\(A\\) be the event that a tweet is positive and \\(B\\) be the event that a tweet contains the word \"amazing\".</p> \\[ \\begin{align} P(A) &amp;= 0.4 \\\\ P(B) &amp;= 0.13 \\\\ P(B|A) &amp;= 0.25 \\\\ P(A|B) &amp;= \\frac{P(B|A)P(A)}{P(B)} = \\frac{0.25 \\times 0.4}{0.13} = 0.7692 \\end{align} \\] <p>The probability that the tweet \"amazing to be here\" is positive is 0.7692.</p> <p>Thomas Bayes</p> <p></p> <p>Thomas Bayes (1701 - 1761) was an English statistician, philosopher and Presbyterian minister. Bayes never published what would become his most famous accomplishment; his notes were edited and published posthumously by Richard Price.</p> <p>In NLP, Bayes' Theorem can be used for:</p> <ul> <li>classification: given a document, what is the probability that it belongs to a certain class? (e.g. spam or not spam or sentiment analysis)</li> <li>information retrieval: given a query, what is the probability that a document is relevant?</li> <li>word sense disambiguation: given a word, what is the probability that it has a certain meaning?</li> </ul> <p>Besides machine learning and NLP, Bayes' Theorem is also used in many other fields, such as:</p> <ul> <li>medicine: given a symptom, what is the probability that a patient has a certain disease?</li> <li>biology: given a genetic profile, what is the probability that a person will develop a certain disease?</li> <li>economics: given a set of economic conditions, what is the probability that the economy will be in a recession next year?</li> <li>finance: given a set of financial conditions, what is the probability that a stock will increase in value next year?</li> </ul>"},{"location":"lectures/naive_bayes/#laplacian-smoothing","title":"Laplacian Smoothing","text":"<p>Using Bayes Rule, we can calculate the probability of a word given a class \\(P(w|c)\\) as follows:</p> \\[ P(w|c) = \\frac{P(c|w)P(w)}{P(c)} = \\frac{freq(w,c)}{N_c} \\] <p>However, for OOV words, if a word has not been seen in the training data, then \\(freq(w,c) = 0\\) and thus, \\(P(w|c) = 0\\).</p> <p>To account for this, we can use Laplacian Smoothing (aka Additive Smoothing).</p> <p>This is done by adding the smoothing constant \\(\\alpha\\) to the numerator and \\(\\alpha|V|\\) to the denominator:</p> \\[ P(w|c) = \\frac{freq(w,c) + \\alpha}{N_c + \\alpha|V|} \\] <p>Info</p> <p>If we add a constant \\(\\alpha\\) to the numerator, and since there are \\(|V|\\) words in the vocabulary to normalize, we have to add \\(\\alpha|V|\\) to the denominator. This way, the probabilities will sum up to 1.</p> <p>Note that \\(\\alpha\\) is usually set to 1.</p>"},{"location":"lectures/naive_bayes/#word-probabilities","title":"Word Probabilities","text":"<p>Let's assume we have the following table of word frequencies (as from the feature extraction lecture):</p> \\(V\\) \\(n_{pos}\\) \\(n_{neg}\\) I 3 3 am 2 2 happy 2 0 sad 0 2 because 1 1 love 1 0 hate 0 1 the 1 1 weather 1 1 \\(\\sum\\) 11 11 <p>Note that we added the last row to calculate the total number of words per class. This allows us to calculate the probabilities \\(P(w|pos)\\) and \\(P(w|neg)\\) for each word \\(w\\) in a class.</p> <p>Using the formula for Laplacian Smoothing with \\(\\alpha=1\\)</p> \\[ P(w|c) = \\frac{freq(w,c) + 1}{N_c + |V|} \\] <p>We end up with the following table:</p> \\(V\\) \\(P(w \\vert pos)\\) \\(P(w \\vert neg)\\) I 0.2 0.2 am 0.15 0.15 happy 0.15 0.05 sad 0.05 0.15 because 0.1 0.1 love 0.1 0.05 hate 0.05 0.1 the 0.1 0.1 weather 0.1 0.1 \\(\\sum\\) \\(1\\) \\(1\\) <p>Example</p> <p>Let's calculate the probability \\(P(\\text{happy}|\\text{pos})\\) of the word \"happy\" given that the tweet is positive.</p> \\[ \\begin{align} P(\\text{happy}|\\text{pos}) &amp;= \\frac{freq(\\text{happy},\\text{pos}) + 1}{N_{pos} + |V|} \\\\ &amp;= \\frac{2 + 1}{11 + 9} \\\\ &amp;= \\frac{3}{20} \\\\ &amp;= 0.15 \\end{align} \\] <p>Let's calculate the probability \\(P(\\text{happy}|\\text{neg})\\) of the word \"happy\" given that the tweet is negative.</p> \\[ \\begin{align} P(\\text{happy}|\\text{neg}) &amp;= \\frac{freq(\\text{happy},\\text{neg}) + 1}{N_{neg} + |V|} \\\\ &amp;= \\frac{0 + 1}{11 + 9} \\\\ &amp;= \\frac{1}{20} \\\\ &amp;= 0.05 \\end{align} \\]"},{"location":"lectures/naive_bayes/#ratio-of-probabilities","title":"Ratio of Probabilities","text":"<p>Now that we have the probabilities \\(P(w|pos)\\) and \\(P(w|neg)\\) for each word \\(w\\) in a class, we can calculate the ratio of probabilities for each word \\(w\\) in the vocabulary:</p> \\[ \\frac{P(w \\vert pos)}{P(w \\vert neg)} \\] <p>Based on the ratio, we can make the following observations:</p> <ul> <li>If the ratio is greater than 1, then the word is more likely to appear in a positive tweet.</li> <li>If the ratio is less than 1, then the word is more likely to appear in a negative tweet.</li> <li>If the ratio is equal to 1, then the word is considered neutral and equally likely to appear in a positive or negative tweet.</li> </ul> \\(V\\) \\(P(w \\vert pos)\\) \\(P(w \\vert neg)\\) \\(\\frac{P(w \\vert pos)}{P(w \\vert neg)}\\) I 0.2 0.2 1.0 am 0.15 0.15 1.0 happy 0.15 0.05 3.0 sad 0.05 0.15 0.3333 because 0.1 0.1 1.0 love 0.1 0.05 2.0 hate 0.05 0.1 0.5 the 0.1 0.1 1.0 weather 0.1 0.1 1.0 <p>Note</p> <p>Words that are neutral don't provide any information for classification.</p>"},{"location":"lectures/naive_bayes/#likelihood","title":"Likelihood","text":"<p>Now that we have the ratio of probabilities for each word \\(w\\) in the vocabulary, we can calculate the probability of a tweet being positive or negative.</p> <p>To classify a whole tweet, we need to multiply the ratios of probabilities for each word in the tweet.</p> <p>This is called the likelihood \\(P(tweet|pos)\\) of a tweet being positive and is calculated as follows:</p> \\[ P(\\text{pos}|\\text{tweet}) = \\prod_{i=1}^{m} \\frac{P(w_i|pos)}{P(w_i|neg)} \\] <p>where</p> <ul> <li>\\(m\\) is the number of words in the tweet and</li> <li>\\(w_i\\) is the \\(i\\)-th word in the tweet.</li> </ul> <p>Note</p> <ul> <li>If the likelihood is greater than 1, then the tweet is more likely to be positive.</li> <li>If the likelihood is less than 1, then the tweet is more likely to be negative.</li> <li>If the likelihood is equal to 1, then the tweet is equally likely to be positive or negative and thus, neutral.</li> </ul> <p>Example</p> <p>Given the table above, let's see if the following tweet is positive or negative:</p> <p>I am happy because I love ice cream</p> <p>We have the following ratios of probabilities:</p> \\[ \\begin{align} \\frac{P(\\text{I}|\\text{pos})}{P(\\text{I}|\\text{neg})} &amp;= \\frac{0.2}{0.2} = 1.0 \\\\ \\frac{P(\\text{am}|\\text{pos})}{P(\\text{am}|\\text{neg})} &amp;= \\frac{0.15}{0.15} = 1.0 \\\\ \\frac{P(\\text{happy}|\\text{pos})}{P(\\text{happy}|\\text{neg})} &amp;= \\frac{0.15}{0.05} = 3.0 \\\\ \\frac{P(\\text{because}|\\text{pos})}{P(\\text{because}|\\text{neg})} &amp;= \\frac{0.1}{0.1} = 1.0 \\\\ \\frac{P(\\text{I}|\\text{pos})}{P(\\text{I}|\\text{neg})} &amp;= \\frac{0.2}{0.2} = 1.0 \\\\ \\frac{P(\\text{love}|\\text{pos})}{P(\\text{love}|\\text{neg})} &amp;= \\frac{0.1}{0.05} = 2.0 \\\\ \\end{align} \\] <p>Note that the words \"ice\" and \"cream\" are not in the vocabulary, so we ignore them.</p> <p>Given these ratios, we can calculate the likelihood of the tweet being positive as follows:</p> \\[ \\begin{align} P(\\text{pos}|\\text{tweet}) &amp;= \\prod_{i=1}^{m} \\frac{P(w_i|pos)}{P(w_i|neg)} \\\\ &amp;= 1.0 \\times 1.0 \\times 3.0 \\times 1.0 \\times 1.0 \\times 2.0 \\\\ &amp;= 6.0 \\end{align} \\]"},{"location":"lectures/naive_bayes/#prior","title":"Prior","text":"<p>The prior \\(P(pos)\\) is the probability of a tweet being positive, regardless of the words in the tweet.</p> <p>The prior probability represents the probability of a particular class before considering any features.</p> <p>The prior is especially important when the dataset is unbalanced.</p> <p>In a binary classification problem, the prior for a class is calculated as follows:</p> \\[ prior = \\frac{N_c}{N} \\] <p>where</p> <ul> <li>\\(N_c\\) is the number of tweets in the class and</li> <li>\\(N\\) is the total number of tweets.</li> </ul> <p>Example</p> <p>Let's assume we have the following corpus of 100 tweets:</p> <ul> <li>35 tweets are positive</li> <li>65 tweets are negative</li> </ul> <p>Then the prior probability of a tweet being positive is calculated as</p> \\[ prior = \\frac{N_c}{N} = \\frac{35}{100} = 0.35 \\] <p>and the prior probability of a tweet being negative is calculated as</p> \\[ prior = \\frac{N_c}{N} = \\frac{65}{100} = 0.65 \\]"},{"location":"lectures/naive_bayes/#prior-ratio","title":"Prior Ratio","text":"<p>The prior ratio is the ratio of the prior probabilities of the two classes.</p> <p>In a binary classification problem, the prior ratio is calculated as follows:</p> \\[ \\text{prior ratio} = \\frac{P(pos)}{P(neg)} \\] <p>Example</p> <p>Let's assume we have the following corpus of 100 tweets:</p> <ul> <li>35 tweets are positive</li> <li>65 tweets are negative</li> </ul> <p>Then the prior ratio is calculated as</p> \\[ \\frac{P(pos)}{P(neg)} = \\frac{0.35}{0.65} = 0.5385 \\] <p>If we apply the prior to the likelihood, we get the following formula:</p> \\[ P(\\text{pos}|\\text{tweet}) = \\frac{P(pos)}{P(neg)} \\times \\prod_{i=1}^{m} \\frac{P(w_i|pos)}{P(w_i|neg)} \\]"},{"location":"lectures/naive_bayes/#using-logarithms","title":"Using Logarithms","text":"<p>The likelihood is the product of many probabilities, i.e. values between 0 and 1.</p> <p>This can have several consequences, and numbers can become so small that computers have trouble representing them.</p> <p>This is called numerical underflow.</p> <p>To avoid this, we can use the logarithm instead.</p> <p>Numerical Underflow</p> <p>Numerical underflow occurs when the result of a calculation is too small to be represented by the computer.</p> <p>This can happen when multiplying many small numbers, because the result gets smaller and smaller with each multiplication.</p> <p>The computer can only represent numbers up to a certain precision, so at some point the result will be rounded to zero.</p> <p>This is a problem and can lead to errors, as we lose information about the probabilities.</p> <p>Tip</p> <p>Because they avoid the risk of numerical underflow and they are way more convenient to work with, logarithms appear throughout deep-learning and NLP.</p>"},{"location":"lectures/naive_bayes/#log-likelihood","title":"Log Likelihood","text":"<p>Applying the logarithm to the likelihood formula from above, we get the following formula:</p> \\[ \\log P(\\text{pos}|\\text{tweet}) = \\log \\frac{P(pos)}{P(neg)} + \\sum_{i=1}^{m} \\log \\frac{P(w_i|pos)}{P(w_i|neg)} \\] <p>Logarithm</p> <p>Besides avoiding numerical underflow, another advantage of logarithms is that they allow us to use simpler operations, such as addition instead of multiplication.</p> <p>This is because of the following property of logarithms:</p> \\[ \\log (ab) = \\log a + \\log b \\] <p>Thus, the product changes to a sum in the formula above.</p>"},{"location":"lectures/naive_bayes/#log-prior-ratio","title":"Log Prior Ratio","text":"<p>When using the logarithm, we speak of the prior as the log prior, and of the prior ratio as the log prior ratio.</p> \\[ \\log \\frac{P(pos)}{P(neg)} \\]"},{"location":"lectures/naive_bayes/#log-ratio-of-probabilities","title":"Log Ratio of Probabilities","text":"<p>Now, if we calculate the ratio of probabilities using the logarithm, the table above looks as follows:</p> \\(V\\) \\(P(w \\vert pos)\\) \\(P(w \\vert neg)\\) \\(\\log \\frac{P(w \\vert pos)}{P(w \\vert neg)}\\) I 0.2 0.2 0.0 am 0.15 0.15 0.0 happy 0.15 0.05 1.0986 sad 0.05 0.15 -1.0986 because 0.1 0.1 0.0 love 0.1 0.05 0.6931 hate 0.05 0.1 -0.6931 the 0.1 0.1 0.0 weather 0.1 0.1 0.0 <p>Example</p> <p>Let's look at a single example, e.g. the word \"happy\". The ratio of probabilities is calculated as follows:</p> \\[ \\log \\frac{P(\\text{happy}|\\text{pos})}{P(\\text{happy}|\\text{neg})} = \\log \\frac{0.15}{0.05} = \\log 3.0 = 1.0986 \\]"},{"location":"lectures/naive_bayes/#training","title":"Training","text":"<p>For training of the Naive Bayes classifier for binary classification, we need to do the following:</p> <ol> <li>Calculate the log prior ratio</li> <li>Compute the table of word frequencies for each class</li> <li>Compute the table of conditional probabilities of a word given a class using Laplacian Smoothing</li> <li>Compute the log ratio of the conditional probabilities</li> </ol> <p>Of course, we need to apply the desired preprocessing steps before the training.</p> <p></p>"},{"location":"lectures/naive_bayes/#prediction","title":"Prediction","text":"<p>To predict a tweet using the Naive Bayes classifier for binary classification, we need to apply the likelihood formula to the tweet, and check if the log likelihood is greater than 0.</p> <p>Note</p> <p>Note that for the log likelihood, we need to check if the value is greater than 0, because we are working with logarithms.</p> \\[ \\log \\frac{P(pos)}{P(neg)} + \\sum_{i=1}^{m} \\log \\frac{P(w_i|pos)}{P(w_i|neg)} &gt; 0 \\] <p>So for every word in the tweet, we look up the log ratio of probabilities in our likelihood table and sum them up. Then we add the log prior ratio to the sum.</p> <p>OOV Words are ignored. They are considered neutral and do not contribute to the log likelihood, as the model can only give a score for words that it has seen in the training data.</p> <p></p> <p>Example</p> <p>Let's assume we have a balanced corpus:</p> \\[ \\log \\frac{P(pos)}{P(neg)} = \\log \\frac{0.5}{0.5} = \\log 1.0 = 0.0 \\] <p>Given the table above, let's see if the following tweet is positive or negative:</p> <p>I am happy because I love ice cream</p> <p>We have the following ratios of probabilities:</p> \\[ \\begin{align} \\log \\frac{P(\\text{I}|\\text{pos})}{P(\\text{I}|\\text{neg})} &amp;= \\log \\frac{0.2}{0.2} = \\log 1.0 = 0.0 \\\\ \\log \\frac{P(\\text{am}|\\text{pos})}{P(\\text{am}|\\text{neg})} &amp;= \\log \\frac{0.15}{0.15} = \\log 1.0 = 0.0 \\\\ \\log \\frac{P(\\text{happy}|\\text{pos})}{P(\\text{happy}|\\text{neg})} &amp;= \\log \\frac{0.15}{0.05} = \\log 3.0 = 1.0986 \\\\ \\log \\frac{P(\\text{because}|\\text{pos})}{P(\\text{because}|\\text{neg})} &amp;= \\log \\frac{0.1}{0.1} = \\log 1.0 = 0.0 \\\\ \\log \\frac{P(\\text{I}|\\text{pos})}{P(\\text{I}|\\text{neg})} &amp;= \\log \\frac{0.2}{0.2} = \\log 1.0 = 0.0 \\\\ \\log \\frac{P(\\text{love}|\\text{pos})}{P(\\text{love}|\\text{neg})} &amp;= \\log \\frac{0.1}{0.05} = \\log 2.0 = 0.6931 \\\\ \\end{align} \\] <p>Note that the words \"ice\" and \"cream\" are not in the vocabulary, so we ignore them.</p> <p>Given these ratios, and considering the log prior, we can calculate the log likelihood of the tweet being positive as follows:</p> \\[ \\begin{align} \\log \\frac{P(pos)}{P(neg)} + \\sum_{i=1}^{m} \\log \\frac{P(w_i|pos)}{P(w_i|neg)} &amp;= 0.0 + 0.0 + 0.0 + 1.0986 + 0.0 + 0.0 + 0.6931\\\\ &amp;= 1.0986 + 0.6931 \\\\ &amp;= 1.7917 \\end{align} \\] <p>Since \\(1.7917 &gt; 0\\), the tweet is classified as positive.</p> <p>Note how only the words \"happy\" and \"love\" contribute to the log likelihood, since the other words are neutral.</p>"},{"location":"lectures/naive_bayes/#limitations","title":"Limitations","text":"<p>Naive Bayes is a very simple but powerful classifier and doesn't require to find any hyperparameters.</p> <p>However, it has some limitations, with the most important one being the independence assumption.</p> <p>The independence assumption in Naive Bayes refers to the assumption that the presence or absence of a particular feature is independent of the presence or absence of any other feature, given the class label.</p> <p>In other words, Naive Bayes assumes that the features are independent of each other, which typically isn't the case in NLP.</p> <p>Some words are more likely to appear together than others, and are thus not independent. Also words can be related to the thing they describe.</p> <p>Example</p> <p>It is sunny and hot in the Sahara desert.</p> <ul> <li>the word \"sunny\" is more likely to appear with the word \"hot\" than with the word \"cold\"</li> <li>the word \"Sahara\" is more likely to appear with the word \"desert\" than with the word \"ocean\"</li> <li>the words \"sunny\" and \"hot\" are related to the word \"desert\"</li> </ul> <p>Example</p> <p>Which word to fill in the blank?</p> <p>It is always cold and snowy in ...</p> <p>For Naive Bayes, the words \"spring\", \"summer\", \"autumn\" and \"winter\" are all equally likely, but from the context, we know that \"winter\" is the most obvious candidate.</p>"},{"location":"lectures/naive_bayes/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Naive Bayes is a simple but powerful classifier that doesn't require to find any hyperparameters.</li> <li>Naive Bayes is based on Bayes Rule, which is a way to calculate the conditional probability \\(P(A|B)\\), given that we know \\(P(B|A)\\).</li> <li>By using Logarithms, we can avoid numerical underflow and simplify the calculations.</li> <li>For training a Naive Bayes classifier, we need to obtain the log ratio of probabilities for each word in the vocabulary.</li> <li>For prediction, we need to use those ratios to calculate the log likelihood of a tweet being positive or negative.</li> <li>The main limitation of Naive Bayes is the independence assumption, which assumes that the features are independent of each other, which typically isn't the case in NLP.</li> <li>However, because of its simplicity, Naive Bayes is often used as a baseline for text classification tasks and can perform surprisingly well.</li> </ul>"},{"location":"lectures/preface/","title":"Preface","text":"<p>Before we start with the course, let's briefly align our expectations and how we approach the fascinating topic of NLP.</p>"},{"location":"lectures/preface/#what-to-expect","title":"What to expect","text":"<p>This course is ...</p> <ul> <li>a practical introduction NLP</li> <li>hands-on: we will learn some important NLP libraries and frameworks</li> <li>collaborative: you are encouraged to contribute to the course content through GitHub</li> <li>driven by real-world use cases</li> <li>a course that will teach you the foundations of NLP and equip you with the knowledge and skills to apply NLP in your domain</li> <li>designed for software engineers and we will focus on the engineering perspective of NLP</li> </ul>"},{"location":"lectures/preface/#what-is-not-included","title":"What is not included","text":"<p>This course is not ...</p> <ul> <li>a theoretical deep dive into NLP algorithms and models: we rather want to know how we can apply NLP in practice</li> <li>primarily about ChatGPT and GenAI: we will cover these topics, but we rather focus on the foundations of NLP</li> <li>a survey of the latest NLP research: the field is moving fast, and we will rather build a good foundation that will allow you to keep up with the latest developments</li> <li>a course that will teach you how to build NLP models from scratch: we will rather focus on how to apply existing models and frameworks</li> </ul>"},{"location":"lectures/preface/#structure-of-the-course","title":"Structure of the course","text":"<p>The following diagram shows the structure we follow in the course. After doing some groundwork and covering NLP foundations, we will take a look at probabilistic NLP approaches. After that, we dive into deep learning and how we can use it for NLP tasks. Finally, we will take a look at large language models, generative artificial intelligence, and the current advances in this field.</p> <p></p>"},{"location":"lectures/preface/#academic-research-vs-practical-applications","title":"Academic research vs. practical applications","text":"<p>NLP is a broad field that encompasses a wide range of topics, including linguistics, computer science, and artificial intelligence. When working on NLP problems, it is important to know which perspective we are having on the problem.</p> <p>There is an academic perspective on NLP, which is focused on advancing the field through research and innovation. And there is a practical perspective on NLP, which is focused on applying NLP techniques to solve real-world problems. It is important to understand the difference between these two views, as they require different skills and lead to different career paths<sup>1</sup>.</p> <p>Depending on the perspective, the goals and metrics, the data and methodology, as well as the challenges that are faced, can be very different:</p> <ul> <li>As a software engineer in the industry, aspects like scalability, cost or robustness of your NLP system are more important than if you are a researcher in academia. But still, if you work on an NLP problem, it is important to understand basic NLP concepts. On the data side, you are likely to deal with noisy real-world data.</li> <li>As a researcher, your primary goal is to find answers to your research questions and optimize towards perfomance metrics (e.g. F1 score or perplexity) rather than dealing with devops topics or the latest frameworks and tools. At the same time, many research projects are driven by practical needs and are often inspired by real-world problems. On the data side, you usually have well-defined datasets available.</li> </ul> <p>The following figure illustrates those differences:</p> <p></p> <p>In this course, we will lean bit more towards the apllication side of NLP. While we will cover all important theoretical foundations, the main objective is to provide you with the skills that are required to successfully design and implement NLP systems in the industry.</p> <ol> <li> <p>https://medium.com/bitgrit-data-science-publication/nlp-in-academia-vs-the-real-world-9dee491bea38 \u21a9</p> </li> </ol>"},{"location":"lectures/preprocessing/","title":"Preprocessing","text":"<p>Natural language data is inherently unstructured and often contains noise, irregularities, and inconsistencies.</p> <p>Machine learning models cannot process raw text directly. Since text is categorical, it isn\u2019t compatible with the mathematical operations used to implement and train neural networks. Therefore, we need a way to represent words as continuous-valued vectors (aka embeddings).<sup>1</sup></p> <p>Embeddings</p> <p>As we will learn later, embeddings are a numerical representation of a token. In the beginning, we initialize them randomly. Later, during training, they are optimized to represent the token in a way that is useful for the task at hand.</p> <p>The goal of preprocessing is to transform raw text data into such embeddings so that we can use them for training machine learning models.</p> <p>In this lecture, we will look at some common preprocessing steps that are essential for preparing text data for NLP tasks.</p>"},{"location":"lectures/preprocessing/#overview","title":"Overview","text":"<p>The image shows the typical preprocessing steps of text data:<sup>1</sup></p> <p></p> <ul> <li>When we have the raw input text available, we need to tokenize it.</li> <li>Afterwards, the each individual token is mapped to an ID.</li> <li>Then, we convert the token IDs into embedding vectors.</li> </ul>"},{"location":"lectures/preprocessing/#the-pipeline-concept-in-nlp","title":"The pipeline concept in NLP","text":"<p>Like with many other complex problems, in NLP, it makes sense to break the problem that needs to be solved down into several sub-problems. This step-by-step processing is also referred to as a pipeline. Using a pipeline and breaking down an NLP problem into different steps offers several advantages that contribute to the overall efficiency and effectiveness of the NLP process:</p> <ol> <li> <p>Modularization and Reusability:     Breaking down the NLP process into distinct steps allows for modularization.     Each step can be designed as a separate module with well-defined inputs and outputs.     This modularity promotes code reusability and makes swapping out or updating individual components easier without affecting the entire system.</p> </li> <li> <p>Complexity Management: NLP tasks can be intricate, involving numerous subtasks and techniques.     By dividing the problem into manageable steps, it becomes easier to focus on each aspect separately.     This simplifies the development, debugging, and maintenance of the NLP solution.</p> </li> <li> <p>Parallelization and Efficiency:     Different steps of the pipeline can often be executed in parallel, speeding up the overall process.     For instance, while one part of the pipeline preprocesses data, another can perform feature engineering, enhancing computational efficiency.</p> </li> <li> <p>Experimentation and Iteration:     A structured pipeline allows researchers and practitioners to experiment with various techniques and algorithms at different stages.     This iterative approach facilitates the testing of different configurations and helps identify the best-performing components for each step.</p> </li> <li> <p>Collaboration:     When working in teams, a well-defined pipeline allows team members to focus on specific stages of the NLP process, enhancing collaboration and specialization.     Team members can work on their respective areas of expertise while contributing to the overall project.</p> </li> <li> <p>Debugging and Troubleshooting:     If a problem arises in a specific stage of the pipeline, it's easier to identify the source and address the issue when the process is divided into distinct steps.     This targeted approach simplifies debugging and reduces the scope of potential errors.</p> </li> <li> <p>Scalability:     As NLP projects grow in complexity, a modular pipeline can accommodate the addition of new components or steps as needed.     This scalability is especially important when dealing with evolving data sources and changing requirements.</p> </li> <li> <p>Adaptation to Different Tasks:     The pipeline structure can be adapted and reused for various NLP tasks.     By modifying or replacing specific steps, the same pipeline can be applied to tasks like sentiment analysis, text summarization, machine translation, and more.</p> </li> <li> <p>Documentation and Transparency:     A well-defined pipeline provides a clear outline of the entire NLP process.     This documentation is valuable for sharing insights, collaborating, and ensuring transparency in the development and deployment process.</p> </li> </ol> <p>In essence, the concept of a pipeline in NLP enhances organization, flexibility, collaboration, and maintainability throughout the development lifecycle. It facilitates the transformation of raw text data into valuable insights by systematically addressing the challenges specific to natural language processing tasks.</p> <p>Info</p> <p>Pipeline processing can be found in many areas of machine learning and computer science in general, e.g., data engineering or DevOps. An NLP pipeline can be seen as an adapted machine learning pipeline, as many of its steps apply to machine learning in general.</p> <p>The following figure shows a generic NLP pipeline, followed by a high-level description of each step. The color indicates whether the pipeline step is relevant for the course.<sup>2</sup><sup>3</sup></p> <p></p> <ol> <li> <p>Business Goal     An organization considering ML should have a clear idea of the problem and the business value to be gained by solving that problem.     You must be able to measure business value against specific business objectives and success criteria.</p> </li> <li> <p>Data Acquisition:     In this initial step, you gather the raw text data that you will use for your NLP task.     This could involve scraping websites, accessing databases, or any other method to collect relevant text documents.</p> </li> <li> <p>Pre-Processing:     Pre-processing includes a series of tasks like tokenization (breaking text into words or subword units), lowercasing, and stemming/lemmatization (reducing words to their base form).     This step helps standardize the text and make it ready for further analysis.</p> </li> <li> <p>Feature Engineering:     Feature engineering involves transforming the pre-processed text into numerical representations that machine learning models can work with.     This could include techniques like TF-IDF or word embeddings like Word2Vec or GloVe.</p> </li> <li> <p>Modeling:     In this step, you select and train a machine learning or deep learning model that suits your NLP task, such as text classification, NER, or machine translation.     The model learns patterns from the numerical representations of the text data.</p> </li> <li> <p>Evaluation:     After training your model, you need to assess its performance.     Evaluation involves using metrics like accuracy, precision, recall, F1-score, or others, depending on the task, to measure how well the model is performing on unseen data.</p> </li> <li> <p>Deployment:     Once you're satisfied with your model's performance, you deploy it to a production environment where it can be used to make predictions on new incoming text data.     This could involve integrating it into a web application, API, or other systems.</p> </li> <li> <p>Monitoring and Model Updating:     After deployment, continuously monitoring the model's performance in real-world scenarios is essential.     Suppose the model's accuracy drops or its predictions become less reliable over time due to changing patterns in the data. In that case, you might need to retrain or update the model to maintain its effectiveness.</p> </li> </ol> <p>Note</p> <p>Depending on the specific NLP task and the complexity of the data, you might need to delve deeper into each step and consider additional techniques or subtasks.</p>"},{"location":"lectures/preprocessing/#acquiring-data-for-nlp","title":"Acquiring data for NLP","text":"<p>Data acquisition techniques in Natural Language Processing (NLP) are crucial for obtaining high-quality data to train and evaluate models. Here's an overview of four common data acquisition techniques:</p> <ol> <li> <p>Web Scraping:     This involves extracting data from websites.     It's widely used for collecting large amounts of text data from various sources, such as news articles, blogs, and social media platforms.     Libraries like Beautiful Soup or Scrapy are commonly used for web scraping.</p> </li> <li> <p>Pre-existing Datasets:     Many publicly available datasets have been curated for specific NLP tasks.     Examples include the IMDb dataset for sentiment analysis, the Penn Treebank for language modeling, or the CoNLL dataset for NER.     Also, these datasets serve as benchmarks for various NLP tasks.</p> </li> <li> <p>Data Augmentation:     Data augmentation involves creating new data samples from existing ones by applying various transformations.     In NLP, this can involve techniques like paraphrasing, synonym replacement, and perturbation of sentences.     Augmentation helps increase the diversity of the training data and can improve model generalization.     A popular tool (not only) for data augmentation is snorkel.</p> </li> </ol> <p>In many cases, it's a combination of these techniques that makes a well-rounded and robust dataset for training and evaluating NLP models. The choice of technique depends on the specific task, available resources, and ethical considerations.</p> <p>The following code snippet is an example that shows how to parse a Wikipedia page.</p> <pre><code>import requests\nfrom bs4 import BeautifulSoup\n\nurl = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\nresponse = requests.get(url)\n\nsoup = BeautifulSoup(response.content, 'html.parser')\ninfobox = soup.find('table', class_='infobox')\nabstract_text = None\n\nif infobox:\n    rows = infobox.find_all('tr')\n    for row in rows:\n        header = row.find('th')\n        if header and 'Paradigm' in header.text:\n            abstract = row.find_next('p')\n            if abstract:\n                abstract_text = abstract.get_text()\n                break\n\nif abstract_text:\n    print(abstract_text)\nelse:\n    print(\"Abstract not found.\")\n</code></pre> <p>Warning</p> <p>As you can see, it already involves quite some custom logic and pitfalls and won't work if Wikipedia makes changes that affect the HTML tree. Many Websites offer APIs that allow much more straightforward access to their data via HTTP call, e.g., the Twitter API.</p> <p>Note</p> <p>While such techniques are valuable, each comes with its own challenges:</p> <ul> <li>Web scraping can be legally and ethically complex and often requires a lot of custom logic</li> <li>Pre-existing datasets might not always align perfectly with your task</li> <li>Human annotation can be expensive and time-consuming</li> <li>Data augmentation requires creativity to maintain semantic meaning.</li> </ul> <p>Tip</p> <p>As you may know, kaggle is an excellent source for browsing public datasets for various use cases. Also, the Linguistic Data Consortium has curated a top ten list of datasets for NLP tasks.</p> <p>Example</p> <p>The <code>data_acquisition.ipynb</code> notebook demonstrates how to acquire data from New York Times articles using the New York Times API.</p>"},{"location":"lectures/preprocessing/#working-with-text-data","title":"Working with text data","text":""},{"location":"lectures/preprocessing/#lowercasing","title":"Lowercasing","text":"<p>Convert all text to lowercase to ensure consistent handling of words regardless of their case.</p> <pre><code>&gt;&gt;&gt; txt = \"Welcome to the HTWG practical NLP course\"\n&gt;&gt;&gt; txt.lower()\n'welcome to the htwg practical nlp course'\n</code></pre> <p>Note</p> <p>Back in the days, having all text lowercase was common practice because it had a positive impact on the performance of machine learning models.</p> <p>But in the era of large language models (LLMs), lowercasing is not common anymore, and capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence structure, and learn to generate text with proper capitalization.</p> <p>Example</p> <p>In some cases, the case of the text may carry valuable information, such as in tasks related to NER, where distinguishing between \"US\" (the country) and \"us\" (the pronoun) is essential.</p> <p>Info</p> <p>Depending on the use case, you may need to do other string operations. Python offers a lot of useful string methods to work with text data.</p>"},{"location":"lectures/preprocessing/#remove-punctuation","title":"Remove punctuation","text":"<p>Punctuation marks like commas, periods, and semicolons can add complexity to text, but they might not always contribute essential information. By removing them, the text is simplified, and the complexity of the data is reduced, which can make it easier for certain NLP tasks to process and analyze.</p> <pre><code>&gt;&gt;&gt; from nltk.tokenize import word_tokenize\n&gt;&gt;&gt; txt = \"Hey, what are you up to? Let's have a pizza tonight!\"\n&gt;&gt;&gt; [word for word in word_tokenize(txt) if word.isalnum()]\n['Hey', 'what', 'are', 'you', 'up', 'to', 'Let', 'have', 'a', 'pizza', 'tonight']\n</code></pre>"},{"location":"lectures/preprocessing/#remove-html-tags-and-urls","title":"Remove HTML tags and URLs","text":"<p>When dealing with text from web pages, HTML tags are often present and need to be removed.</p> <pre><code>&gt;&gt;&gt; from bs4 import BeautifulSoup\n&gt;&gt;&gt; html = \"&lt;div&gt;&lt;h1&gt;Hello, World!&lt;/h1&gt;&lt;p&gt;This is a &lt;strong&gt;simple&lt;/strong&gt; HTML page.&lt;/p&gt;&lt;/div&gt;\"\n&gt;&gt;&gt; BeautifulSoup(html).text\n'Hello, World!This is a simple HTML page.'\n</code></pre> <p>Usually, it's also a good idea to remove URLs, as they often contain random characters and symbols and, therefore, add noise to the text.</p> <p>Note</p> <p>While you could also achieve it with a simple regex <code>re.sub(r'&lt;.*?&gt;', '', html)</code>, it might not cover all edge cases and HTML entities like, e.g., <code>&amp;nsbm</code>. Therefore, using a well-established library for such tasks is generally a better approach.</p>"},{"location":"lectures/preprocessing/#other-common-text-processing-steps","title":"Other common text processing steps","text":"<p>Here are some other common text processing steps worth mentioning:</p> <ol> <li> <p>Handling Numbers:     Decide whether to keep or remove numbers based on the analysis's goals.     You might replace numbers with placeholders like <code>NUM</code> or convert them to words.</p> </li> <li> <p>Handling Special Characters     Special characters like \"+\" or emojis often convey a meaning.     Depending on your application, you can choose to preserve emojis as-is, replace them with their textual descriptions, e.g., \"\u263a\ufe0f\" becomes \"smiley face\", or remove them altogether.     Not surprisingly, emojis are helpful for sentiment analysis.</p> </li> <li> <p>Removing whitespace:     Extra spaces, tabs <code>\\t</code>, or line breaks <code>\\n</code> should be normalized to a single space to ensure consistent formatting.</p> </li> <li> <p>Spelling Correction:     Human-typed data often has spelling errors. Custom solutions are usually not robust enough, but online services like Bing Spell Check offer REST APIs to tackle the spelling correction problem more sophisticatedly.</p> </li> <li> <p>Contractions expansion:     Especially in the English language, expanding contractions like \"can't\" to \"cannot\" or \"I'm\" to \"I am.\" should be resolved to ensure consistent word representations.</p> </li> <li> <p>Handling Abbreviations and Acronyms:     Expanding abbreviations and acronyms to their full forms, or vice versa, to ensure consistency.</p> </li> <li> <p>Normalization of Date and Time Formats:     Converting various date and time representations into a standardized format, for example, transforming <code>September 1st, 2023</code> to <code>2023-09-01</code>.</p> </li> </ol>"},{"location":"lectures/preprocessing/#tokenization","title":"Tokenization","text":"<p>Tokenization is the process of breaking down a text into substrings, like words or tokens. In English and many other languages, words are often separated by spaces, making space a common delimiter.</p> <p>We could use a simple regex to perform tokenization:</p> <pre><code>&gt;&gt;&gt; import re\n&gt;&gt;&gt; txt = \"The quick brown fox jumps over the lazy dog.\"\n&gt;&gt;&gt; re.split(r'\\s+', txt)\n['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n</code></pre> <p>But a simple regex might not always be sufficient in practice. Consider the following example:</p> <pre><code>&gt;&gt;&gt; txt = \"Hello, world. This, is a test.\"\n&gt;&gt;&gt; re.split(r'\\s+', txt)\n['Hello,', 'world.', 'This,', 'is', 'a', 'test.']\n</code></pre> <p>Usually, we also want to have the punctuation marks separated from the words, so a simple whitespace split is not sufficient anymore, and the regex will become more complex:</p> <pre><code>&gt;&gt;&gt; re.split(r'([,.]|\\s)', txt)\n['Hello', 'world', 'This', 'is', 'a', 'test']\n</code></pre> <p>Like this, the task of tokenization can become quite complex, and it's better to use dedicated libraries like NLTK or spaCy for tokenization.</p> <pre><code>&gt;&gt;&gt; from nltk.tokenize import word_tokenize\n&gt;&gt;&gt; txt = \"The quick brown fox jumps over the lazy dog.\"\n&gt;&gt;&gt; word_tokenize(txt)\n['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n</code></pre> <p>You can also tokenize at the level of sentences using the sentence tokenizer:</p> <pre><code>&gt;&gt;&gt; from nltk.tokenize import sent_tokenize\n&gt;&gt;&gt; txt = \"Salvatore has the best pizza in town. You should try the Calabrese. It's my favorite.\"\n&gt;&gt;&gt; sent_tokenize(txt)\n['Salvatore has the best pizza in town.', 'You should try the Calabrese.', \"It's my favorite.\"]\n</code></pre> <p>Info</p> <p>Which tokenizer to use can be a crucial decision for your NLP project. The code example above uses NLTK's default word tokenizer, but others are available. Please consider the API reference of NLTK's tokenize module for more information.</p> <p>The following sections show some more sophisticated tokenization techniques.</p>"},{"location":"lectures/preprocessing/#stopword-removal","title":"Stopword removal","text":"<p>Stopwords are common words (such as \"and,\" \"the,\" \"is,\" etc.) that don't carry significant meaning in a text. Stopword removal involves filtering out these words from a text to reduce noise and save processing time. The goal is to focus on the more meaningful content words that contribute to the overall context of the text.</p> <pre><code>&gt;&gt;&gt; from nltk.tokenize import word_tokenize\n&gt;&gt;&gt; from nltk.corpus import stopwords\n&gt;&gt;&gt; txt = \"Early morning walks provide fresh air and energy for the day.\"\n&gt;&gt;&gt; [word for word in word_tokenize(txt) if word not in stopwords.words('english')]\n['Early', 'morning', 'walks', 'provide', 'fresh', 'air', 'energy', 'day', '.']\n</code></pre> <p>Question</p> <p>In the era of LLMs, would you say it is still a good idea to remove stopwords?</p>"},{"location":"lectures/preprocessing/#stemming","title":"Stemming","text":"<p>Stemming is a process of reducing words to their base or root form by removing suffixes or prefixes. The resulting stem might not always be a valid word, but it's intended to capture the core meaning of the word. Some popular stemming algorithms include the Porter stemming algorithm and the Snowball stemmer. Stemming can be computationally faster than lemmatization since it involves simpler rule-based manipulations.</p> <pre><code>&gt;&gt;&gt; from nltk.stem import PorterStemmer\n&gt;&gt;&gt; stemmer = PorterStemmer()\n&gt;&gt;&gt; words = [\"happily\", \"flies\", \"feet\", \"denied\", \"sensational\", \"airliner\", \"cats\", \"houses\", \"agreed\", \"better\"]\n&gt;&gt;&gt; [stemmer.stem(word) for word in words]\n['happili', 'fli', 'feet', 'deni', 'sensat', 'airlin', 'cat', 'hous', 'agre', 'better']\n</code></pre> <p>Warning</p> <p>Stemming algorithms use heuristic rules to perform these transformations, which can lead to over-stemming (reducing words too aggressively) or under-stemming (not reducing words enough).</p>"},{"location":"lectures/preprocessing/#lemmatization","title":"Lemmatization","text":"<p>Lemmatization is a more advanced technique that reduces words to their base form, known as the lemma. Unlike stemming, lemmatization takes into account the word's morphological analysis and its POS to ensure that the resulting lemma is a valid word in the language. This makes lemmatization more accurate but potentially slower than stemming.</p> <pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer\n&gt;&gt;&gt; lemmatizer = WordNetLemmatizer()\n&gt;&gt;&gt; words = [(\"happily\", \"r\"), (\"flies\", \"v\"), (\"feet\", \"n\"), (\"denied\", \"v\"), (\"sensational\", \"a\"), (\"airliner\", \"n\"), (\"cats\", \"n\"), (\"houses\", \"n\"), (\"agreed\", \"v\"), (\"better\", \"a\")]\n&gt;&gt;&gt; [lemmatizer.lemmatize(*word) for word in words]\n['happily', 'fly', 'foot', 'deny', 'sensational', 'airliner', 'cat', 'house', 'agree', 'good']\n</code></pre> <p>Lemmatization is often preferred in tasks where maintaining the grammaticality of the words is crucial, such as language generation or language understanding tasks. It provides a cleaner and more linguistically accurate representation of words.</p> <p>Stemming is faster but might result in less linguistically accurate roots, while lemmatization is more accurate but can be slower due to its linguistic analysis. The choice between these techniques depends on the specific NLP task and the desired trade-off between speed and accuracy.</p> <p>Note</p> <p>Note that we require the POS tag as a second argument for NLTK's <code>WordNetLemmatizer</code>. This is why we use the Python unpacking operator <code>*</code> here</p>"},{"location":"lectures/preprocessing/#converting-tokens-into-embeddings","title":"Converting tokens into embeddings","text":"<p>Now, let\u2019s convert these tokens from a Python string to an integer representation to produce the token IDs. This conversion is an intermediate step before converting the token IDs into embedding vectors.<sup>1</sup></p> <p></p> <ul> <li>We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens.</li> <li>These individual tokens are then sorted alphabetically, and duplicate tokens are removed.</li> <li>The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value.</li> </ul> <p>The next goal is to apply this vocabulary to convert new text into token IDs:<sup>1</sup></p> <p></p> <ul> <li>Starting with a new text sample, we tokenize the text and use the vocabulary to convert the text tokens into token IDs.</li> <li>The vocabulary is built from the entire training set and can be applied to the training set itself and any new text samples.</li> </ul> <p>Encoding and Decoding</p> <p>The process of converting tokens into token IDs is also known as encoding, while the reverse process of converting token IDs back into tokens is also known as decoding.<sup>1</sup></p> <p></p> <p>Example</p> <p>According to the illustration, the following sentence:</p> <pre><code>The quick fox.\n</code></pre> <p>would be tokenized into the following tokens:</p> <pre><code>[\"The\", \"quick\", \"fox\"]\n</code></pre> <p>and could be represented by the following token IDs:</p> <pre><code>[7, 6, 2]\n</code></pre>"},{"location":"lectures/preprocessing/#adding-special-context-tokens","title":"Adding special context tokens","text":"<p>To address the issue of unknown words or other edge cases, we can add special context tokens to the vocabulary. These special tokens can be used to represent specific contexts or conditions that are not covered by the regular vocabulary tokens. For example, these special tokens can include markers for unknown words or document boundaries.<sup>1</sup></p> <p></p> <ul> <li>In this example, we add special tokens to a vocabulary to deal with certain contexts.</li> <li>For instance, we add an <code>&lt;|unk|&gt;</code> token to represent new and unknown words that were not part of the training data and thus not part of the existing vocabulary.</li> <li>Furthermore, we add an <code>&lt;|endoftext|&gt;</code> token that we can use to separate two unrelated text sources. This helps a model to understand that although these text sources are concatenated for training, they are, in fact, unrelated.</li> </ul>"},{"location":"lectures/preprocessing/#key-takeaways","title":"Key takeaways","text":"<ul> <li>In preprocessing, we convert raw input text into tokens, map them to IDs, and convert these IDs into embeddings.</li> <li>The NLP pipeline is a systematic approach to solving NLP problems by breaking them down into distinct steps.</li> <li>Many times, the success of an NLP project is determined already before the actual modeling step. Preprocessing and data acquisition play an important role, and in practice, much effort is spent on these steps.</li> <li>When working with text data, we can apply a variety of text processing techniques to clean and normalize the text, including lowercasing, removing punctuation, and handling special characters.</li> <li>Tokenization is the process of breaking down text into tokens, which is essential for further processing and analysis.</li> <li>Encoding refers to the process of converting tokens into token IDs, while decoding refers to the reverse process of converting token IDs back into tokens.</li> <li>We can add special context tokens to the vocabulary to address edge cases and unknown words that are not covered by the regular vocabulary tokens.</li> </ul> <ol> <li> <p>Image adapted from Raschka, Sebastian. Build a Large Language Model (From Scratch). Shelter Island, NY: Manning, 2024. https://www.manning.com/books/build-a-large-language-model-from-scratch.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Vajjala, Sowmya, S.V. Bharathi, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana. Practical Natural Language Processing: A Comprehensive Guide to Building Real-world NLP Systems. Sebastopol, CA: O'Reilly Media, 2020. https://www.practicalnlp.ai/.\u00a0\u21a9</p> </li> <li> <p>https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-lifecycle.html \u21a9</p> </li> </ol>"},{"location":"lectures/sequence_models/","title":"Sequence Models","text":"<p>In this lecture, we will cover sequence models. Starting with the limitations of N-gram language models, we will introduce recurrent neural networks along with some of their variants. We will also introduce the concept of the vanishing gradient problem, and explain how it can be addressed using long short-term memory networks.</p>"},{"location":"lectures/sequence_models/#limitations-of-n-gram-language-models","title":"Limitations of N-Gram Language Models","text":"<p>Recall that N-gram language models are used to compute the probability of a sequence of words. For that, we need to compute the conditional probability of a word given the \\(N-1\\) previous words. This approach has two main limitations:</p> <ul> <li>N-gram models consider only a fixed number of preceding words, i.e. \\(N-1\\), to predict the next word, and thus, have limited contextual information. This limitation results in the model being unable to capture long-range dependencies or understand the context beyond the immediate history.</li> <li>To capture dependencies of words that are very distant from each other, we need to use a large \\(N\\). This can be difficult to estimate without a large corpus. In practice, this can lead to sparsity issues, where many possible n-grams may not be observed in the training data.</li> <li>Even with a large corpus, such a model would require a lot of memory to store the counts of all possible \\(N\\)-grams.</li> </ul> <p>So for large \\(N\\), this becomes very impractical. A type of model that can help us with this is the recurrent neural network (RNN).</p> <p>long-range dependencies</p> <p>Consider the following sentence:</p> <p>Mary was supposed to study with me. I called her, but she did not &lt;?&gt;.</p> <p>Where the expected word is \"answer\".</p> <p>A traditional language model (let's say trigram) would probably predict a word like \"have\", since we can assume that the combination \"did not have\" is very frequent (or at least more frequent than the word \"answer\") in regular corpora.</p> <p>We would need a very large \\(N\\) to capture the dependency between \"did not\" and \"answer\", as we would also need to consider the beginning of the sentence \"I called her\".</p> <p>Penta-gram</p> <p>To predict the probability of a five-word sequence using a penta-gram model, we can use the following formula:</p> \\[P(w_1, w_2, w_3, w_4, w_5) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1, w_2) \\cdot P(w_4 | w_1, w_2, w_3) \\cdot P(w_5 | w_1, w_2, w_3, w_4)\\] <p>We can easily imagine that the larger the \\(N\\), the more sparse the data will be, as large (N-1)-grams are unlikely to appear in the corpus. Thus, the more difficult it will be to estimate the probabilities for the N-grams.</p> <p>Note that the formula above shows the sequence probability (which makes use of the Markov assumption), and not the N-gram probability.</p>"},{"location":"lectures/sequence_models/#recurrent-neural-networks","title":"Recurrent Neural Networks","text":"<p>RNNs propagate information from the beginning of a sequence through to the end. This allows them to capture long-range dependencies.</p> <p>We can see this process illustrated in the following figure:</p> <p></p> <p>Here is a summary of the steps:</p> <ul> <li>Each of the boxes represent the values computed at each particular step.</li> <li>The colors represent the information that is propagated through the network.</li> <li>The arrows indicate how the information is propagated through the network.</li> <li>The information from every word in the sequence is multiplied by the input weight matrix \\(W_x\\).</li> <li>To propagate information from one step to the next, we multiply the information from the previous step by the hidden state weight matrix \\(W_h\\).</li> <li>The hidden state at each time step is computed as a function of the previous hidden state \\(h_{t-1}\\) and the input at the current step \\(x_t\\).</li> </ul> <p>The hidden states are what allow the RNN to capture long-range dependencies. As we can see, in the last step, there is still information from the first step. This is what allows the RNN to capture long-range dependencies.</p> <p>Hidden State</p> <p>We can think of the hidden state as a memory or internal representation that the network updates as it processes each element of a sequence.</p> <p>The hidden state acts as a way for the network to maintain information about what it has seen so far and use that information to make predictions or decisions about the current input.</p> <p>Info</p> <p>The weights \\(W_x\\) and \\(W_h\\) are shared across all steps, that means we only need to learn them once, and then we can apply them to every step.</p> <p>This is why the RNN is called a recurrent neural network, because it performs the same task for every element of a sequence, with the output being dependent on the previous computations.</p> <p>Loss Function</p> <p>For RNNs, typically the cross entropy loss function is used.</p> <p>Implementation Note</p> <p>Tensorflows <code>tf.scan</code> function can be used to implement RNNs. It takes a function and applies it to all elements of a sequence. You can also pass an optional initializer, which is used to initialize the first element of the sequence.</p> <p>Here is a simple variant of the <code>scan</code> function, which shows how it basically works:</p> <pre><code>def scan(fn, elems, weights, h_0=None):\n    h_t = h_0\n    ys = []\n    for e in elems:\n        y, h_t = fn([e, h_t], weights)\n        ys.append(y)\n    return ys, h_t\n</code></pre> <p>Note that in Python, you can pass a function as an argument to another function.</p> <p>Gated Recurrent Units</p> <p>The Gated Recurrent Unit (GRU) is a variant of the RNN that is easier to train, and often performs better in practice.</p> <p>It is similar to the RNN, but it has two gates:</p> <ul> <li>The update gate controls how much of the previous state is kept.</li> <li>The reset gate controls how much of the previous state is forgotten.</li> </ul> <p>Tip</p> <p>In general, we can say that sequence models like RNNs have the form of a chain of repeating modules of neural networks.</p>"},{"location":"lectures/sequence_models/#bi-directional-rnns","title":"Bi-directional RNNs","text":"<p>In a bi-directional RNN, information flows in both directions.</p> <ul> <li>The forward RNN propagates information from the beginning of the sequence to the end.</li> <li>The backward RNN propagates information from the end of the sequence to the beginning.</li> </ul> <p>With this, a bi-directional RNN can capture information from both the past and the future context.</p> <p>Note that the computations of the forward and backward RNNs are independent of each other, and thus, can be parallelized.</p> <p></p> <p>Example</p> <p>Given the sentence:</p> <p>I was trying really hard to get a hold of &lt;?&gt;. Louise finally answered when I was about to give up.</p> <p>Since Louise doesn't appear until the beginning of the second sentence, a regular RNN would have to guess between \"her\", \"him\", or \"them\". However, a bi-directional RNN would be able to capture the context from the end of the sequence, and thus, would be able to predict \"her\" with a higher probability.</p> <p>Deep RNNs</p> <p>Another variant of RNNs are deep RNNs. Similar to deep neural networks, deep RNNs are RNNs with multiple hidden layers. They can be used to capture more complex patterns. We can think of them as multiple RNNs stacked on top of each other.</p> <ol> <li>Get the hidden state for the current layer (propagate information through time). \u27a1\ufe0f</li> <li>Use the hidden state of the current layer as input for the next layer (propagate information through layers). \u2b06\ufe0f</li> </ol> <p></p>"},{"location":"lectures/sequence_models/#vanishing-gradient-problem","title":"Vanishing Gradient Problem","text":"<p>While RNNs are very powerful, they have a major limitation: the vanishing gradient problem.</p> <p>Here is an illustration:</p> <p></p> <p>The problem can be summarized as follows:</p> <ul> <li>During the training of neural networks, the backpropagation algorithm is used to calculate gradients of the loss function with respect to the weights of the network.</li> <li>These gradients are then used to update the weights of the network.</li> <li>The vanishing gradient problem occurs when the gradients calculated by the backpropagation algorithm become very small as the algorithm progresses backward through the layers of the network.</li> <li>This means that the weights of the network are not updated significantly, and thus, the network is not trained effectively.</li> </ul> <p>We can summarize the advantages and disadvantages of RNNs as follows:</p> <ul> <li>\u2705 They can capture long-range dependencies</li> <li>\u2705 They can be used to process sequences of varying lengths</li> <li>\u2705 They use less memory than traditional N-gram language models</li> <li>\u274c They suffer from the vanishing or exploding gradient problems</li> <li>\u274c Struggle with very long sequences</li> </ul> <p>Tip</p> <p>Think of the weights as factors that influence the importance of information at each step. If these factors are repeatedly small, the overall impact of the information from earlier steps diminishes exponentially as you go back in time.</p> <p>Here is an analogy:</p> <p>It's like whispering a secret in a long line of people, and each person passing it on speaks in a softer voice. By the time it reaches the end of the line, the original message is barely audible.</p> <p>Backpropagation</p> <p>The backpropagation algorithm is used in the training of deep neural networks (i.e. networks with more than one hidden layer). It is used to update the weights of the network by propagating the error backwards through the network.</p> <p>Exploding Gradient Problem</p> <p>The opposite of the vanishing gradient problem is the exploding gradient problem. This occurs when the gradients calculated by the backpropagation algorithm become very large as the algorithm progresses backward through the layers of the network.</p> <p>This can cause the weights of the network to be updated too much, and thus, the network is also not trained effectively.</p> <p>Further Reading</p> <p>Here are some nice blog posts for further reading:</p> <ul> <li>Intro to Optimization in Deep Learning: Vanishing Gradients and Choosing the Right Activation Function</li> <li>Intro to optimization in deep learning: Gradient Descent</li> </ul>"},{"location":"lectures/sequence_models/#long-short-term-memory-networks","title":"Long Short-Term Memory Networks","text":"<p>Long Short-Term Memory (LSTM) networks are a type of RNN that are designed to address the vanishing gradient problem.</p> <p>The following figure shows the architecture of an LSTM model<sup>1</sup>:</p> <p></p> <p>LSTM allows your model to remember and forget certain inputs. It does this by using a combination of two states:</p> <ul> <li>Cell state, that can be thought of as a memory cell. It is the horizontal line running through the top of the diagram. It is like a conveyor belt and runs straight down the entire chain, with only some minor linear interactions.</li> <li>Hidden state, that is used to propagate information through time. It outputs \\(h_t\\) and is the vertical line running through the right of the diagram.</li> </ul> <p>The hidden state is used to control the flow of information in the LSTM. It consists of three gates. Gates are a way to optionally let information through. They are passed in the following order:</p> <ul> <li>Forget gate: decides what information should be thrown away or kept (left sigmoid gate)</li> <li>Input gate: decides which values from the input to update (middle sigmoid and tanh gates)</li> <li>Output gate: decides what information to pass over to the next hidden state (right sigmoid gate)</li> </ul> <p>The gates allow the gradients to flow unchanged. This means, the risk of vanishing or exploding gradients is mitigated.</p> <p>An LSTM model is a chain of repeating LSTM units.</p> <p>Example</p> <p>Here is a little analogy that may help you to better understand the idea of LSTMs:</p> <p>Imagine you receive a phone call from a friend. \u260e\ufe0f \ud83d\udc67</p> <p>At the time your phone rings, you might be thinking of any number of things unrelated to your friend. \ud83c\udfc3\ud83c\udf55\ud83c\udfd6\ufe0f</p> <p>This is similar to the cell state at the beginning of the loop.</p> <p>When you answer the call, you put aside those unrelated thoughts while retaining anything you need to talk to your friend about. \ud83d\udc67</p> <p>This is similar to what the forget gate does.</p> <p>As your conversation progresses, you'll be taking in all the new information from your friend while also thinking of what else might be relevant to talk about next. \ud83e\udd73\u2615\ud83c\udfd9\ufe0f</p> <p>This is similar to what the input gates does.</p> <p>Finally, you decide what to say next. \ud83c\udf54</p> <p>This is similar to what the output gate does.</p> <p>You will then continue to evolve the conversation in this way until you hang up at the end of the call, leaving you with a new set of thoughts and memories. \u260e\ufe0f\ud83d\udc4b</p> <p>your cell states have been updated a few times since you began your conversation</p> <p>Tip</p> <p>We can say that an LSTM is able to learn which information it should remember and which information it should forget.</p> <p>Info</p> <p>LSTMs were introduced in the paper Long Short-Term Memory by Hochreiter and Schmidhuber (1997).</p> <p>Further Reading</p> <p>Here is a nice blog post that explains for better understanding the concepts of RNNs and LSTMs: Understanding LSTM Networks.</p>"},{"location":"lectures/sequence_models/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>N-gram language models have limited contextual information, and thus, have difficulty capturing long-range dependencies.</li> <li>Recurrent Neural Networks (RNNs) are a type of neural network that can capture long-range dependencies.</li> <li>They achieve this by propagating information from the beginning of a sequence through to the end.</li> <li>There are some variants of RNNs, such as bi-directional RNNs and deep RNNs, that can be used to capture more complex patterns.</li> <li>Even though they can capture long range dependencies, RNNs suffer from the vanishing gradient problem. This occurs when the gradients calculated by the backpropagation algorithm become very small. As a result, the weights of the network are not updated significantly, and thus, the network is not trained effectively.</li> <li>Long Short-Term Memory (LSTM) networks are a type of RNN that are designed to address the vanishing gradient problem. They are able to learn which information it should remember and which information it should forget.</li> </ul> <ol> <li> <p>https://colah.github.io/posts/2015-08-Understanding-LSTMs/ \u21a9</p> </li> </ol>"},{"location":"lectures/vector_space_models/","title":"Vector Space Models","text":"<p>Vector space models are a way of representing the meaning of words in a document. They are a fundamental concept in NLP, and are used in many applications such as document classification, information retrieval, and question answering.</p>"},{"location":"lectures/vector_space_models/#introduction","title":"Introduction","text":"<p>Using vector space models, we can capture similarities, differences, dependencies or many other relationships between words.</p> <p>Example</p> <p>The following sentence have very similar words, but they have different meanings:</p> <p>Where are you from?</p> <p>Where are you going?</p> <p>In contrast, the following sentences have very different words, but they have similar meanings:</p> <p>What is your age?</p> <p>How old are you?</p> <p>Vector space models can be used to capture such similarities and differences between words.</p> <p>Example</p> <p>In the following sentence, the word cereal and the word spoon are related.</p> <p>You eat cereal with a spoon.</p> <p>In the following sentence, the word sell depends on the word buy.</p> <p>You sell something to someone who buys it.</p> <p>Vector space models help us to capture such and many other relationships between words.</p> <p>John Firth, 1957</p> <p>You shall know a word by the company it keeps.</p> <p>This is one of the most fundamental concepts in NLP. When using vector space models, the way that representations are made is by identifying the context around each word in the text, which captures the relative meaning.</p> <p>When learning these vectors, we usually make use of the neighboring words to extract meaning and information about the center word.</p> <p>If we would cluster vectors together, we can observe that adjectives, nouns, verbs, etc. tend to be near to one another.</p> <p>Question</p> <p>In vector space models, synonyms and antonyms are very close to one another. Why do you think this is the case?</p>"},{"location":"lectures/vector_space_models/#co-occurrence-matrix","title":"Co-Occurrence Matrix","text":"<p>The co-occurrence matrix is a matrix that counts the number of times that a word appears in the context of other words within a given window size \\(k\\).</p> <p>From the co-occurrence matrix, we can extract the word vectors.</p> <p>The vector representation of a word is called a word embedding.</p> <p>We can use those word embeddings to find relationships between words.</p> <p>In the following, we will look at two different approaches to create word embeddings.</p> <p></p> <p>Info</p> <p>The terms word vector is often used interchangeably with word embedding. Both terms refer to a numerical representation of words in a continuous vector space.</p>"},{"location":"lectures/vector_space_models/#word-by-word-design","title":"Word by Word Design","text":"<p>In the word by word design, the co-occurrence matrix counts the number of times that a word appears in the context of other words within a given window size \\(k\\).</p> <p>Example</p> <p>Suppose we have the following two sentences:</p> <p>I like simple data</p> <p>I prefer simple raw data</p> <p>With a window size of \\(k=2\\), the co-occurrence matrix would look as follows:</p> I like prefer simple raw data I 0 1 1 2 0 0 like 1 0 0 1 0 1 prefer 1 0 0 1 1 0 simple 2 1 1 0 1 2 raw 0 0 1 1 0 1 data 0 1 0 2 1 0 <p>If we look more closely at the word data, we can see that it appears in the context of the word simple twice, and in the context of the word and once, given a window size of \\(k=2\\).</p> <p>So the word data can be represented as the following vector:</p> \\[x_{data} = [0, 1, 0, 2, 1, 0]\\] <p>Note that the vector is of size \\(n\\), where \\(n\\) is the number of unique words in the vocabulary.</p> <p>Note</p> <p>The co-occurance matrix, as shown here, shows the unigram counts. Later in the course, we will also look at bigram counts, or n-gram counts in general, which allow us to capture more context. This concept is the basis for language models.</p>"},{"location":"lectures/vector_space_models/#word-by-document-design","title":"Word by Document Design","text":"<p>For a word by document design, the process is quite similar.</p> <p>But instead of counting co-occurrences of words, we count the number of times that a word appears in documents of a specific category.</p> <p>Example</p> <p>Let's assume our corpus contains documents of three categories:</p> <ul> <li>entertainment</li> <li>economy</li> <li>machine learning</li> </ul> <p>For the words data and movie, we could assume the following counts per category:</p> entertainment economy machine learning data 1000 4000 9000 movie 7500 2000 500 <p>So the word data can be represented as the following vector:</p> \\[x_{data} = [1000, 4000, 9000]\\] <p>And the word movie can be represented as the following vector:</p> \\[x_{movie} = [7500, 2000, 500]\\] <p>Note that the vector is of size \\(n\\), where \\(n\\) is the number of categories.</p> <p>We could visualize those vectors in a vector space as follows:</p> <p></p> <p>Note</p> <p>For the sake of drawing, instead of using the categories as the axis and showing the word vectors, the figure shows the vectors of the categories.</p> <p>However, we should see similar results if we would draw the word vectors in a three-dimensional vector space, using the categories as the axes (as you should usually do).</p>"},{"location":"lectures/vector_space_models/#eucledian-distance","title":"Eucledian Distance","text":"<p>The Euclidean distance between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) in an n-dimensional space is defined as follows:</p> \\[ \\begin{align} d &amp;= \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\dots + (x_n - y_n)^2} \\\\ &amp;= \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} \\end{align} \\] <p>It is the straight line between two points in an n-dimensional space, and derived from the Pythagorean theorem.</p> <p>We can also interpret it as the length of the vector that connects the two points (aka the norm of the vector).</p> <p>Pythagoraen Theorem</p> <p>In a right triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides.</p> \\[ c^2 = a^2 + b^2 \\] <p>Using the Euclidean distance, we can calculate how close two vectors are and thus, use it as a similarity metric.</p> <p></p> <p>Example</p> <p>If we continue with the example from above, we can calculate the Euclidean distance between the machine learning category vector \\(\\mathbf{ml}\\) and the entertainment category vector \\(\\mathbf{e}\\) as follows:</p> <p>Let the two vectors be:</p> \\[ \\begin{align} \\mathbf{ml} &amp;= [9000, 500] \\\\ \\mathbf{e} &amp;= [1000, 7500] \\end{align} \\] <p>Then the Euclidean distance is calculated as follows:</p> \\[ \\begin{align} d &amp;= \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} \\\\ &amp;= \\sqrt{(ml_1 - e_1)^2 + (ml_2 - e_2)^2} \\\\ &amp;= \\sqrt{(9000 - 1000)^2 + (500 - 7500)^2} \\\\ &amp;= 8000 \\end{align} \\] <p>As you can see from the formula above, this generalizes to any number of dimensions.</p> <p>Example</p> <p>Let's assume we have the following co-oocurrence matrix, and we want to calculate the Euclidean distance between the words beer and pizza.</p> data beer pizza AI 6 0 1 drinks 0 4 6 food 0 6 8 <p>Based on this co-occurrence matrix, we can represent the words beer and pizza as the following vectors:</p> \\[ \\begin{align} \\mathbf{b} &amp;= [0, 4, 6] \\\\ \\mathbf{p} &amp;= [1, 6, 8] \\end{align} \\] <p>Then the Euclidean distance is calculated as follows:</p> \\[ \\begin{align} d &amp;= \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2} \\\\ &amp;= \\sqrt{(b_1 - p_1)^2 + (b_2 - p_2)^2 + (b_3 - p_3)^2} \\\\ &amp;= \\sqrt{(0 - 1)^2 + (4 - 6)^2 + (6 - 8)^2} \\\\ &amp;= \\sqrt{1 + 4 + 4} \\\\ &amp;= 3 \\end{align} \\] <p>In Python, we can calculate the Euclidean distance using the <code>numpy.linalg.norm</code> function.</p> <pre><code>import numpy as np\n\n# Define two vectors\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Calculate the Euclidean distance\ndistance = np.linalg.norm(a - b)\n\nprint(\"Euclidean distance:\", distance)\n</code></pre> <p>Note</p> <p>The <code>numpy.linalg.norm</code> function calculates the norm of a vector or matrix.</p> <p>Since the norm of a vector is its length, the function can be used to calculate the Euclidean distance between two vectors.</p>"},{"location":"lectures/vector_space_models/#cosine-similarity","title":"Cosine Similarity","text":"<p>The cosine similarity is another way of measuring the similarity between two vectors.</p> <p>The intuition is that vectors pointing in the similar direction share a similar proportion of words, and thus, are more similar to each other.</p> <p>Let's take a look at the following figure:</p> <p></p> <p>Normally, the food and agriculture categories would be considered more similar, because they share a similar proportion of words.</p> <p>However, the Euclidean distance would suggest that the agriculture and history categories are more similar to each other than the agriculture and food categories, since \\(d_2 &lt; d_1\\).</p> <p>This is because the Euclidean distance is biased towards longer vectors, or in our case, categories with more words.</p> <p>To avoid that, we could also compare the angles, or the cosine of the angles, between the vectors to measure their similarity.</p> <p>As the figure shows, the angle between the agriculture and food categories is smaller than the angle between the agriculture and history categories, and thus, would be a better measure of similarity in this case.</p> <p>Info</p> <p>When corpora are different in size, the Euclidean distance is biased towards longer vectors. In such cases, it is better to use the cosine similarity as a measure of similarity.</p> <p>Note</p> <p>Form the figure, we can see that:</p> <ul> <li>vectors pointing in the similar direction means their word frequencies are similar, and</li> <li>vectors pointing in different directions means the word frequencies are dissimilar.</li> </ul> <p>Let's take a look at the math behind the cosine similarity.</p> <p>The norm of a vector \\(\\mathbf{x}\\), or its length, is defined as the square root of the sum of the squared vector elements:</p> \\[ \\|\\mathbf{x}\\| = \\sqrt{\\sum_{i=1}^n x_i^2} \\] <p>The dot product of two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is defined as the sum of the products of the corresponding vector elements:</p> \\[ \\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i \\cdot y_i \\] <p>From trigonometry, we know that the cosine of the angle is defined as follows:</p> \\[ \\cos(\\theta) = \\frac{\\text{adjacent}}{\\text{hypotenuse}} \\] <p>In vector space,</p> <ul> <li>the adjacent is the dot product of the two vectors (the projection of one vector onto the other), and</li> <li>the hypotenuse is the product of the norms of the two vectors,</li> </ul> <p>which leads us to the following formula:</p> \\[ \\cos(\\theta) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\] <p>The following figure shows how we can use the cosine similarity to measure the similarity between two vectors.</p> <p></p> <ul> <li>If the vectors are orthogonal, like the vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\), the cosine similarity is 0, since \\(\\cos(90) = 0\\).</li> <li>If the vectors point exactly in the same direction, like the vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), the cosine similarity is 1, since \\(\\cos(0) = 1\\).</li> </ul> <p>Note</p> <p>Since we are dealing with word counts, there won't be any negative values in the vectors, and our vectors will always point in the first quadrant.</p> <p>Example</p> <p>Given the following diagram</p> <p></p> <p>we can calculate the cosine similarity between the vectors for the categories agriculture \\(\\mathbf{a}\\) and history \\(\\mathbf{h}\\) as follows:</p> <p>Let the two vectors be:</p> \\[ \\begin{align} \\mathbf{a} &amp;= [10, 40] \\\\ \\mathbf{h} &amp;= [30, 10] \\end{align} \\] <p>Then the cosine similarity is calculated as follows:</p> \\[ \\begin{align} \\cos(\\theta) &amp;= \\frac{\\mathbf{a} \\cdot \\mathbf{h}}{\\|\\mathbf{a}\\| \\|\\mathbf{h}\\|} \\\\ &amp;= \\frac{\\sum_{i=1}^n a_i h_i}{\\sqrt{\\sum_{i=1}^n a_i^2} \\sqrt{\\sum_{i=1}^n h_i^2}} \\\\ &amp;= \\frac{(10 \\times 30) + (40 \\times 10)}{\\sqrt{(10^2 + 40^2)} \\sqrt{(30^2 + 10^2)}} \\\\ &amp;= 0.5368 \\end{align} \\] <p>Example</p> <p>Let's assume we have the following co-oocurrence matrix, and we want to calculate the cosine similarity between the words beer and pizza.</p> data beer pizza AI 6 0 1 drinks 0 4 6 food 0 6 8 <p>Based on this co-occurrence matrix, we can represent the words beer and pizza as the following vectors:</p> \\[ \\begin{align} \\mathbf{b} &amp;= [0, 4, 6] \\\\ \\mathbf{p} &amp;= [1, 6, 8] \\end{align} \\] <p>Then the cosine similarity is calculated as follows:</p> \\[ \\begin{align} \\cos(\\theta) &amp;= \\frac{\\mathbf{b} \\cdot \\mathbf{p}}{\\|\\mathbf{b}\\| \\|\\mathbf{p}\\|} \\\\ &amp;= \\frac{\\sum_{i=1}^n b_i p_i}{\\sqrt{\\sum_{i=1}^n b_i^2} \\sqrt{\\sum_{i=1}^n p_i^2}} \\\\ &amp;= \\frac{(0 \\times 1) + (4 \\times 6) + (6 \\times 8)}{\\sqrt{(0^2 + 4^2 + 6^2)} \\sqrt{(1^2 + 6^2 + 8^2)}} \\\\ &amp;= 0.9935 \\end{align} \\] <p>Here is a NumPy implementation of the cosine similarity:</p> <pre><code>import numpy as np\n\n# Define two vectors\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Calculate the cosine similarity\nsimilarity = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\nprint(\"Cosine similarity:\", similarity)\n</code></pre>"},{"location":"lectures/vector_space_models/#working-with-word-vectors","title":"Working with Word Vectors","text":"<p>We can use word vectors to find relationships between words.</p> <p>Using simple vector arithmetics, we can already do some interesting things, like finding the capital of a country.</p> <p></p> <p>Knowing that the capital of Germany is Berlin, we can use this relationship to find the capital of the USA.</p> <p>Question</p> <p>If you were given the pair Australia and Sydney instead of Germany and Berlin, where do you think we would end up in the vector space for the USA?</p> <p>Example</p> <p>A similar analogy would be the following:</p> <p>prince \ud83e\udd34 - male \u2642\ufe0f + female \u2640\ufe0f = princess \ud83d\udc78</p> <p>Suppose we have the following vector space, that has vector representations for countries and their capitals.</p> <p></p> <p>We can express the relationship between a country and its capital by the difference between their vectors.</p> <p>Using this relationship, we can find the capital of a country by adding the difference vector to the vector of the country.</p> <p>Usually, we do not end up exactly at the capital, so we need to utilize similarity metrics to find the closest vector.</p> <p>As we learned, we could use the Euclidean distance or the cosine similarity to achieve this.</p> <p>In the end, we can leverage known relationships between words to find unknown relationships and make predictions.</p> <p>Example</p> <p>Form the figure, we can see that</p> \\[ \\begin{align} \\mathbf{v}_{\\text{Germany}} = [5, 6]\\\\ \\mathbf{v}_{\\text{Berlin}} = [10,5] \\end{align} \\] <p>Given these vectors, we can derive that the relationship \\(\\mathbf{r}\\) between Germany and its capital Berlin can be expressed by the difference between the two vectors:</p> \\[ \\begin{align} \\mathbf{r} &amp;= \\mathbf{v}_{\\text{Germany}} - \\mathbf{v}_{\\text{Berlin}} \\\\ &amp;= [10, 5] - [5, 6] \\\\ &amp;= [5, -1] \\end{align} \\] <p>So the relationship \\(\\mathbf{r}\\) between country and capital can be expressed by the vector \\([5, -1]\\).</p> <p>If we add this vector to the vector for the USA, we should end up close to the capital of the USA.</p> \\[ \\begin{align} \\mathbf{v}_{\\text{dest}} &amp;= \\mathbf{v}_{\\text{USA}} + \\mathbf{r} \\\\ &amp;= [5, 5] + [5, -1] \\\\ &amp;= [10, 4] \\end{align} \\] <p>Now we can use the Euclidean distance or the cosine similarity to find the capital that is closest to the vector \\(\\mathbf{v}_{\\text{dest}}\\).</p> <p>Looking at the figure, the capital that is closest to the vector \\(\\mathbf{v}_{\\text{dest}}\\) is Washington DC with \\(\\mathbf{v}_{\\text{Washington DC}} = [9, 3]\\).</p> <p>Having words represented in a vector space allows us to capture relative meaning of words and find patterns in text. This is the basis for many advanced NLP tasks.</p> <p>As you can imagine, similar words will have similar vectors, and thus, will be close to each other in the vector space.</p> <p>Example</p> <p>Given the word doctor, if we look at the words that are close to it in the vector space, we can see that they are probably all related to the medical field. For example: nurse, hospital, patient, medicine, etc.</p> <p>We can also imagine that all countries will be close to each other, and all cities will be close to each other, or all animals will be close to each other, etc.</p> <p>Also we can expect that sub groups like water animals, land animals, etc. will be grouped together.</p> <p>Info</p> <p>For a demo, please see the related notebook <code>vector_space_models.ipynb</code>.</p>"},{"location":"lectures/vector_space_models/#transforming-word-vectors","title":"Transforming Word Vectors","text":"<p>We can make use of vector transformations to build a simple translation system.</p> <p>Example</p> <p>Let's assume that in the english language, the word cat \ud83d\udc08 is represented by the vector</p> \\[ \\mathbf{v}_{\\text{cat}} = [1, 3, 4] \\] <p>And in the french language, the word chat \ud83d\udc08 is represented by the vector</p> \\[ \\mathbf{v}_{\\text{chat}} = [2, -4, -1] \\] <p>Then we want to find a transformation operation that transforms the english word vector \\(\\mathbf{v}_{\\text{cat}}\\) \ud83d\udc08 into the french word vector \\(\\mathbf{v}_{\\text{chat}}\\) \ud83d\udc08.</p> <p>The basic idea is that one vector space can be transformed into another vector space using a rotation matrix.</p> <p>Example</p> <p>If we have the english word vectors for cat, dog, and bird, and the french word vectors for chat, chien, and oiseau, we can find a rotation matrix that transforms the english word vectors into the french word vectors.</p> <p></p> <p>Mathematically, we want to minimize the distance between the dot product of the two matrices \\(\\mathbf{X} \\mathbf{R}\\) and the matrix \\(\\mathbf{Y}\\).</p> \\[ \\mathbf{X} \\mathbf{R} \\approx \\mathbf{Y} \\] <p>We can find the rotation matrix \\(\\mathbf{R}\\) by calculating a loss function that measures the difference between the dot product of the two matrices \\(\\mathbf{X} \\mathbf{R}\\) and the matrix \\(\\mathbf{Y}\\).</p> <ol> <li>Initialize the rotation matrix \\(\\mathbf{R}\\) with random values.</li> <li>Calculate the dot product of the two matrices \\(\\mathbf{X} \\mathbf{R}\\).</li> <li>Calculate the loss function by comparing the dot product of the two matrices \\(\\mathbf{X} \\mathbf{R}\\) and the matrix \\(\\mathbf{Y}\\).</li> <li>Update the rotation matrix \\(\\mathbf{R}\\) using gradient descent.</li> <li>Repeat steps 2-4 until the loss function is minimized.</li> </ol> <p>Once we have the rotation matrix \\(\\mathbf{R}\\), we can use it to transform the word vectors from one language into another. We will end up somewhere in the vector space of the other language, and then apply a similarity metric to find candidates for the translation.</p> <p>Example</p> <p>Let's assume we have the following three english word vectors:</p> \\[ \\begin{align} \\mathbf{v}_{\\text{cat}} &amp;= [1, 3, 4] \\\\ \\mathbf{v}_{\\text{dog}} &amp;= [2, 2, 2] \\\\ \\mathbf{v}_{\\text{bird}} &amp;= [3, 1, 0] \\end{align} \\] <p>And the following three equivalent french word vectors:</p> \\[ \\begin{align} \\mathbf{v}_{\\text{chat}} &amp;= [2, -4, -1] \\\\ \\mathbf{v}_{\\text{chien}} &amp;= [2, -2, -2] \\\\ \\mathbf{v}_{\\text{oiseau}} &amp;= [2, -2, -1] \\end{align} \\] <p>We can represent the english vectors as a matrix \\(\\mathbf{X}\\) and the french vectors as matrix \\(\\mathbf{Y}\\) as follows:</p> \\[ \\mathbf{X} = \\begin{pmatrix}   1 &amp; 3 &amp; 4 \\\\   2 &amp; 2 &amp; 2 \\\\   3 &amp; 1 &amp; 0 \\end{pmatrix} \\] \\[ \\mathbf{Y} = \\begin{pmatrix}   2 &amp; -4 &amp; -1 \\\\   2 &amp; -2 &amp; -2 \\\\   2 &amp; -2 &amp; -1 \\end{pmatrix} \\] <p>Now, we are lookng for a rotation matrix \\(\\mathbf{R}\\) that transforms the word vectors \\(\\mathbf{v}_{\\text{cat}}\\), \\(\\mathbf{v}_{\\text{dog}}\\), and \\(\\mathbf{v}_{\\text{bird}}\\) into the word vectors \\(\\mathbf{v}_{\\text{chat}}\\), \\(\\mathbf{v}_{\\text{chien}}\\), and \\(\\mathbf{v}_{\\text{oiseau}}\\) such that:</p> \\[ \\begin{align} \\begin{pmatrix}   1 &amp; 3 &amp; 4 \\\\   2 &amp; 2 &amp; 2 \\\\   3 &amp; 1 &amp; 0 \\end{pmatrix} \\mathbf{R} &amp;= \\begin{pmatrix}   2 &amp; -4 &amp; -1 \\\\   2 &amp; -2 &amp; -2 \\\\   2 &amp; -2 &amp; -1 \\end{pmatrix} \\end{align} \\] <p>Here is a NumPy implementation of such a transformation using <code>numpy.dot</code>:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 3, 4], [2, 2, 2], [3, 1, 0], [4, 2, 1], [5, 1, 3]])\n&gt;&gt;&gt; R = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n&gt;&gt;&gt; np.dot(X, R)\narray([[1, 3, 4],\n       [2, 2, 2],\n       [3, 1, 0],\n       [4, 2, 1],\n       [5, 1, 3]])\n</code></pre> <p>Question</p> <p>Why do we want to find a rotation matrix \\(\\mathbf{R}\\) that transforms the english word vectors into the french word vectors, instead of just using a dictionary to translate the words?</p>"},{"location":"lectures/vector_space_models/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Vector space models are a way of representing the meaning of words in a document. They are a fundamental concept in NLP, and are used in many applications such as document classification, information retrieval, and question answering.</li> <li>Using vector space models, we can capture similarities, differences, dependencies or many other relationships between words.</li> <li>With vector space models, we can capture the relative meaning of words by identifying the context around each word in the text.</li> <li>Using the co-occurrence matrix, we can extract the word vectors. The vector representation of a word is called a word embedding.</li> <li>In this lecture, we learned two different approaches to create word embeddings: the word by word design and the word by document design.</li> <li>When we have word vectors available, we can use a similarity metric like the Euclidean distance or the cosine similarity to measure the similarity between vectors.</li> <li>Using simple vector arithmetics, we can put word vectors into relation with each other and find interesting relationships between words.</li> <li>Translation can be done by finding a rotation matrix that transforms the word vectors from one language into another.</li> <li>Understanding the concept of vector space models is the basis for many advanced NLP tasks.</li> </ul>"},{"location":"lectures/word_embeddings/","title":"Word Embeddings","text":"<p>In this lecture, we will learn about word embeddings, which are a way to represent words as vectors. We will learn about the CBOW model, which is a machine learning model that learns word embeddings from a corpus.</p> <p>Deep learning models cannot process data formats like video, audio, and text in their raw form. Thus, we use an embedding model to transform this raw data into a dense vector representation that deep learning architectures can easily understand and process. Specifically, this figure illustrates the process of converting raw data into a three-dimensional numerical vector.</p> <p></p> <p>At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space. The primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.<sup>1</sup></p> <p></p> <p>Info</p> <p>There are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval-augmented generation. Retrieval augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text.</p>"},{"location":"lectures/word_embeddings/#revisit-one-hot-encoding","title":"Revisit One-Hot Encoding","text":"<p>In the lecture about feature extraction, we have seen that we can represent words as vectors using one hot encoding.</p> <p>One-hot encoding is a very simple way to represent words as vectors, but it has some major disadvantages:</p> <ul> <li>\u274c the resulting vectors are very high dimensional, i.e. one dimension for each word in the vocabulary: \\(n_{\\text{dim}} = |V|\\)</li> <li>\u274c it does not capture meaning, i.e. all words have the same distance to each other:</li> </ul> <p></p>"},{"location":"lectures/word_embeddings/#word-embeddings-overview","title":"Word Embeddings Overview","text":"<p>From the lecture about vector space models, we already know that similar words should be close to each other in the vector space.</p> <p>But how can we achieve this? In this lecture we will learn about word embeddings, which are a way to represent words as vectors.</p> <p></p> <p>In the figure above, we have two dimensional word embeddings:</p> <ul> <li>the first dimension represents the word's sentiment in terms of positive or negative.</li> <li>the second dimensions indicates whether the word is more concrete or abstract.</li> </ul> <p>In the real world, word embeddings are usually much higher dimensional, e.g. 100 or 300 dimensions.</p> <p>Each dimension represents a different aspect of the word. We do not know what exactly each dimension represents, but we know that similar words are close to each other in the vector space.</p> <p>Word embeddings have the following advantages:</p> <ul> <li>\u2705 they are dense, i.e. they do not contain many zeros</li> <li>\u2705 they are low dimensional, i.e. they do not require much memory</li> <li>\u2705 they allow us to encode meaning</li> <li>\u2705 they capture semantic and syntactic information</li> <li>\u2705 they are computationally efficient</li> </ul> <p>Note</p> <p>Note that in the word by word design, we have as many features as we have words in our vocabulary. This is not very efficient, because we have to store a lot of zeros. With word embeddings, we can reduce the number of features to a much smaller number, e.g. 100 or 300, while at the same time capturing the meaning of the words (which is not possible with the word by word design).</p> <p>We could also say we are giving up precision for gaining meaning.</p> <p>Tip</p> <p>Here is a list of popular word embeddings methods/models:</p> <ul> <li>FastText (Facebook, 2016)</li> <li>GloVe (Stanford, 2014)</li> <li>Word2Vec (Google, 2013)</li> </ul> <p>More sophisticated models use advanced deep learning network architectures to learn word embeddings. In these advanced models, words have different embeddings depending on their context (e.g. plant as flower or factory or as a verb). Here are some popular examples:</p> <ul> <li>ELMo (Allen Institute, 2018)</li> <li>BERT (Google, 2018)</li> <li>GPT-2 (OpenAI, 2019)</li> </ul> <p>There are also approaches to fine tune such models on your own corpus.</p>"},{"location":"lectures/word_embeddings/#word-embeddings-process","title":"Word Embeddings Process","text":"<p>Here is a high level overview of the word embeddings process:</p> <p></p> <p>The corpus are words in their context of interest, e.g. wikipedia, news articles, etc. This can be generic or domain specific.</p> <p>Corpus</p> <p>Suppose we want to generate word embeddings based on Shakespeare's plays, then the corpus would be all of Shakespeare's plays, but not Wikipedia or news articles.</p> <p>After preprocessing, we should have the words represented as vectors. Typically, we use one-hot encoding for this.</p> <p>Those one-hot encoded vectors are then fed into the word embeddings method. This is usually a machine learning model, that performs a learning task on the corpus, for example, predicting the next word in a sentence, or predicting the center word in a context window.</p> <p>The dimension of the word embeddings is one of the hyperparameters of the model which needs to be determined. In practice, it typically ranges from a few hundred to a few thousand dimensions.</p> <p>Tip</p> <p>Remember that it is the context that determines the meaning of a word.</p> <p>Self-supervised Learning</p> <p>The learning task for word embeddings is self-supervised, i.e. the input data is unlabeled, but the data itself provides the necessary context (which would otherwise be the labels), because we are looking at the context of a word.</p> <p>A corpus is a self-contained data set, i.e. it contains all the information necessary to learn the embeddings, that is, the training data and the labels.</p> <p>Dimensions of Word Embeddings</p> <p>Using a higher number of dimensions allows us to capture more nuances of the meaning of the words, but it also requires more memory and computational resources.</p> <p>The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions. The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions.<sup>1</sup></p> <p>Note that a number of parameters is not the same as the number of dimensions of the word embeddings. The number of parameters is the total number of weights in the model that need to be trained (aka all entries in the weight matrices), while the embedding size is the number of dimensions of the word embeddings (aka the number of rows).</p>"},{"location":"lectures/word_embeddings/#continuous-bag-of-words-cbow","title":"Continuous Bag of Words (CBOW)","text":"<p>For the CBOW model, the learning task is to predict the center word based on its context \\(C\\).</p> <p>The rationale of the CBOW model is, that if two words are surrounded by a similar sets of words when used in various sentences, then those two words tend to be related in their meaning.</p> <p>Example</p> <p>Given the sentence:</p> <p>The little ___ is barking \ud83d\udc36</p> <p>Candidates for the center word are: dog, puppy, hound, terrier, etc., as they are all used frequently in the same context.</p> <p></p> <p>As a by-product of this learning task, we will obtain the word embeddings.</p> <p>The following visualization shows the CBOW model for a context size of \\(C=2\\). This means, we take into account the two words before and after the center word. This would be equivalent to a window size of 5.</p> <p></p> <p>Info</p> <p>The context window size \\(C\\) is another hyperparameter of the model.</p> <p>Example</p> <p>Given the sentence:</p> <p>I am happy because I am learning</p> <p>and a context window of size \\(C=2\\), the input and output pairs would be:</p> Input (Context) Output (Center Word) [I, am, because, I] happy [am, happy, I, am] because [happy, because, am, learning] I <p>Skip-Gram Model</p> <p>The Skip-Gram model can be seen as the opposite of the CBOW model. It tries to predict the context words given the center word.</p> <p>While this may seem counterintuitive at first, the Skip-Gram model predicts each context word independently, which allows it to capture more information about the context words.</p> <p>Consider the sentence \"The cat sat on the mat\". For the center word \"sat\", the context words are [\"cat\", \"on\"], but the model doesn\u2019t need to predict both \"cat\" and \"on\" simultaneously as a pair. Instead, it predicts \"cat\" given \"sat\" and \"on\" given \"sat\" as separate tasks.</p> <p>Also, for \"sat\" as the center word, the model might assign higher probabilities to \"cat\", \"on\", and \"mat\" because they commonly appear in similar contexts, even if some surrounding words are missing or irrelevant.</p> <p>So instead of learning an exact mapping, the model learns probabilistic associations between words.</p> <p>More details can be found in the original paper.</p> <p>Both CBOW and Skip-Gram are architectures of the Word2Vec model.</p>"},{"location":"lectures/word_embeddings/#training-data-for-cbow-model","title":"Training Data for CBOW Model","text":"<p>To train the CBOW model, we need to generate training data.</p> <p>For this, we need to transform the vectors of the context words into a single vector. This can be done by averaging the one-hot vectors of the context words. The resulting vector is the context vector, which is the input to the CBOW model.</p> <p>Here is an overview of the process:</p> <p></p> <p>With this approach, we can generate training data for the CBOW model.</p> <ul> <li>The input is the context vector</li> <li>The expected output is the vector of the center word</li> </ul> <p>Example</p> <p>Given the corpus:</p> <p>I am happy because I am learning</p> <p>The vocabulary is:</p> <p>\\([\\text{am}, \\text{because}, \\text{happy}, \\text{I}, \\text{learning}]\\)</p> <p>And the one-hot encoded vectors are:</p> Word Vector am \\([1, 0, 0, 0, 0]\\) because \\([0, 1, 0, 0, 0]\\) happy \\([0, 0, 1, 0, 0]\\) I \\([0, 0, 0, 1, 0]\\) learning \\([0, 0, 0, 0, 1]\\) <p>To build a single vector for any given context words, we average the one-hot encoded vectors of the context words.</p> <p>For example, if the context words are [I, am, because, I], then the average vector would be:</p> \\[ \\frac{1}{4} \\cdot \\begin{pmatrix}\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix}\\end{pmatrix} = \\frac{1}{4} \\cdot \\begin{pmatrix}\\begin{bmatrix}1 \\\\ 1 \\\\ 0 \\\\ 2 \\\\ 0\\end{bmatrix}\\end{pmatrix} = \\begin{pmatrix}\\begin{bmatrix}0.25 \\\\ 0.25 \\\\ 0 \\\\ 0.5 \\\\ 0\\end{bmatrix}\\end{pmatrix} \\] <p>Doing this for a few more examples, we can generate the training data for the CBOW model:</p> Context words Context vector Center word Center vector \\([\\text{I}, \\text{am}, \\text{because}, \\text{I}]\\) \\([0.25, 0.25, 0, 0.5, 0]\\) \\(\\text{happy}\\) \\([0, 0, 1, 0, 0]\\) \\([\\text{am}, \\text{happy}, \\text{I}, \\text{am}]\\) \\([0.5, 0, 0.25, 0.25, 0]\\) \\(\\text{because}\\) \\([0, 1, 0, 0, 0]\\) \\([\\text{happy}, \\text{because}, \\text{am}, \\text{learning}]\\) \\([0.25, 0.25, 0.25, 0, 0.25]\\) \\(\\text{I}\\) \\([0, 0, 0, 1, 0]\\)"},{"location":"lectures/word_embeddings/#architecure-of-cbow-model","title":"Architecure of CBOW Model","text":"<p>The architecture of CBOW is a neural network model with a single hidden layer. The input layer corresponds to the context words, and the output layer corresponds to the target word.</p> <p>Shallow Dense Neural Network</p> <p>From an architectural point of view, we speak of a shallow dense neural network, because it has only one hidden layer and all neurons are connected to each other.</p> <p>Note that the number of Neurons here is the first dimension of the matrix, i.e. the number of rows.</p> <p>The learning objective is to minimize the prediction error between the predicted target word and the actual target word. The hidden layer weights of the neural network are adjusted to achieve this task.</p> <p></p> <p>Tip</p> <p>Right-click on the image and select \"Open image in new tab\" to see a larger version of the image, or use the zoom function of your browser.</p> <p>Let's clarify the notation:</p> <ul> <li>\\(V\\) is the vocabulary size</li> <li>\\(N\\) is the number of dimensions of the word embeddings</li> <li>\\(m\\) is the number of samples in the training data set</li> </ul> <p>Now, let's look at the architecture in more detail:</p> <ul> <li>\\(\\mathbf{X}\\) is the input matrix of size \\(V \\times m\\). This is the matrix of the context vectors, where each column is a context vector. This means the input layer has \\(V\\) neurons, one for each word in the vocabulary.</li> <li>\\(\\mathbf{H}\\) is the hidden layer matrix of size \\(N \\times m\\). This means the hidden layer has \\(N\\) neurons, which is the number of dimensions of the word embeddings.</li> <li>\\(\\mathbf{\\hat{Y}}\\) is the output matrix of size \\(V \\times m\\). This is the matrix of the predicted center word vectors, where each column is a word vector. This mean the output layer has \\(V\\) neurons, one for each word in the vocabulary.</li> <li>\\(\\mathbf{Y}\\) represent the expected output matrix of size \\(V \\times m\\). This is the matrix of the actual center word vectors, where each column is a word vector.</li> </ul> <p>There are two weight matrices, one that connects the input layer to the hidden layer, and one that connects the hidden layer to the output layer.</p> <ul> <li> <p>\\(\\mathbf{W}_1\\) is the weight matrix that connects the input layer to the hidden layer and is of size \\(N \\times V\\) (aka weight matrix of the hidden layer). This is the matrix of the word embeddings, where each column represents the word embedding of a word in the vocabulary.</p> </li> <li> <p>\\(\\mathbf{W}_2\\) is the weight matrix for the output layer and is of size \\(V \\times N\\) (aka weight matrix of the output layer).</p> </li> </ul> <p>Word Embeddings</p> <p>The weights of the hidden layer \\(\\mathbf{W}_1\\), after training, serve as the word embeddings. Each word in the vocabulary is associated with a unique set of weights, and these weights serve as the vector representation or embedding for that word.</p> <p>In the case of CBOW, the input layer represents the context words, and the weights connecting this input layer to the hidden layer capture the relationships and semantics of the words in the context.</p> <p>The hidden layer acts as a transformative space where the input (context words) is transformed into the output (center word prediction), and the weights within this layer serve as the learned representations or embeddings for the words.</p> <p>The weights connecting the hidden layer to the output layer \\(\\mathbf{W}_2\\) are responsible for producing the final prediction based on the learned representations from the embeddings, and thus cannot be considered as the word embeddings.</p> <p>To compute the next layer \\(\\mathbf{Z}\\), we multiply the weight matrix with the previous layer:</p> \\[ \\mathbf{Z}_{N \\times m} = \\mathbf{W}_{N \\times V} \\cdot \\mathbf{X}_{V \\times m} \\] <p>Since the number of columns in the weight matrix matches the number of rows in the input matrix, we can multiply the two matrices, and the resulting matrix \\(\\mathbf{Z}\\) will be of size \\(N \\times m\\).</p> <p>Warning</p> <p>When looking at the diagram, you may think it may be the other way around, i.e. \\(\\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{W}\\), but this is not the case.</p> <p>Example</p> <p>Assume you have a vocabulary size of 8000 words, and want to learn 400-dimensional word embeddings.</p> <p>Then the sizes of the layers, i.e. the number of neurons, would be as follows:</p> <ul> <li>Input layer: \\(V = 8000\\)</li> <li>Hidden layer: \\(N = 400\\)</li> <li>Output layer: \\(V = 8000\\)</li> </ul> <p>Note</p> <p>In this lecture, we don't want to go into the details of the neural network architecture. However, it is important to know what the matrices represent.</p> <p>Activation Functions</p> <p>You would also need to apply an activation function on \\(\\mathbf{Z}\\) but we will not cover this in this lecture. What is important is to understand how the dimensions of the matrices are related.</p> <p>The purpose of the activation function is to introduce non-linearity into the network, allowing it to learn from complex patterns and relationships in the data.</p> <p>Without activation functions (or with only linear activation functions), a neural network would behave like a linear model, regardless of its depth. This is because a composition of linear functions is still a linear function. Introducing non-linear activation functions enables the neural network to model and learn more complex mappings between inputs and outputs.</p> <p>Two popular activation functions are the sigmoid function and the ReLU function.</p> <p>Matrix Multiplication</p> <p>Two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) can be multiplied if the number of columns of \\(\\mathbf{A}\\) is equal to the number of rows of \\(\\mathbf{B}\\).</p> \\[ A_{m \\times n} \\cdot B_{n \\times p} = C_{m \\times p} \\] <p>Batch Processing</p> <p>Usually when training a neural network, we use batch processing, i.e. we process multiple samples at once. This is more efficient than processing one sample at a time.</p> <p>The batch size is another hyperparameter of the model.</p> <p>As we can see from the matrix multiplication above, the weight matrices are independent of the batch size, i.e. they are the same for any batch size.</p> <p>Batch processing is also important for parallelization, i.e. we can process multiple batches in parallel on different processors.</p> <p>Loss Function</p> <p>In CBOW, the loss function is typically the cross entropy loss function.</p> <p>The cross entropy loss function is a measure of how well the predicted probability distribution of the target word matches the actual probability distribution of the target word.</p> <p>The goal during training is to minimize this cross-entropy loss, and the backpropagation algorithm is used to adjust the model's parameters (weights) accordingly.</p>"},{"location":"lectures/word_embeddings/#evaluation-of-word-embeddings","title":"Evaluation of Word Embeddings","text":"<p>When evaluating word embeddings, we can distinguish between intrinsic and extrinsic evaluation.</p> <p>Intrinsic evaluation methods assess how well the word embeddings capture the semantic or syntactic relationships between the words.</p> <p>We can evaluate the relationship between words using the following methods:</p> <ul> <li>Analogies</li> <li>Clustering</li> <li>Visualization</li> </ul> <p>Analogies</p> <p>Semantic analogies:</p> <p>\"France\" is to \"Paris\" as \"Italy\" is to &lt;?&gt;</p> <p>Syntactic analogies:</p> <p>\"seen\" is to \"saw\" as \"been\" is to &lt;?&gt;</p> <p>Ambiguous Words</p> <p>Be careful with ambiguous words:</p> <p>\"wolf\" is to \"pack\" as \"bee\" is to &lt;?&gt;</p> <p>Correct answers could be \"hive\", \"colony\", or \"swarm\".</p> <p>In external evaluation, we use the word embeddings as input to a downstream task, e.g. sentiment analysis, NER, or parts-of-speech tagging, and evaluate the performance of the task with established metrics such as accuracy or F1-score.</p> <p>The performance of the task is then used as a measure of the quality of the word embeddings.</p> <p>External evaluation is more practical, because it allows us to evaluate the word embeddings in the context of a real-world application.</p> <p>Example</p> <p>In NER, we want to identify the names of people, places, organizations, etc. in a text. Given the sentence:</p> <p>\"Marc is going to Paris to visit the Louvre.\"</p> <p>We can find the following named entities:</p> <p>\"Marc\" (person), \"Paris\" (location), \"Louvre\" (organization)</p> <p>Warning</p> <p>Note that with external evaluation methods, we evaluate both the word embeddings and the downstream task. This makes troubleshooting more difficult.</p>"},{"location":"lectures/word_embeddings/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Word embeddings are a way to represent words as vectors.</li> <li>With word embeddings, each dimension represents a different aspect of the word. We do not know what exactly each dimension represents, but we know that similar words are close to each other in the vector space.</li> <li>To create word embeddings, we typically use a machine learning model that performs a learning task on a corpus.</li> <li>A famous example of a word embeddings model is the CBOW model. It learns to predict the center word based on its context words.</li> <li>The rationale of the CBOW model is, that if two words are surrounded by a similar sets of words when used in various sentences, then those two words tend to be related in their meaning.</li> <li>It's architecture is a shallow dense neural network with a single hidden layer. The word embeddings are essentially a by-product of the learning task, i.e. they are the weights of the hidden layer after training.</li> <li>We can evaluate the quality of word embeddings using intrinsic and extrinsic evaluation methods, where extrinsic evaluation is more practical, because it allows us to evaluate the word embeddings in the context of a real-world application.</li> </ul> <ol> <li> <p>Raschka, Sebastian. Build a Large Language Model (From Scratch). Shelter Island, NY: Manning, 2024. https://www.manning.com/books/build-a-large-language-model-from-scratch.\u00a0\u21a9\u21a9</p> </li> </ol>"}]}