
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A practical course on natural language processing @ HTWG Konstanz.">
      
      
      
      
        <link rel="prev" href="../language_models/">
      
      
        <link rel="next" href="../sequence_models/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Word Embeddings - Practical NLP</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="cyan" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#word-embeddings" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Practical NLP" class="md-header__button md-logo" aria-label="Practical NLP" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Practical NLP
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Word Embeddings
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pkeilbach/htwg-practical-nlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pkeilbach/htwg-practical-nlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Welcome

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../course_profile/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../getting_started/" class="md-tabs__link">
        
  
  
    
  
  Getting Started

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../preface/" class="md-tabs__link">
          
  
  
    
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../assignments/" class="md-tabs__link">
        
  
  
    
  
  Assignments

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../presentations/" class="md-tabs__link">
        
  
  
    
  
  Presentations

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../faq/" class="md-tabs__link">
        
  
  
    
  
  FAQ

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../exam/" class="md-tabs__link">
        
  
  
    
  
  Exam

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Practical NLP" class="md-nav__button md-logo" aria-label="Practical NLP" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Practical NLP
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pkeilbach/htwg-practical-nlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pkeilbach/htwg-practical-nlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course_profile/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Lectures
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Lectures
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preface/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Preface
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preprocessing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Preprocessing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_extraction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Feature Extraction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../logistic_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logistic Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../naive_bayes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Naive Bayes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vector_space_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector Space Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minimum_edit_distance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Minimum Edit Distance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../language_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Word Embeddings
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Word Embeddings
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#revisit-one-hot-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      Revisit One-Hot Encoding
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-embeddings-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Word Embeddings Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-embeddings-process" class="md-nav__link">
    <span class="md-ellipsis">
      Word Embeddings Process
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#continuous-bag-of-words-cbow" class="md-nav__link">
    <span class="md-ellipsis">
      Continuous Bag of Words (CBOW)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-data-for-cbow-model" class="md-nav__link">
    <span class="md-ellipsis">
      Training Data for CBOW Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecure-of-cbow-model" class="md-nav__link">
    <span class="md-ellipsis">
      Architecure of CBOW Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-of-word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation of Word Embeddings
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sequence_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequence Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../attention_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../further_reading/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Where to go from here?
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../assignments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Assignments
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../presentations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Presentations
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exam
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#revisit-one-hot-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      Revisit One-Hot Encoding
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-embeddings-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Word Embeddings Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-embeddings-process" class="md-nav__link">
    <span class="md-ellipsis">
      Word Embeddings Process
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#continuous-bag-of-words-cbow" class="md-nav__link">
    <span class="md-ellipsis">
      Continuous Bag of Words (CBOW)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-data-for-cbow-model" class="md-nav__link">
    <span class="md-ellipsis">
      Training Data for CBOW Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecure-of-cbow-model" class="md-nav__link">
    <span class="md-ellipsis">
      Architecure of CBOW Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-of-word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation of Word Embeddings
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="word-embeddings">Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permanent link">&para;</a></h1>
<p>In this lecture, we will learn about <strong>word embeddings</strong>, which are a way to represent words as vectors.
We will learn about the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model, which is a machine learning model that learns word embeddings from a corpus.</p>
<p>Deep learning models <strong>cannot process</strong> data formats like video, audio, and text in their raw form.
Thus, we use an <strong>embedding model</strong> to transform this raw data into a dense vector representation
that deep learning architectures can easily understand and process.
Specifically, this figure illustrates the process of converting raw data into a three-dimensional numerical vector.</p>
<p><img alt="Embedding models" src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp" /></p>
<p>At its core, an embedding is a mapping from <strong>discrete objects</strong>, such as words, images, or even entire documents,
to points in a continuous vector space.
The primary purpose of embeddings is to convert <strong>nonnumeric data</strong> into a format that <strong>neural networks can process</strong>.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<p><img alt="Idea of word embeddings" src="../../img/word-embeddings-idea.drawio.svg" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>There are also embeddings for sentences, paragraphs, or whole documents.
Sentence or paragraph embeddings are popular choices for <strong>retrieval-augmented generation</strong>.
Retrieval augmented generation combines <strong>generation</strong> (like producing text) with <strong>retrieval</strong>
(like searching an external knowledge base) to pull relevant information when generating text.</p>
</div>
<h2 id="revisit-one-hot-encoding">Revisit One-Hot Encoding<a class="headerlink" href="#revisit-one-hot-encoding" title="Permanent link">&para;</a></h2>
<p>In the lecture about feature extraction, we have seen that we can represent words as vectors using <a href="../feature_extraction/#one-hot-encoding">one hot encoding</a>.</p>
<p>One-hot encoding is a very simple way to represent words as vectors, but it has some major <strong>disadvantages</strong>:</p>
<ul>
<li>❌ the resulting vectors are very <strong>high dimensional</strong>, i.e. one dimension for each word in the vocabulary: <span class="arithmatex">\(n_{\text{dim}} = |V|\)</span></li>
<li>❌ it does not capture <strong>meaning</strong>, i.e. all words have the same distance to each other:</li>
</ul>
<p><img alt="One-Hot Encoding" src="../../img/word-embeddings-one-hot-encoding-distance.png" /></p>
<h2 id="word-embeddings-overview">Word Embeddings Overview<a class="headerlink" href="#word-embeddings-overview" title="Permanent link">&para;</a></h2>
<p>From the lecture about vector space models, we already know that similar words should be close to each other in the vector space.</p>
<p>But how can we achieve this? In this lecture we will learn about word embeddings, which are a way to represent words as vectors.</p>
<p><img alt="Word Embeddings" src="../../img/word-embeddings.drawio.svg" /></p>
<p>In the figure above, we have two dimensional word embeddings:</p>
<ul>
<li>the first dimension represents the word's sentiment in terms of positive or negative.</li>
<li>the second dimensions indicates whether the word is more concrete or abstract.</li>
</ul>
<p>In the real world, word embeddings are usually <strong>much higher dimensional</strong>, e.g. 100 or 300 dimensions.</p>
<p>Each dimension <strong>represents a different aspect of the word</strong>. We <strong>do not know what</strong> exactly each dimension represents, but we know that similar words are close to each other in the vector space.</p>
<p>Word embeddings have the following <strong>advantages</strong>:</p>
<ul>
<li>✅ they are <strong>dense</strong>, i.e. they do not contain many zeros</li>
<li>✅ they are <strong>low dimensional</strong>, i.e. they do not require much memory</li>
<li>✅ they allow us to <strong>encode meaning</strong></li>
<li>✅ they <strong>capture</strong> semantic and syntactic <strong>information</strong></li>
<li>✅ they are <strong>computationally efficient</strong></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that in the <a href="../vector_space_models/#word-by-word-design">word by word design</a>, we have as many features as we have words in our vocabulary. This is not very efficient, because we have to store a lot of zeros. With word embeddings, we can reduce the number of features to a much smaller number, e.g. 100 or 300, while at the same time capturing the meaning of the words (which is not possible with the word by word design).</p>
<p>We could also say we are giving up precision for gaining meaning.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Here is a list of popular word embeddings methods/models:</p>
<ul>
<li><a href="https://arxiv.org/abs/1607.04606">FastText (Facebook, 2016)</a></li>
<li><a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe (Stanford, 2014)</a></li>
<li><a href="https://arxiv.org/abs/1301.3781">Word2Vec (Google, 2013)</a></li>
</ul>
<p>More sophisticated models use <strong>advanced deep learning network architectures</strong> to learn word embeddings.
In these advanced models, words have different embeddings depending on their context (e.g. plant as flower or factory or as a verb).
Here are some popular examples:</p>
<ul>
<li><a href="https://arxiv.org/abs/1802.05365">ELMo (Allen Institute, 2018)</a></li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT (Google, 2018)</a></li>
<li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">GPT-2 (OpenAI, 2019)</a></li>
</ul>
<p>There are also approaches to fine tune such models on your own corpus.</p>
</div>
<h2 id="word-embeddings-process">Word Embeddings Process<a class="headerlink" href="#word-embeddings-process" title="Permanent link">&para;</a></h2>
<p>Here is a high level overview of the word embeddings process:</p>
<p><img alt="Word Embeddings Process" src="../../img/word-embeddings-process.drawio.svg" /></p>
<p>The <a href="../language_models/#text-corpus">corpus</a> are words in their context of interest, e.g. wikipedia, news articles, etc. This can be generic or domain specific.</p>
<div class="admonition info">
<p class="admonition-title">Corpus</p>
<p>Suppose we want to generate word embeddings based on Shakespeare's plays, then the corpus would be all of Shakespeare's plays, but not Wikipedia or news articles.</p>
</div>
<p>After <strong>preprocessing</strong>, we should have the words represented as <strong>vectors</strong>. Typically, we use one-hot encoding for this.</p>
<p>Those one-hot encoded vectors are then fed into the word <strong>embeddings method</strong>. This is usually a machine learning model, that performs a <strong>learning task</strong> on the corpus, for example, predicting the next word in a sentence, or <strong>predicting the center word</strong> in a context window.</p>
<p>The dimension of the word embeddings is one of the <strong>hyperparameters</strong> of the model which needs to be determined. In practice, it typically ranges from a few hundred to a few thousand dimensions.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Remember that it is the <a href="../vector_space_models/#introduction">context</a> that determines the meaning of a word.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Self-supervised Learning</p>
<p>The learning task for word embeddings is self-supervised, i.e. the input data is unlabeled, but the data itself provides the necessary context (which would otherwise be the labels), because we are looking at the context of a word.</p>
<p>A corpus is a <em>self-contained</em> data set, i.e. it contains all the information necessary to learn the embeddings, that is, the training data and the labels.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Dimensions of Word Embeddings</p>
<p>Using a higher number of dimensions allows us to capture more nuances of the meaning of the words, but it also requires more memory and computational resources.</p>
<p>The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions.
The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions.<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<p>Note that a number of parameters is not the same as the number of dimensions of the word embeddings. The number of parameters is the <em>total number of weights</em> in the model that need to be trained (aka all entries in the weight matrices), while the embedding size is the <em>number of dimensions</em> of the word embeddings (aka the number of rows).</p>
</div>
<h2 id="continuous-bag-of-words-cbow">Continuous Bag of Words (<abbr title="Continuous Bag-of-Words">CBOW</abbr>)<a class="headerlink" href="#continuous-bag-of-words-cbow" title="Permanent link">&para;</a></h2>
<p>For the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model, the learning task is to <strong>predict the center word</strong> based on its context <span class="arithmatex">\(C\)</span>.</p>
<p>The <strong>rationale</strong> of the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model is, that if two words are surrounded by a similar sets of words when used in various sentences, then those two words tend to be <strong>related in their meaning</strong>.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Given the sentence:</p>
<blockquote>
<p>The little ___ is barking 🐶</p>
</blockquote>
<p>Candidates for the <strong>center word</strong> are: dog, puppy, hound, terrier, etc., as they are all used frequently in the same context.</p>
</div>
<p><img alt="CBOW Schema" src="../../img/word-embeddings-cbow-schema.drawio.svg" /></p>
<!-- TODO EXAM -->
<p>As a <em>by-product</em> of this learning task, we will obtain the <strong>word embeddings</strong>.</p>
<p>The following visualization shows the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model for a context size of <span class="arithmatex">\(C=2\)</span>. This means, we take into account the two words before and after the center word. This would be equivalent to a window size of 5.</p>
<p><img alt="CBOW Model" src="../../img/word-embeddings-cbow.drawio.svg" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>The context window size <span class="arithmatex">\(C\)</span> is another <strong>hyperparameter</strong> of the model.</p>
</div>
<!-- TODO EXAM -->
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Given the sentence:</p>
<blockquote>
<p>I am happy because I am learning</p>
</blockquote>
<p>and a context window of size <span class="arithmatex">\(C=2\)</span>, the input and output pairs would be:</p>
<table>
<thead>
<tr>
<th>Input (Context)</th>
<th>Output (Center Word)</th>
</tr>
</thead>
<tbody>
<tr>
<td>[I, am, because, I]</td>
<td>happy</td>
</tr>
<tr>
<td>[am, happy, I, am]</td>
<td>because</td>
</tr>
<tr>
<td>[happy, because, am, learning]</td>
<td>I</td>
</tr>
</tbody>
</table>
</div>
<div class="admonition tip">
<p class="admonition-title">Skip-Gram Model</p>
<p>The <strong>Skip-Gram model</strong> can be seen as the <strong>opposite</strong> of the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model. It tries to predict the context words given the center word.</p>
<p>While this may seem counterintuitive at first, the Skip-Gram model predicts <strong>each context word independently</strong>, which allows it to capture more information about the context words.</p>
<p>Consider the sentence "The cat sat on the mat". For the center word "sat", the context words are ["cat", "on"], but the model doesn’t need to predict both "cat" and "on" simultaneously as a pair. Instead, it predicts "cat" given "sat" and "on" given "sat" as <strong>separate tasks</strong>.</p>
<p>Also, for "sat" as the center word, the model might assign higher probabilities to "cat", "on", and "mat" because they commonly appear in <strong>similar contexts</strong>, even if some surrounding words are missing or irrelevant.</p>
<p>So instead of learning an exact mapping, the model learns <strong>probabilistic associations</strong> between words.</p>
<p>More details can be found in the <a href="https://arxiv.org/abs/1301.3781">original paper</a>.</p>
<p>Both <abbr title="Continuous Bag-of-Words">CBOW</abbr> and Skip-Gram are architectures of the <a href="https://arxiv.org/abs/1301.3781">Word2Vec</a> model.</p>
</div>
<h2 id="training-data-for-cbow-model">Training Data for <abbr title="Continuous Bag-of-Words">CBOW</abbr> Model<a class="headerlink" href="#training-data-for-cbow-model" title="Permanent link">&para;</a></h2>
<p>To train the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model, we need to generate training data.</p>
<p>For this, we need to transform the vectors of the context words into a single vector. This can be done by <strong>averaging</strong> the one-hot vectors of the context words. The resulting vector is the <strong>context vector</strong>, which is the input to the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model.</p>
<p>Here is an overview of the process:</p>
<p><img alt="Transforming Words to Vectors" src="../../img/word-embeddings-cbow-transformation.drawio.svg" /></p>
<p>With this approach, we can generate training data for the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model.</p>
<ul>
<li>The <strong>input</strong> is the context vector</li>
<li>The <strong>expected output</strong> is the vector of the center word</li>
</ul>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Given the corpus:</p>
<blockquote>
<p>I am happy because I am learning</p>
</blockquote>
<p>The vocabulary is:</p>
<blockquote>
<p><span class="arithmatex">\([\text{am}, \text{because}, \text{happy}, \text{I}, \text{learning}]\)</span></p>
</blockquote>
<p>And the one-hot encoded vectors are:</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>Vector</th>
</tr>
</thead>
<tbody>
<tr>
<td>am</td>
<td><span class="arithmatex">\([1, 0, 0, 0, 0]\)</span></td>
</tr>
<tr>
<td>because</td>
<td><span class="arithmatex">\([0, 1, 0, 0, 0]\)</span></td>
</tr>
<tr>
<td>happy</td>
<td><span class="arithmatex">\([0, 0, 1, 0, 0]\)</span></td>
</tr>
<tr>
<td>I</td>
<td><span class="arithmatex">\([0, 0, 0, 1, 0]\)</span></td>
</tr>
<tr>
<td>learning</td>
<td><span class="arithmatex">\([0, 0, 0, 0, 1]\)</span></td>
</tr>
</tbody>
</table>
<p>To build a single vector for any given context words, we average the one-hot encoded vectors of the context words.</p>
<p>For example, if the context words are [I, am, because, I], then the average vector would be:</p>
<div class="arithmatex">\[
\frac{1}{4} \cdot \begin{pmatrix}\begin{bmatrix}0 \\ 0 \\ 0 \\ 1 \\ 0\end{bmatrix} + \begin{bmatrix}1 \\ 0 \\ 0 \\ 0 \\ 0\end{bmatrix} + \begin{bmatrix}0 \\ 1 \\ 0 \\ 0 \\ 0\end{bmatrix} + \begin{bmatrix}0 \\ 0 \\ 0 \\ 1 \\ 0\end{bmatrix}\end{pmatrix} = \frac{1}{4} \cdot \begin{pmatrix}\begin{bmatrix}1 \\ 1 \\ 0 \\ 2 \\ 0\end{bmatrix}\end{pmatrix} = \begin{pmatrix}\begin{bmatrix}0.25 \\ 0.25 \\ 0 \\ 0.5 \\ 0\end{bmatrix}\end{pmatrix}
\]</div>
<p>Doing this for a few more examples, we can generate the training data for the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model:</p>
<table>
<thead>
<tr>
<th>Context words</th>
<th>Context vector</th>
<th>Center word</th>
<th>Center vector</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\([\text{I}, \text{am}, \text{because}, \text{I}]\)</span></td>
<td><span class="arithmatex">\([0.25, 0.25, 0, 0.5, 0]\)</span></td>
<td><span class="arithmatex">\(\text{happy}\)</span></td>
<td><span class="arithmatex">\([0, 0, 1, 0, 0]\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\([\text{am}, \text{happy}, \text{I}, \text{am}]\)</span></td>
<td><span class="arithmatex">\([0.5, 0, 0.25, 0.25, 0]\)</span></td>
<td><span class="arithmatex">\(\text{because}\)</span></td>
<td><span class="arithmatex">\([0, 1, 0, 0, 0]\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\([\text{happy}, \text{because}, \text{am}, \text{learning}]\)</span></td>
<td><span class="arithmatex">\([0.25, 0.25, 0.25, 0, 0.25]\)</span></td>
<td><span class="arithmatex">\(\text{I}\)</span></td>
<td><span class="arithmatex">\([0, 0, 0, 1, 0]\)</span></td>
</tr>
</tbody>
</table>
</div>
<h2 id="architecure-of-cbow-model">Architecure of <abbr title="Continuous Bag-of-Words">CBOW</abbr> Model<a class="headerlink" href="#architecure-of-cbow-model" title="Permanent link">&para;</a></h2>
<p>The <strong>architecture</strong> of <abbr title="Continuous Bag-of-Words">CBOW</abbr> is a neural network model with a single hidden layer. The input layer corresponds to the context words, and the output layer corresponds to the target word.</p>
<!-- TODO EXAM -->
<div class="admonition note">
<p class="admonition-title">Shallow Dense Neural Network</p>
<p>From an architectural point of view, we speak of a <strong>shallow dense neural network</strong>, because it has only one hidden layer and all neurons are connected to each other.</p>
<p>Note that the number of Neurons here is the first dimension of the matrix, i.e. the number of rows.</p>
</div>
<p>The <strong>learning objective</strong> is to minimize the prediction error between the predicted target word and the actual target word. The hidden layer weights of the neural network are adjusted to achieve this task.</p>
<p><img alt="CBOW Architecture" src="../../img/word-embeddings-cbow-architecture.drawio.svg" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Right-click on the image and select "Open image in new tab" to see a larger version of the image, or use the zoom function of your browser.</p>
</div>
<p>Let's clarify the notation:</p>
<ul>
<li><span class="arithmatex">\(V\)</span> is the vocabulary size</li>
<li><span class="arithmatex">\(N\)</span> is the number of dimensions of the word embeddings</li>
<li><span class="arithmatex">\(m\)</span> is the number of samples in the training data set</li>
</ul>
<p>Now, let's look at the architecture in more detail:</p>
<ul>
<li><span class="arithmatex">\(\mathbf{X}\)</span> is the input matrix of size <span class="arithmatex">\(V \times m\)</span>. This is the matrix of the context vectors, where each <em>column</em> is a context vector. This means the <strong>input layer</strong> has <span class="arithmatex">\(V\)</span> neurons, one for each word in the vocabulary.</li>
<li><span class="arithmatex">\(\mathbf{H}\)</span> is the <strong>hidden layer</strong> matrix of size <span class="arithmatex">\(N \times m\)</span>. This means the <strong>hidden layer</strong> has <span class="arithmatex">\(N\)</span> neurons, which is the number of dimensions of the word embeddings.</li>
<li><span class="arithmatex">\(\mathbf{\hat{Y}}\)</span> is the output matrix of size <span class="arithmatex">\(V \times m\)</span>. This is the matrix of the predicted center word vectors, where each <em>column</em> is a word vector. This mean the <strong>output layer</strong> has <span class="arithmatex">\(V\)</span> neurons, one for each word in the vocabulary.</li>
<li><span class="arithmatex">\(\mathbf{Y}\)</span> represent the expected output matrix of size <span class="arithmatex">\(V \times m\)</span>. This is the matrix of the actual center word vectors, where each <em>column</em> is a word vector.</li>
</ul>
<!-- TODO EXAM -->
<p>There are <strong>two weight matrices</strong>, one that connects the input layer to the hidden layer, and one that connects the hidden layer to the output layer.</p>
<ul>
<li>
<p><span class="arithmatex">\(\mathbf{W}_1\)</span> is the weight matrix that connects the input layer to the hidden layer and is of size <span class="arithmatex">\(N \times V\)</span> (aka <em>weight matrix of the hidden layer</em>). This is the matrix of the <strong>word embeddings</strong>, where each <em>column</em> represents the word embedding of a word in the vocabulary.</p>
</li>
<li>
<p><span class="arithmatex">\(\mathbf{W}_2\)</span> is the weight matrix for the output layer and is of size <span class="arithmatex">\(V \times N\)</span> (aka <em>weight matrix of the output layer</em>).</p>
</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Word Embeddings</p>
<p>The <strong>weights of the hidden layer</strong> <span class="arithmatex">\(\mathbf{W}_1\)</span>, after training, serve as the <strong>word embeddings</strong>. Each word in the vocabulary is associated with a unique set of weights, and these weights serve as the vector representation or embedding for that word.</p>
<p>In the case of <abbr title="Continuous Bag-of-Words">CBOW</abbr>, the <strong>input layer</strong> represents the context words, and the <strong>weights</strong> connecting this input layer to the hidden layer capture the <strong>relationships</strong> and <strong>semantics</strong> of the words in the context.</p>
<p>The <strong>hidden layer</strong> acts as a transformative space where the input (context words) is transformed into the output (center word prediction), and the weights within this layer serve as the learned representations or embeddings for the words.</p>
<p>The weights connecting the hidden layer to the output layer <span class="arithmatex">\(\mathbf{W}_2\)</span> are responsible for producing the final prediction based on the learned representations from the embeddings, and thus cannot be considered as the word embeddings.</p>
</div>
<p>To compute the next layer <span class="arithmatex">\(\mathbf{Z}\)</span>, we multiply the weight matrix with the previous layer:</p>
<div class="arithmatex">\[
\mathbf{Z}_{N \times m} = \mathbf{W}_{N \times V} \cdot \mathbf{X}_{V \times m}
\]</div>
<p>Since the number of columns in the weight matrix matches the number of rows in the input matrix, we can multiply the two matrices, and the resulting matrix <span class="arithmatex">\(\mathbf{Z}\)</span> will be of size <span class="arithmatex">\(N \times m\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When looking at the diagram, you may think it may be the other way around, i.e. <span class="arithmatex">\(\mathbf{Z} = \mathbf{X} \cdot \mathbf{W}\)</span>, but this is <strong>not</strong> the case.</p>
</div>
<!-- TODO EXAM -->
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Assume you have a vocabulary size of 8000 words, and want to learn 400-dimensional word embeddings.</p>
<p>Then the sizes of the layers, i.e. the number of neurons, would be as follows:</p>
<ul>
<li>Input layer: <span class="arithmatex">\(V = 8000\)</span></li>
<li>Hidden layer: <span class="arithmatex">\(N = 400\)</span></li>
<li>Output layer: <span class="arithmatex">\(V = 8000\)</span></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this lecture, we don't want to go into the details of the neural network architecture. However, it is important to know what the matrices represent.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Activation Functions</p>
<p>You would also need to apply an <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> on <span class="arithmatex">\(\mathbf{Z}\)</span> but we will not cover this in this lecture. What is important is to understand how the dimensions of the matrices are related.</p>
<p>The purpose of the activation function is to introduce non-linearity into the network, allowing it to learn from complex patterns and relationships in the data.</p>
<p>Without activation functions (or with only linear activation functions), a neural network would behave like a linear model, regardless of its depth. This is because a composition of linear functions is still a linear function. Introducing non-linear activation functions enables the neural network to model and learn more complex mappings between inputs and outputs.</p>
<p>Two popular activation functions are the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> and the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU function</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Matrix Multiplication</p>
<p>Two matrices <span class="arithmatex">\(\mathbf{A}\)</span> and <span class="arithmatex">\(\mathbf{B}\)</span> can be multiplied if the number of columns of <span class="arithmatex">\(\mathbf{A}\)</span> is equal to the number of rows of <span class="arithmatex">\(\mathbf{B}\)</span>.</p>
<div class="arithmatex">\[
A_{m \times n} \cdot B_{n \times p} = C_{m \times p}
\]</div>
</div>
<!-- TODO EXAM -->
<div class="admonition info">
<p class="admonition-title">Batch Processing</p>
<p>Usually when training a neural network, we use <strong>batch processing</strong>, i.e. we process multiple samples at once. This is more efficient than processing one sample at a time.</p>
<p>The batch size is another <strong>hyperparameter</strong> of the model.</p>
<p>As we can see from the matrix multiplication above, <em>the weight matrices are independent of the batch size</em>, i.e. they are the same for any batch size.</p>
<p>Batch processing is also important for <strong>parallelization</strong>, i.e. we can process multiple batches in parallel on different processors.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Loss Function</p>
<p>In <abbr title="Continuous Bag-of-Words">CBOW</abbr>, the <strong>loss function</strong> is typically the <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a> loss function.</p>
<p>The cross entropy loss function is a measure of how well the <strong>predicted probability distribution</strong> of the target word matches the <strong>actual probability distribution</strong> of the target word.</p>
<p>The goal during training is to minimize this cross-entropy loss, and the <strong>backpropagation</strong> algorithm is used to adjust the model's parameters (weights) accordingly.</p>
</div>
<h2 id="evaluation-of-word-embeddings">Evaluation of Word Embeddings<a class="headerlink" href="#evaluation-of-word-embeddings" title="Permanent link">&para;</a></h2>
<p>When evaluating word embeddings, we can distinguish between intrinsic and extrinsic evaluation.</p>
<p><strong>Intrinsic evaluation</strong> methods assess how well the word embeddings capture the semantic or syntactic relationships between the words.</p>
<p>We can evaluate the relationship between words using the following methods:</p>
<ul>
<li>Analogies</li>
<li>Clustering</li>
<li>Visualization</li>
</ul>
<div class="admonition example">
<p class="admonition-title">Analogies</p>
<p>Semantic analogies:</p>
<blockquote>
<p>"France" is to "Paris" as "Italy" is to &lt;?&gt;</p>
</blockquote>
<p>Syntactic analogies:</p>
<blockquote>
<p>"seen" is to "saw" as "been" is to &lt;?&gt;</p>
</blockquote>
</div>
<div class="admonition warning">
<p class="admonition-title">Ambiguous Words</p>
<p>Be careful with ambiguous words:</p>
<blockquote>
<p>"wolf" is to "pack" as "bee" is to &lt;?&gt;</p>
</blockquote>
<p>Correct answers could be "hive", "colony", or "swarm".</p>
</div>
<p>In <strong>external evaluation</strong>, we use the word embeddings as input to a <strong>downstream task</strong>, e.g. sentiment analysis, <abbr title="Named Entity Recognition">NER</abbr>, or parts-of-speech tagging, and evaluate the performance of the task with established metrics such as accuracy or F1-score.</p>
<p>The performance of the task is then used as a measure of the quality of the word embeddings.</p>
<p>External evaluation is more <strong>practical</strong>, because it allows us to evaluate the word embeddings in the context of a real-world application.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>In <abbr title="Named Entity Recognition">NER</abbr>, we want to identify the names of people, places, organizations, etc. in a text. Given the sentence:</p>
<blockquote>
<p>"Marc is going to Paris to visit the Louvre."</p>
</blockquote>
<p>We can find the following named entities:</p>
<blockquote>
<p>"Marc" (person), "Paris" (location), "Louvre" (organization)</p>
</blockquote>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that with external evaluation methods, we evaluate both the word embeddings and the downstream task. This makes troubleshooting more difficult.</p>
</div>
<h2 id="key-takeaways">Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h2>
<ul>
<li>Word embeddings are a way to represent words as vectors.</li>
<li>With word embeddings, each dimension represents a different <strong>aspect</strong> of the word. We do not know what exactly each dimension represents, but we know that similar words are close to each other in the vector space.</li>
<li>To create word embeddings, we typically use a <strong>machine learning model</strong> that performs a <strong>learning task</strong> on a corpus.</li>
<li>A famous example of a word embeddings model is the <strong><abbr title="Continuous Bag-of-Words">CBOW</abbr></strong> model. It learns to predict the center word based on its context words.</li>
<li>The <strong>rationale</strong> of the <abbr title="Continuous Bag-of-Words">CBOW</abbr> model is, that if two words are surrounded by a similar sets of words when used in various sentences, then those two words tend to be <strong>related in their meaning</strong>.</li>
<li>It's architecture is a <strong>shallow dense neural network</strong> with a single hidden layer. The word embeddings are essentially a by-product of the learning task, i.e. they are the <strong>weights of the hidden layer</strong> after training.</li>
<li>We can evaluate the quality of word embeddings using <strong>intrinsic</strong> and <strong>extrinsic</strong> evaluation methods, where extrinsic evaluation is more practical, because it allows us to evaluate the word embeddings in the context of a real-world application.</li>
</ul>
<!-- footnotes -->
<!-- markdownlint-disable MD041 -->
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Raschka, Sebastian. <em>Build a Large Language Model (From Scratch)</em>. Shelter Island, NY: Manning, 2024. <a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">https://www.manning.com/books/build-a-large-language-model-from-scratch</a>.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:pascal.keilbach@htwg-konstanz.de" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/pkeilbach/htwg-practical-nlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.tooltips", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.top", "toc.follow"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>