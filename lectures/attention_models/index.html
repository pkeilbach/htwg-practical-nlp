
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A practical course on natural language processing @ HTWG Konstanz.">
      
      
      
      
        <link rel="prev" href="../sequence_models/">
      
      
        <link rel="next" href="../further_reading/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Attention Models - Practical NLP</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="cyan" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#attention-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Practical NLP" class="md-header__button md-logo" aria-label="Practical NLP" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Practical NLP
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Attention Models
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pkeilbach/htwg-practical-nlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pkeilbach/htwg-practical-nlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Welcome

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../course_profile/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../getting_started/" class="md-tabs__link">
        
  
  
    
  
  Getting Started

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../preface/" class="md-tabs__link">
          
  
  
    
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../assignments/" class="md-tabs__link">
        
  
  
    
  
  Assignments

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../presentations/" class="md-tabs__link">
        
  
  
    
  
  Presentations

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../faq/" class="md-tabs__link">
        
  
  
    
  
  FAQ

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../exam/" class="md-tabs__link">
        
  
  
    
  
  Exam

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Practical NLP" class="md-nav__button md-logo" aria-label="Practical NLP" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Practical NLP
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pkeilbach/htwg-practical-nlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pkeilbach/htwg-practical-nlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course_profile/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Lectures
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Lectures
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preface/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Preface
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preprocessing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Preprocessing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_extraction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Feature Extraction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../logistic_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logistic Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../naive_bayes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Naive Bayes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vector_space_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector Space Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minimum_edit_distance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Minimum Edit Distance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../language_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../word_embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Word Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sequence_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequence Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Attention Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Attention Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      Attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Transformers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Head Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      Positional Encoding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#encoder-decoder-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Encoder-Decoder Architecture
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../further_reading/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Where to go from here?
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../assignments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Assignments
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../presentations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Presentations
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exam
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      Attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Transformers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Head Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      Positional Encoding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#encoder-decoder-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Encoder-Decoder Architecture
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="attention-models">Attention Models<a class="headerlink" href="#attention-models" title="Permanent link">&para;</a></h1>
<p>This lecture will introduce an imprtant concept of state-of-the-art <abbr title="Natural Language Processing">NLP</abbr> models: <strong>attention</strong>. Specifically, we will look how the <strong>transformer</strong> architecture uses attention to process sequences of words.</p>
<h2 id="attention">Attention<a class="headerlink" href="#attention" title="Permanent link">&para;</a></h2>
<p>Attention is a mechanism that allows models to <strong>focus on specific parts</strong> of input sequences when making predictions or generating output sequences. It mimics the <strong>human ability to selectively concentrate</strong> on relevant information while processing input data. This allows the model to understand and generate more contextually relevant and accurate text.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Imagine you're reading a long article or a book, and there's a specific word or phrase that is <strong>crucial for understanding the overall meaning</strong>. Your attention naturally focuses on that important part to comprehend the context better. Attention models work somewhat similarly.</p>
</div>
<p>In <abbr title="Natural Language Processing">NLP</abbr>, when a computer is trying to understand or generate text, attention models help to decide <strong>which parts of the input text are more important</strong> at any given moment. Instead of treating all words equally, an attention model allows the model to focus <strong>more on specific words</strong> that are relevant to the task at hand.</p>
<!-- TODO EXAM -->
<p>The general attention mechanism makes use of three main components, namely <strong>Query (Q), Key (K), and Value (V)</strong>. They are used to capture the <strong>relationships</strong> between different words in a sequence, and thus, allow the model to focus on different parts of the input sequence when generating or understanding output.</p>
<ul>
<li>
<p><strong>Key (K):</strong> The Key vector is like a unique identifier or key associated with each word in the input sequence. It captures information about the word that is relevant for determining its relationship with other words.</p>
</li>
<li>
<p><strong>Query (Q):</strong> The Query vectors represent the word for which we are trying to compute attention weights. The model uses the Query vectors to compare against the Key vectors of all other words in the sequence.</p>
</li>
<li>
<p><strong>Value (V):</strong> The Value vector is used to calculate the weighted sum, and it represents the content or meaning of the word. The attention weights determine how much importance should be given to each word's content when computing the final attention representation.</p>
</li>
</ul>
<p>Attention is the mechanism that allows the model to find the <strong>best matching keys for a given query</strong>, and return the corresponding values.</p>
<p>The attention mechanism computes the attention scores by <strong>measuring the similarity</strong> between the query and keys: the <em>higher</em> the attention score, the <em>more focus</em> the model places on the corresponding values associated with those keys.</p>
<p>Here is a simplified visualization of the attention mechanism:</p>
<p><img alt="Visualization of the attention mechanism" src="../../img/attention-models-attention-layer.drawio.svg" /></p>
<p>The input for the attention layer are the Query, Key, and Value vectors. The output is the attention vector, which is the weighted sum of the Value vectors.</p>
<!--The attention scores are computed by measuring the similarity between the Query and Key vectors. The softmax function is used to normalize the scores, and the weighted sum of the Value vectors gives the final attention vector. -->

<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Imagine encoding the following sentence :</p>
<blockquote>
<p>Vincent van Gogh is a painter, known for his stunning and emotionally expressive artworks.</p>
</blockquote>
<p>When encoding the <em>query</em> <code>van Gogh</code>, the output may be <code>Vincent van Gogh</code> as the <em>key</em>, with <code>painter</code> as the associated <em>value</em>.</p>
<p>The model stores keys and values in a table, which it can then use for future decoding:</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vincent van Gogh</td>
<td>painter</td>
</tr>
<tr>
<td>William Shakespeare</td>
<td>playwright</td>
</tr>
<tr>
<td>Charles Dickens</td>
<td>writer</td>
</tr>
</tbody>
</table>
<p>Whenever a new sentence is presented, e.g.</p>
<blockquote>
<p>Shakespeare's work has influenced many movies, mostly thanks to his work as a ...</p>
</blockquote>
<p>The model can complete the sentence by taking <code>Shakespeare</code> as the query and finding it in the table of keys and values:</p>
<p><code>Shakespeare</code> as <em>query</em> is closest to <code>William Shakespeare</code> the <em>key</em>, and thus the associated <em>value</em> <code>playwright</code> is presented as the output.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Analogy from Retrieval Systems</p>
<p>I found this analogy from retrieval systems quite helpful to understand the intuition behind the Query, Key, Value concept<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>:</p>
<blockquote>
<p>The Key/Query/Value concept can be seen analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query  (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).</p>
</blockquote>
<p>In attention models, the Query vector is used to retrieve the most relevant Key vectors, and the Value vectors are used to compute the final attention representation.</p>
</div>
<p>Suppose we have a sequence of words <span class="arithmatex">\(X = \{x_1, x_2, ..., x_n\}\)</span>. Depending on the <abbr title="Natural Language Processing">NLP</abbr> task (e.g. translation or summarization), we might want to assign different weights to each word based on its relevance to the current task.</p>
<ol>
<li>
<p><strong>Key, Query, and Value:</strong></p>
<ul>
<li>Each word in the sequence has associated Key <span class="arithmatex">\(K\)</span>, Query <span class="arithmatex">\(Q\)</span>, and Value <span class="arithmatex">\(V\)</span> vectors.</li>
<li>For each word <span class="arithmatex">\(x_i\)</span>, we have corresponding <span class="arithmatex">\(k_i\)</span>, <span class="arithmatex">\(q_i\)</span>, and <span class="arithmatex">\(v_i\)</span> vectors.</li>
</ul>
</li>
<li>
<p><strong>Score Calculation:</strong></p>
<ul>
<li>The attention score <span class="arithmatex">\(e_{ij}\)</span> between a query vector <span class="arithmatex">\(q_i\)</span> and a key vector <span class="arithmatex">\(k_j\)</span> is calculated using a function, most commonly the dot product:</li>
</ul>
<div class="arithmatex">\[
e_{ij} = q_i \cdot k_j
\]</div>
</li>
<li>
<p><strong>Softmax:</strong></p>
<ul>
<li>The scores are passed through a softmax function to get normalized attention weights:</li>
</ul>
<div class="arithmatex">\[
a_{ij} = \frac{e^{e_{ij}}}{\sum_{k=1}^{n} e^{e_{ik}}}
\]</div>
<ul>
<li>These weights <span class="arithmatex">\(a_{ij}\)</span> represent the importance of each word <span class="arithmatex">\(x_j\)</span> with respect to the query word <span class="arithmatex">\(x_i\)</span>.</li>
<li>In other words, the softmax function's output includes which keys are closest to the query.</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Softmax</p>
<p>The softmax function is a generalization of the logistic function to multiple dimensions. It is used to normalize a vector of arbitrary real values to a probability distribution, that is, the sum of all values is 1.</p>
<p>The softmax function is defined as follows:</p>
<div class="arithmatex">\[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\]</div>
<p>The softmax function is often used in classification problems to convert a vector of real values to a probability distribution over predicted output classes.</p>
</div>
</li>
<li>
<p><strong>Weighted Sum:</strong></p>
<ul>
<li>The final attention vector <span class="arithmatex">\(A_i\)</span> for a word <span class="arithmatex">\(x_i\)</span> is the weighted sum of the Value vectors <span class="arithmatex">\(v_j\)</span> across all words, based on the attention weights:</li>
</ul>
<div class="arithmatex">\[
A_i = \sum_{j=1}^{n} a_{ij} \cdot v_j
\]</div>
</li>
</ol>
<p>When we write the equations above in matrix form, we get the following:</p>
<div class="arithmatex">\[
Attention(Q, K, V) = \text{softmax}(QK^T)V
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(Q\)</span> is the matrix of Query vectors.</li>
<li><span class="arithmatex">\(K\)</span> is the matrix of Key vectors.</li>
<li><span class="arithmatex">\(V\)</span> is the matrix of Value vectors.</li>
<li><span class="arithmatex">\(d_k\)</span> is the dimensionality of the Key vectors.</li>
</ul>
<p>In summary, attention allows the model to <strong>dynamically focus</strong> on different parts of the input sequence. It is a <strong>layer of calculations</strong> that let the model <strong>focus on the most important parts</strong> of the sequence for each step.</p>
<p>The attention scores are obtained by measuring the <strong>similarity</strong> between Query and Key vectors, and the <strong>softmax</strong> function ensures that the weights are normalized (i.e. sum to 1).</p>
<p>The weighted sum of the Value vectors, using these attention weights, gives the final <strong>attention vector</strong> for a specific word in the sequence.</p>
<p><strong>Combining all attention vectors</strong> for all words in the sequence gives the final <strong>attention matrix</strong>.</p>
<p>This mechanism allows the model to <strong>selectively attend to relevant information</strong> during processing.</p>
<p>Attention models are used in many <abbr title="Natural Language Processing">NLP</abbr> tasks, such as machine translation, text summarization, and question answering.</p>
<div class="admonition example">
<p class="admonition-title">Machine Translation</p>
<p>The attention mechanism is particularly useful for machine translation as the most relevant words for the output often occur at similar positions in the input sequence.</p>
<p>Here is a visualization of the attention matrix of a translation model<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>:</p>
<p><img alt="Visualization of the attention mechanism" src="../../img/attention-models-visualization.png" /></p>
<p>The x-axis represents the source sentence and the y-axis represents the target sentence.
We can see which positions in the source sentence were considered more important when generating the target word.</p>
<p>The model correctly translates a phrase <code>European Economic Area</code> into <code>zone economique européen</code>.
It was able to correctly align <code>zone</code> with <code>Area</code>, jumping over the two words <code>European</code> and <code>Economic</code>,
and then looked one word back at a time to complete the whole phrase <code>zone economique européen</code>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Here is a great interactive <a href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces">visualization</a> of the attention mechanism (make sure to scroll down a bit).</p>
</div>
<p>Here is the general attention mechanism implemented with SciPy and NumPy<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">softmax</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="c1"># encoder representations of four different words</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="n">word_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">word_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="n">word_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="n">word_4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="c1"># stacking the word embeddings into a single array</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="n">words</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">word_1</span><span class="p">,</span> <span class="n">word_2</span><span class="p">,</span> <span class="n">word_3</span><span class="p">,</span> <span class="n">word_4</span><span class="p">])</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="c1"># generating the weight matrices</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="n">W_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="n">W_K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="n">W_V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="c1"># generating the queries, keys and values</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="n">Q</span> <span class="o">=</span> <span class="n">words</span> <span class="o">@</span> <span class="n">W_Q</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="n">K</span> <span class="o">=</span> <span class="n">words</span> <span class="o">@</span> <span class="n">W_K</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a><span class="n">V</span> <span class="o">=</span> <span class="n">words</span> <span class="o">@</span> <span class="n">W_V</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a><span class="c1"># scoring the query vectors against all key vectors</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a><span class="n">scores</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a><span class="c1"># computing the weights by a softmax operation</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a><span class="n">weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span> <span class="o">/</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a><span class="c1"># computing the attention by a weighted sum of the value vectors</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a><span class="n">attention</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">@</span> <span class="n">V</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a><span class="nb">print</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
</code></pre></div>
<p>This would output the following:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="p">[[</span><span class="mf">0.98522025</span> <span class="mf">1.74174051</span> <span class="mf">0.75652026</span><span class="p">]</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a> <span class="p">[</span><span class="mf">0.90965265</span> <span class="mf">1.40965265</span> <span class="mf">0.5</span>       <span class="p">]</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a> <span class="p">[</span><span class="mf">0.99851226</span> <span class="mf">1.75849334</span> <span class="mf">0.75998108</span><span class="p">]</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a> <span class="p">[</span><span class="mf">0.99560386</span> <span class="mf">1.90407309</span> <span class="mf">0.90846923</span><span class="p">]]</span>
</code></pre></div>
<h2 id="transformers">Transformers<a class="headerlink" href="#transformers" title="Permanent link">&para;</a></h2>
<p>Now the next step is to port the attention mechanism into a neural network architecture. This is where <strong>transformers</strong> come into play.</p>
<p>Transformers make use of the <strong>attention mechanism</strong> to process sequences of words.</p>
<blockquote>
<p>We could also say transformers is a type of neural network architecture that <em>implements</em> the attention mechanism.</p>
</blockquote>
<p>They are a type of neural network architecture that is used for a wide range of natural language processing tasks, such as machine translation, text summarization, and question answering.</p>
<!-- TODO EXAM -->
<p>The main <strong>advantages</strong> of Transformers over RNNs are:</p>
<ul>
<li>❌ RNNs are slow to train because they process words sequentially and cannot be parallelized.</li>
<li>❌ RNNs cannot capture long-range dependencies effectively.</li>
<li>❌ RNNs are computationally expensive.</li>
<li>✅ Transformers can capture long-range dependencies more effectively using attention.</li>
<li>✅ Transformers are parallelizable</li>
<li>✅ Transformers are faster to train and require less computation.</li>
</ul>
<p>Since the position of a word and the order of words in a sentence are important to understand the meaning of a text, while still being able to process text in parallel, Transformers use <strong>positional encoding</strong> to keep track of the sequence.</p>
<p>To be more specific, transformers use a variant of the attention mechanism, called <strong>multi-head attention</strong>, which allows them to capture more complex patterns and relationships between words in the sequence.</p>
<p>We will look at both of these concepts in more detail below.</p>
<div class="admonition info">
<p class="admonition-title">Attention Is All You Need</p>
<p>The transformer architecture was introduced in the paper <a href="https://arxiv.org/abs/1706.03762">"Attention Is All You Need"</a> by Vaswani, et al. from 2017.</p>
<p>It was a breakthrough in the field of natural language processing, as it outperformed previous state-of-the-art models on a wide range of tasks, including machine translation, text summarization, and question answering<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup>:</p>
<p>This figure is taken from Google's <a href="https://blog.research.google/2017/08/transformer-novel-neural-network.html">blog post</a>, accompanying the original paper:</p>
<p><img alt="BLEU scores of single models on the standard WMT newstest2014 English to German translation benchmark." src="https://4.bp.blogspot.com/-ovHQGevt5Ks/WaiCfS0OPUI/AAAAAAAAB_U/nEqsh9fgecM1v98NAvGp8Zgr5BwBbOGBQCEwYBhgL/s1600/image4.png" width="50%" /></p>
</div>
<div class="admonition info">
<p class="admonition-title">BLUE Score</p>
<p>The BLEU score is a metric for evaluating the <strong>quality of machine translation</strong>. It is based on the precision of the translation, and it measures how many words in the machine translation match the reference translation. The <strong>higher</strong> the BLEU score, the <strong>better</strong> the translation.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Large Language Models</p>
<p>Large language models (LLMs) are essentially a <strong>type of transformer</strong> model that are trained on large amounts of text data. The GPT models by <a href="https://platform.openai.com/docs/models">OpenAI</a> are examples of large language models.</p>
</div>
<h3 id="multi-head-attention">Multi-Head Attention<a class="headerlink" href="#multi-head-attention" title="Permanent link">&para;</a></h3>
<p>Attention mechanisms enable models to focus on different parts of the input sequence when processing <strong>each output token</strong>, allowing them to capture long-range dependencies effectively.</p>
<p>Multi-head attention extends this concept by computing attention from multiple linearly projected "heads" in <strong>parallel</strong>, which are then concatenated and projected again to obtain the final output.</p>
<p>Multi-head attention involves the following steps:</p>
<ol>
<li>
<p><strong>Splitting</strong>: Multi-head attention involves splitting the query (Q), key (K), and value (V) vectors into multiple smaller vectors, or "heads."</p>
</li>
<li>
<p><strong>Independent Attention Calculation</strong>: Attention scores are calculated independently for each head, allowing the model to focus on different parts of the input sequence simultaneously.</p>
</li>
<li>
<p><strong>Weighted Summation</strong>: Using the attention scores, a weighted sum of the value vectors is computed for each head.</p>
</li>
<li>
<p><strong>Concatenation and Projection</strong>: The results from all heads are concatenated and linearly projected again to obtain the final output of the multi-head attention layer.</p>
</li>
</ol>
<p><img alt="Multi-Head Attention" src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_12.17.05_AM_st5S0XV.png" width="50%" /></p>
<p>Not only is this more <strong>computationally efficient</strong>, but it also enables the model to <strong>capture more complex patterns</strong> and relationships between words in the sequence.
This is because  it enables the model to process and combine multiple aspects of the input data simultaneously, leading to richer and more nuanced representations.</p>
<div class="admonition info">
<p class="admonition-title">Self Attention</p>
<p>Traditional attention focuses on different parts of the <strong>input sequence</strong> (e.g., English sentence) to generate each word in the <strong>output sequence</strong> (e.g., translated French sentence).</p>
<p>Self attention, on the other hand, captures dependencies and relationships <strong>within the same sequence</strong> (e.g., an English sentence) by allowing each element to attend to all other elements, including itself.</p>
<p>It is computed on the fly, meaning that the query, key, and value vectors are all <strong>derived from the same input sequence</strong>.</p>
<p>Multi-head attention can be seen as an <strong>extension</strong> of self attention, where the input is split into multiple heads, and attention is computed independently for each head.</p>
<p>In the case where <code>it</code> refers to the animal, we can observe a different attention than when <code>it</code> refers to the street:</p>
<p><img alt="Example of self attention" src="https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png" /></p>
</div>
<h3 id="positional-encoding">Positional Encoding<a class="headerlink" href="#positional-encoding" title="Permanent link">&para;</a></h3>
<p>Transformers <strong>do not inherently understand the sequential order</strong> of the input tokens.</p>
<p>This brings huge computational benefits, because if the order doesn't matter, we can process the text in <strong>parallel</strong>.</p>
<p>But the position of a word and the order of words in a sentence are important to understand the meaning of a text.</p>
<p>Therefore transformers must still keep track of the <strong>order of words in a sentence</strong>, and must somehow inject the positional information of the words in the sequence into the model, namely into the input embeddings.</p>
<p>This is done by adding <strong>positional encoding</strong> to the input embeddings.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>In a simple version of positional encoding, we could encode the position of a word in a sentence by using a single number to represent its position in the sequence.</p>
<p>Given the sentence:</p>
<blockquote>
<p>The work of William Shakespeare inspired many movies.</p>
</blockquote>
<p>We could encode the position of each word in the sentence as follows:</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>Index</th>
</tr>
</thead>
<tbody>
<tr>
<td>The</td>
<td>0</td>
</tr>
<tr>
<td>work</td>
<td>1</td>
</tr>
<tr>
<td>of</td>
<td>2</td>
</tr>
<tr>
<td>William</td>
<td>3</td>
</tr>
<tr>
<td>Shakespeare</td>
<td>4</td>
</tr>
<tr>
<td>inspired</td>
<td>5</td>
</tr>
<tr>
<td>many</td>
<td>6</td>
</tr>
<tr>
<td>movies</td>
<td>7</td>
</tr>
</tbody>
</table>
</div>
<p>Mathematically, positional encoding in transformers means <strong>adding</strong> the positional vectors to the input embeddings (they have the same shape). By doing so, the encoded text includes information about the <strong>both the meaning and position</strong> of a word in a sentence.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Word Embeddings + Positional Encoding = Input Embeddings</p>
</div>
<p>For Transformers, the encoding mechanism is a bit more complex than in the example above. It uses a <strong>sine</strong> and <strong>cosine</strong> function to encode the position of a word in a sentence. This allows the model to learn <strong>relative positions</strong> between words, which is important for understanding the meaning of a sentence.</p>
<h3 id="encoder-decoder-architecture">Encoder-Decoder Architecture<a class="headerlink" href="#encoder-decoder-architecture" title="Permanent link">&para;</a></h3>
<p>There are two main components in the transformer architecture:</p>
<ol>
<li>
<p><strong>Encoder</strong>: The encoder takes the <strong>input sequence</strong> and generates a representation of it (i.e. a vector) that captures the meaning of the sequence. This representation is then passed to the decoder.</p>
</li>
<li>
<p><strong>Decoder</strong>: The decoder takes the representation generated by the encoder and uses it to generate the <strong>output sequence</strong>, one word at a time.</p>
</li>
</ol>
<p>The encoder and decoder are both composed of <strong>multiple layers</strong>, each of which contains a multi-head attention layer and a feed-forward neural network. The encoder and decoder layers are stacked on top of each other, with each layer passing its output to the next layer in the stack.</p>
<p><img alt="Simplified Encoder-Decoder Architecture of the Transformer Model" src="../../img/attention-models-encoder-decoder.drawio.svg" /></p>
<p>The connection between the encoder and decoder is established through the <strong>attention</strong> mechanism, which allows the decoder to <strong>selectively attend</strong> to different parts of the input sequence based on their importance for generating the next token in the output sequence.</p>
<p>This enables the model to effectively capture dependencies between input and output sequences, making it well-suited for tasks such as machine translation, where the input and output sequences may have complex and non-linear relationships.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <a href="https://arxiv.org/abs/1706.03762">original paper</a> shows the transformer architecture in more detail.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sentence word by word while consulting the representation generated by the encoder.</p>
<p>The animation below shows the encoder-decoder architecture of the transformer model in action. It is from the <a href="https://blog.research.google/2017/08/transformer-novel-neural-network.html">transformer blog post</a> by Google:</p>
<ul>
<li>The Transformer starts by generating <strong>initial representations</strong>, or embeddings, for each word. These are represented by the unfilled circles.</li>
<li>Then, using self-attention, it <strong>aggregates information from all of the other words</strong>, generating a <strong>new representation</strong> per word <strong>informed by the entire context</strong>, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations.</li>
<li>The decoder operates similarly, but generates <strong>one word at a time</strong>, from left to right. It attends not only to the other previously generated words, but also to the final representations generated by the encoder.</li>
</ul>
<p><img alt="Animation of Transformer Encoder-Decoder for Machine Translation" src="https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif" /></p>
</div>
<h2 id="key-takeaways">Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h2>
<ul>
<li>Attention models address the limitations of sequence models like RNNs by enabling the model to <strong>selectively weigh and focus</strong> on specific parts of the input sequence when making predictions or generating output sequences.</li>
<li>Attention <strong>mimics the human ability</strong> to selectively concentrate on relevant information while processing input data.</li>
<li>Mathematically, attention is a weighted sum of the values, calculated from <strong>Query (Q) and Key (K) and Value (V)</strong> vectors.</li>
<li>A model architecture that uses attention is called a <strong>Transformer</strong>. It is a type of neural network architecture that is used for a wide range of natural language processing tasks, such as machine translation, text summarization, and question answering.</li>
<li>The transformer architecture was introduced in the paper <a href="https://arxiv.org/abs/1706.03762">"Attention Is All You Need"</a> by Vaswani, et al. from 2017 and set new standards in natural language processing.</li>
<li>Transformers can be <strong>parallelized</strong> because attention does not require sequential processing of words, making them much faster to train than RNNs. However, to process text in parallel, they still need to maintain the <strong>order of words</strong> in a sentence, which is achieved by incorporating <strong>positional encoding</strong> into the input embeddings.</li>
<li><strong>Multi-head attention</strong> is a variant of the attention mechanism that allows the model to capture more complex patterns and relationships between words in the sequence.</li>
<li>The transformer model consists of an <strong>encoder</strong> and a <strong>decoder</strong> that are connected through the attention mechanism.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Further Reading</p>
<p>The transformer is a sophisticated architecture and attention is a complex concept. If you want to dig deeper into those topics, here are some resources that may be interesting for you:</p>
<ul>
<li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></li>
<li><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">https://jalammar.github.io/how-gpt3-works-visualizations-animations/</a></li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://learn.microsoft.com/en-us/training/modules/explore-foundation-models-in-model-catalog/4-transformers">Microsoft Learn: Understand the transformer architecture used for <abbr title="Natural Language Processing">NLP</abbr></a></li>
<li><a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html">The Transformer Family</a></li>
</ul>
</div>
<!-- footnotes -->

<!-- markdownlint-disable MD053 -->
<!-- markdownlint-disable MD041 -->
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><a href="https://stats.stackexchange.com/a/424127">https://stats.stackexchange.com/a/424127</a>&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p><a href="https://machinelearningmastery.com/the-attention-mechanism-from-scratch/">https://machinelearningmastery.com/the-attention-mechanism-from-scratch/</a>&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p><a href="https://blog.research.google/2017/08/transformer-novel-neural-network.html">https://blog.research.google/2017/08/transformer-novel-neural-network.html</a>&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:pascal.keilbach@htwg-konstanz.de" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/pkeilbach/htwg-practical-nlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.tooltips", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.top", "toc.follow"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>