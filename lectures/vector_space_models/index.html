
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A practical course on natural language processing @ HTWG Konstanz.">
      
      
      
      
        <link rel="prev" href="../naive_bayes/">
      
      
        <link rel="next" href="../minimum_edit_distance/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Vector Space Models - Practical NLP</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="cyan" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#vector-space-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Practical NLP" class="md-header__button md-logo" aria-label="Practical NLP" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Practical NLP
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Vector Space Models
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pkeilbach/htwg-practical-nlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pkeilbach/htwg-practical-nlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Welcome

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../course_profile/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../getting_started/" class="md-tabs__link">
        
  
  
    
  
  Getting Started

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../preface/" class="md-tabs__link">
          
  
  
    
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../assignments/" class="md-tabs__link">
        
  
  
    
  
  Assignments

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../presentations/" class="md-tabs__link">
        
  
  
    
  
  Presentations

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../faq/" class="md-tabs__link">
        
  
  
    
  
  FAQ

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../exam/" class="md-tabs__link">
        
  
  
    
  
  Exam

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Practical NLP" class="md-nav__button md-logo" aria-label="Practical NLP" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Practical NLP
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pkeilbach/htwg-practical-nlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pkeilbach/htwg-practical-nlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course_profile/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Lectures
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Lectures
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preface/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Preface
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preprocessing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Preprocessing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_extraction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Feature Extraction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../logistic_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logistic Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../naive_bayes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Naive Bayes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Vector Space Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Vector Space Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#co-occurrence-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Co-Occurrence Matrix
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-by-word-design" class="md-nav__link">
    <span class="md-ellipsis">
      Word by Word Design
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-by-document-design" class="md-nav__link">
    <span class="md-ellipsis">
      Word by Document Design
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eucledian-distance" class="md-nav__link">
    <span class="md-ellipsis">
      Eucledian Distance
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cosine-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      Cosine Similarity
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#working-with-word-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      Working with Word Vectors
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transforming-word-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      Transforming Word Vectors
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minimum_edit_distance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Minimum Edit Distance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../language_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../word_embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Word Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sequence_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequence Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../attention_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../further_reading/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Where to go from here?
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../assignments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Assignments
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../presentations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Presentations
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exam
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#co-occurrence-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Co-Occurrence Matrix
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-by-word-design" class="md-nav__link">
    <span class="md-ellipsis">
      Word by Word Design
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-by-document-design" class="md-nav__link">
    <span class="md-ellipsis">
      Word by Document Design
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eucledian-distance" class="md-nav__link">
    <span class="md-ellipsis">
      Eucledian Distance
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cosine-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      Cosine Similarity
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#working-with-word-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      Working with Word Vectors
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transforming-word-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      Transforming Word Vectors
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="vector-space-models">Vector Space Models<a class="headerlink" href="#vector-space-models" title="Permanent link">&para;</a></h1>
<p>Vector space models are a way of representing the meaning of words in a document. They are a <strong>fundamental concept in <abbr title="Natural Language Processing">NLP</abbr></strong>, and are used in many applications such as document classification, information retrieval, and question answering.</p>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>Using vector space models, we can capture <strong>similarities, differences, dependencies</strong> or many other <strong>relationships</strong> between words.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>The following sentence have very <strong>similar words</strong>, but they have <strong>different meanings</strong>:</p>
<blockquote>
<p>Where are you <strong>from</strong>?</p>
<p>Where are you <strong>going</strong>?</p>
</blockquote>
<p>In contrast, the following sentences have very <strong>different words</strong>, but they have <strong>similar meanings</strong>:</p>
<blockquote>
<p>What is your age?</p>
<p>How old are you?</p>
</blockquote>
<p>Vector space models can be used to capture such similarities and differences between words.</p>
</div>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>In the following sentence, the word <strong>cereal</strong> and the word <strong>spoon</strong> are related.</p>
<blockquote>
<p>You eat <strong>cereal</strong> with a <strong>spoon</strong>.</p>
</blockquote>
<p>In the following sentence, the word <strong>sell</strong> depends on the word <strong>buy</strong>.</p>
<blockquote>
<p>You <strong>sell</strong> something to someone who <strong>buys</strong> it.</p>
</blockquote>
</div>
<p>Vector space models help us to capture such and many other relationships between words.</p>
<div class="admonition quote">
<p class="admonition-title">John Firth, 1957</p>
<p><strong>You shall know a word by the company it keeps.</strong></p>
<p>This is one of the most fundamental concepts in <abbr title="Natural Language Processing">NLP</abbr>. When using vector space models, the way that representations are made is by identifying the context around each word in the text, which captures the relative meaning.</p>
</div>
<p>When learning these vectors, we usually make use of the neighboring words to extract meaning and information about the center word.</p>
<p>If we would cluster vectors together, we can observe that adjectives, nouns, verbs, etc. tend to be near to one another.</p>
<div class="admonition question">
<p class="admonition-title">Question</p>
<p>In vector space models, synonyms and antonyms are very close to one another. Why do you think this is the case?</p>
</div>
<!--
Answer: this is because you can easily interchange them in a sentence, and they tend to have similar neighboring words!

Synonyms:
I bought a new automobile last week.
I bought a new car last week.

Antonyms:
She considered him her enemy
She considered him her friend
-->

<h2 id="co-occurrence-matrix">Co-Occurrence Matrix<a class="headerlink" href="#co-occurrence-matrix" title="Permanent link">&para;</a></h2>
<p>The <strong>co-occurrence matrix</strong> is a matrix that counts the number of times that a word appears in the context of other words within a given window size <span class="arithmatex">\(k\)</span>.</p>
<p>From the co-occurrence matrix, we can extract the word vectors.</p>
<p>The <strong>vector representation</strong> of a word is called a <strong>word embedding</strong>.</p>
<p>We can use those word embeddings to find <strong>relationships</strong> between words.</p>
<p>In the following, we will look at two different approaches to create word embeddings.</p>
<p><img alt="Vector space models workflow" src="../../img/vector-space-models-workflow.drawio.svg" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>The terms <strong>word vector</strong> is often used interchangeably with <strong>word embedding</strong>.
Both terms refer to a numerical representation of words in a continuous vector space.</p>
</div>
<h2 id="word-by-word-design">Word by Word Design<a class="headerlink" href="#word-by-word-design" title="Permanent link">&para;</a></h2>
<p>In the word by word design, the <strong>co-occurrence matrix</strong> counts the number of times that a word appears in the context of other words within a given window size <span class="arithmatex">\(k\)</span>.</p>
<!-- TODO EXAM -->
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Suppose we have the following two sentences:</p>
<blockquote>
<p>I like simple data</p>
<p>I prefer simple raw data</p>
</blockquote>
<p>With a window size of <span class="arithmatex">\(k=2\)</span>, the co-occurrence matrix would look as follows:</p>
<table>
<thead>
<tr>
<th></th>
<th>I</th>
<th>like</th>
<th>prefer</th>
<th>simple</th>
<th>raw</th>
<th>data</th>
</tr>
</thead>
<tbody>
<tr>
<td>I</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>like</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>prefer</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>simple</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>raw</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>data</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>If we look more closely at the word <strong>data</strong>, we can see that it appears in the context of the word <strong>simple</strong> twice, and in the context of the word <strong>and</strong> once, given a window size of <span class="arithmatex">\(k=2\)</span>.</p>
<p>So the word <strong>data</strong> can be represented as the following vector:</p>
<div class="arithmatex">\[x_{data} = [0, 1, 0, 2, 1, 0]\]</div>
<p>Note that the vector is of size <span class="arithmatex">\(n\)</span>, where <span class="arithmatex">\(n\)</span> is the number of unique words in the vocabulary.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The co-occurance matrix, as shown here, shows the <strong>unigram</strong> counts. Later in the course, we will also look at <strong>bigram</strong> counts, or <strong>n-gram</strong> counts in general, which allow us to capture more context. This concept is the basis for <strong>language models</strong>.</p>
</div>
<h2 id="word-by-document-design">Word by Document Design<a class="headerlink" href="#word-by-document-design" title="Permanent link">&para;</a></h2>
<p>For a word by document design, the process is quite similar.</p>
<p>But instead of counting co-occurrences of words, we count the number of times that a word appears in documents of a specific category.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Let's assume our corpus contains documents of three categories:</p>
<ul>
<li>entertainment</li>
<li>economy</li>
<li>machine learning</li>
</ul>
<p>For the words <strong>data</strong> and <strong>movie</strong>, we could assume the following counts per category:</p>
<table>
<thead>
<tr>
<th></th>
<th>entertainment</th>
<th>economy</th>
<th>machine learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>data</td>
<td>1000</td>
<td>4000</td>
<td>9000</td>
</tr>
<tr>
<td>movie</td>
<td>7500</td>
<td>2000</td>
<td>500</td>
</tr>
</tbody>
</table>
<p>So the word <strong>data</strong> can be represented as the following vector:</p>
<div class="arithmatex">\[x_{data} = [1000, 4000, 9000]\]</div>
<p>And the word <strong>movie</strong> can be represented as the following vector:</p>
<div class="arithmatex">\[x_{movie} = [7500, 2000, 500]\]</div>
<p>Note that the vector is of size <span class="arithmatex">\(n\)</span>, where <span class="arithmatex">\(n\)</span> is the number of categories.</p>
</div>
<p>We could visualize those vectors in a vector space as follows:</p>
<p><img alt="Word by document design" src="../../img/vector-space-models-word-by-document.drawio.svg" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the sake of drawing, instead of using the categories as the axis and showing the word vectors, the figure shows the vectors of the categories.</p>
<p>However, we should see similar results if we would draw the word vectors in a three-dimensional vector space, using the categories as the axes (as you should usually do).</p>
</div>
<h2 id="eucledian-distance">Eucledian Distance<a class="headerlink" href="#eucledian-distance" title="Permanent link">&para;</a></h2>
<p>The <strong>Euclidean distance</strong> between two vectors <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span> in an n-dimensional space is defined as follows:</p>
<div class="arithmatex">\[
\begin{align}
d &amp;= \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_n - y_n)^2} \\
&amp;= \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
\end{align}
\]</div>
<p>It is the <strong>straight line</strong> between two points in an n-dimensional space, and derived from the Pythagorean theorem.</p>
<p>We can also interpret it as the <strong>length of the vector</strong> that connects the two points (aka the <strong>norm</strong> of the vector).</p>
<div class="admonition quote">
<p class="admonition-title">Pythagoraen Theorem</p>
<p>In a right triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides.</p>
<div class="arithmatex">\[
c^2 = a^2 + b^2
\]</div>
</div>
<p>Using the Euclidean distance, we can calculate <strong>how close</strong> two vectors are and thus, use it as a <strong>similarity metric</strong>.</p>
<p><img alt="Euclidean distance" src="../../img/vector-space-models-euclidean-distance.drawio.svg" /></p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>If we continue with the example from above, we can calculate the Euclidean distance between the <strong>machine learning</strong> category vector <span class="arithmatex">\(\mathbf{ml}\)</span> and the <strong>entertainment</strong> category vector <span class="arithmatex">\(\mathbf{e}\)</span> as follows:</p>
<p>Let the two vectors be:</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{ml} &amp;= [9000, 500] \\
\mathbf{e} &amp;= [1000, 7500]
\end{align}
\]</div>
<p>Then the Euclidean distance is calculated as follows:</p>
<div class="arithmatex">\[
\begin{align}
d &amp;= \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} \\
&amp;= \sqrt{(ml_1 - e_1)^2 + (ml_2 - e_2)^2} \\
&amp;= \sqrt{(9000 - 1000)^2 + (500 - 7500)^2} \\
&amp;= 8000
\end{align}
\]</div>
</div>
<p>As you can see from the formula above, this generalizes to <strong>any number of dimensions</strong>.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Let's assume we have the following <strong>co-oocurrence matrix</strong>, and we want to calculate the Euclidean distance between the words <strong>beer</strong> and <strong>pizza</strong>.</p>
<table>
<thead>
<tr>
<th></th>
<th>data</th>
<th>beer</th>
<th>pizza</th>
</tr>
</thead>
<tbody>
<tr>
<td>AI</td>
<td>6</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>drinks</td>
<td>0</td>
<td>4</td>
<td>6</td>
</tr>
<tr>
<td>food</td>
<td>0</td>
<td>6</td>
<td>8</td>
</tr>
</tbody>
</table>
<p>Based on this co-occurrence matrix, we can represent the words <strong>beer</strong> and <strong>pizza</strong> as the following vectors:</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{b} &amp;= [0, 4, 6] \\
\mathbf{p} &amp;= [1, 6, 8]
\end{align}
\]</div>
<p>Then the Euclidean distance is calculated as follows:</p>
<div class="arithmatex">\[
\begin{align}
d &amp;= \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2} \\
&amp;= \sqrt{(b_1 - p_1)^2 + (b_2 - p_2)^2 + (b_3 - p_3)^2} \\
&amp;= \sqrt{(0 - 1)^2 + (4 - 6)^2 + (6 - 8)^2} \\
&amp;= \sqrt{1 + 4 + 4} \\
&amp;= 3
\end{align}
\]</div>
</div>
<p>In Python, we can calculate the Euclidean distance using the <code>numpy.linalg.norm</code> function.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="c1"># Define two vectors</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="c1"># Calculate the Euclidean distance</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="n">distance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Euclidean distance:&quot;</span><span class="p">,</span> <span class="n">distance</span><span class="p">)</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"><code>numpy.linalg.norm</code></a> function calculates the norm of a vector or matrix.</p>
<p>Since the <strong>norm of a vector is its length</strong>, the function can be used to calculate the Euclidean distance between two vectors.</p>
</div>
<h2 id="cosine-similarity">Cosine Similarity<a class="headerlink" href="#cosine-similarity" title="Permanent link">&para;</a></h2>
<p>The cosine similarity is another way of measuring the similarity between two vectors.</p>
<p>The <strong>intuition</strong> is that vectors pointing in the <strong>similar direction</strong> share a similar proportion of words, and thus, are more similar to each other.</p>
<p>Let's take a look at the following figure:</p>
<p><img alt="Cosine similarity" src="../../img/vector-space-models-euclidean-distance-cosine-similarity.drawio.svg" /></p>
<p>Normally, the <strong>food</strong> and <strong>agriculture</strong> categories would be considered more similar, because they share a similar proportion of words.</p>
<p>However, the Euclidean distance would suggest that the <strong>agriculture</strong> and <strong>history</strong> categories are more similar to each other than the <strong>agriculture</strong> and <strong>food</strong> categories, since <span class="arithmatex">\(d_2 &lt; d_1\)</span>.</p>
<p>This is because the <strong>Euclidean distance</strong> is biased towards longer vectors, or in our case, categories with more words.</p>
<p>To avoid that, we could also compare the angles, or the <strong>cosine of the angles</strong>, between the vectors to measure their similarity.</p>
<p>As the figure shows, the angle between the <strong>agriculture</strong> and <strong>food</strong> categories is smaller than the angle between the <strong>agriculture</strong> and <strong>history</strong> categories, and thus, would be a better measure of similarity in this case.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>When corpora are different in size, the Euclidean distance is <strong>biased towards longer vectors</strong>. In such cases, it is better to use the <strong>cosine similarity</strong> as a measure of similarity.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Form the figure, we can see that:</p>
<ul>
<li>vectors pointing in the <strong>similar direction</strong> means their word frequencies are <strong>similar</strong>, and</li>
<li>vectors pointing in <strong>different directions</strong> means the word frequencies are <strong>dissimilar</strong>.</li>
</ul>
</div>
<p>Let's take a look at the math behind the cosine similarity.</p>
<p>The <strong>norm</strong> of a vector <span class="arithmatex">\(\mathbf{x}\)</span>, or its length, is defined as the square root of the sum of the squared vector elements:</p>
<div class="arithmatex">\[
\|\mathbf{x}\| = \sqrt{\sum_{i=1}^n x_i^2}
\]</div>
<p>The <strong>dot product</strong> of two vectors <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span> is defined as the sum of the products of the corresponding vector elements:</p>
<div class="arithmatex">\[
\mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^n x_i \cdot y_i
\]</div>
<p>From trigonometry, we know that the <strong>cosine of the angle</strong> is defined as follows:</p>
<div class="arithmatex">\[
\cos(\theta) = \frac{\text{adjacent}}{\text{hypotenuse}}
\]</div>
<p>In vector space,</p>
<ul>
<li>the <strong>adjacent</strong> is the <strong>dot product</strong> of the two vectors (the projection of one vector onto the other), and</li>
<li>the <strong>hypotenuse</strong> is the <strong>product of the norms</strong> of the two vectors,</li>
</ul>
<p>which leads us to the following formula:</p>
<div class="arithmatex">\[
\cos(\theta) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}
\]</div>
<p>The following figure shows how we can use the cosine similarity to measure the similarity between two vectors.</p>
<p><img alt="Cosine similarity" src="../../img/vector-space-models-cosine-similarity.drawio.svg" /></p>
<!-- TODO EXAM -->
<ul>
<li>If the vectors are orthogonal, like the vectors <span class="arithmatex">\(\mathbf{v}\)</span> and <span class="arithmatex">\(\mathbf{w}\)</span>, the cosine similarity is 0, since <span class="arithmatex">\(\cos(90) = 0\)</span>.</li>
<li>If the vectors point exactly in the same direction, like the vectors <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span>, the cosine similarity is 1, since <span class="arithmatex">\(\cos(0) = 1\)</span>.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since we are dealing with word counts, there won't be any negative values in the vectors, and our vectors will always point in the first quadrant.</p>
</div>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Given the following diagram</p>
<p><img alt="Cosine similarity example" src="../../img/vector-space-models-cosine-similarity-example.drawio.svg" /></p>
<p>we can calculate the cosine similarity between the vectors for the categories <strong>agriculture</strong> <span class="arithmatex">\(\mathbf{a}\)</span> and <strong>history</strong> <span class="arithmatex">\(\mathbf{h}\)</span> as follows:</p>
<p>Let the two vectors be:</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{a} &amp;= [10, 40] \\
\mathbf{h} &amp;= [30, 10]
\end{align}
\]</div>
<p>Then the cosine similarity is calculated as follows:</p>
<div class="arithmatex">\[
\begin{align}
\cos(\theta) &amp;= \frac{\mathbf{a} \cdot \mathbf{h}}{\|\mathbf{a}\| \|\mathbf{h}\|} \\
&amp;= \frac{\sum_{i=1}^n a_i h_i}{\sqrt{\sum_{i=1}^n a_i^2} \sqrt{\sum_{i=1}^n h_i^2}} \\
&amp;= \frac{(10 \times 30) + (40 \times 10)}{\sqrt{(10^2 + 40^2)} \sqrt{(30^2 + 10^2)}} \\
&amp;= 0.5368
\end{align}
\]</div>
</div>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Let's assume we have the following <strong>co-oocurrence matrix</strong>, and we want to calculate the cosine similarity between the words <strong>beer</strong> and <strong>pizza</strong>.</p>
<table>
<thead>
<tr>
<th></th>
<th>data</th>
<th>beer</th>
<th>pizza</th>
</tr>
</thead>
<tbody>
<tr>
<td>AI</td>
<td>6</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>drinks</td>
<td>0</td>
<td>4</td>
<td>6</td>
</tr>
<tr>
<td>food</td>
<td>0</td>
<td>6</td>
<td>8</td>
</tr>
</tbody>
</table>
<p>Based on this co-occurrence matrix, we can represent the words <strong>beer</strong> and <strong>pizza</strong> as the following vectors:</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{b} &amp;= [0, 4, 6] \\
\mathbf{p} &amp;= [1, 6, 8]
\end{align}
\]</div>
<p>Then the cosine similarity is calculated as follows:</p>
<div class="arithmatex">\[
\begin{align}
\cos(\theta) &amp;= \frac{\mathbf{b} \cdot \mathbf{p}}{\|\mathbf{b}\| \|\mathbf{p}\|} \\
&amp;= \frac{\sum_{i=1}^n b_i p_i}{\sqrt{\sum_{i=1}^n b_i^2} \sqrt{\sum_{i=1}^n p_i^2}} \\
&amp;= \frac{(0 \times 1) + (4 \times 6) + (6 \times 8)}{\sqrt{(0^2 + 4^2 + 6^2)} \sqrt{(1^2 + 6^2 + 8^2)}} \\
&amp;= 0.9935
\end{align}
\]</div>
</div>
<p>Here is a NumPy implementation of the cosine similarity:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="c1"># Define two vectors</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="c1"># Calculate the cosine similarity</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="n">similarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cosine similarity:&quot;</span><span class="p">,</span> <span class="n">similarity</span><span class="p">)</span>
</code></pre></div>
<h2 id="working-with-word-vectors">Working with Word Vectors<a class="headerlink" href="#working-with-word-vectors" title="Permanent link">&para;</a></h2>
<p>We can use word vectors to find relationships between words.</p>
<p>Using simple vector arithmetics, we can already do some interesting things, like finding the capital of a country.</p>
<p><img alt="Working with word vectors" src="../../img/vector-space-models-capitals.drawio.svg" /></p>
<p>Knowing that the capital of Germany is Berlin, we can use this relationship to find the capital of the USA.</p>
<div class="admonition question">
<p class="admonition-title">Question</p>
<p>If you were given the pair <strong>Australia</strong> and <strong>Sydney</strong> instead of Germany and Berlin, where do you think we would end up in the vector space for the USA?</p>
</div>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>A similar analogy would be the following:</p>
<p>prince 🤴 - male ♂️ + female ♀️ = princess 👸</p>
</div>
<p>Suppose we have the following vector space, that has vector representations for countries and their capitals.</p>
<p><img alt="Countries and their capitals" src="../../img/vector-space-models-countries-capitals.drawio.svg" /></p>
<p>We can express the relationship between a country and its capital by the <strong>difference</strong> between their vectors.</p>
<p>Using this relationship, we can find the capital of a country by <strong>adding the difference vector</strong> to the vector of the country.</p>
<p>Usually, we do not end up exactly at the capital, so we need to utilize <strong>similarity metrics</strong> to find the closest vector.</p>
<p>As we learned, we could use the Euclidean distance or the cosine similarity to achieve this.</p>
<p>In the end, we can <strong>leverage known relationship</strong>s between words to find unknown relationships and make predictions.</p>
<!-- TODO EXAM -->
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Form the figure, we can see that</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{v}_{\text{Germany}} = [5, 6]\\
\mathbf{v}_{\text{Berlin}} = [10,5]
\end{align}
\]</div>
<p>Given these vectors, we can derive that the relationship <span class="arithmatex">\(\mathbf{r}\)</span> between Germany and its capital Berlin can be expressed by the difference between the two vectors:</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{r} &amp;= \mathbf{v}_{\text{Germany}} - \mathbf{v}_{\text{Berlin}} \\
&amp;= [10, 5] - [5, 6] \\
&amp;= [5, -1]
\end{align}
\]</div>
<p>So the relationship <span class="arithmatex">\(\mathbf{r}\)</span> between country and capital can be expressed by the vector <span class="arithmatex">\([5, -1]\)</span>.</p>
<p>If we add this vector to the vector for the USA, we should end up close to the capital of the USA.</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{v}_{\text{dest}} &amp;= \mathbf{v}_{\text{USA}} + \mathbf{r} \\
&amp;= [5, 5] + [5, -1] \\
&amp;= [10, 4]
\end{align}
\]</div>
<p>Now we can use the Euclidean distance or the cosine similarity to find the capital that is closest to the vector <span class="arithmatex">\(\mathbf{v}_{\text{dest}}\)</span>.</p>
<p>Looking at the figure, the capital that is <strong>closest</strong> to the vector <span class="arithmatex">\(\mathbf{v}_{\text{dest}}\)</span> is <strong>Washington DC</strong> with <span class="arithmatex">\(\mathbf{v}_{\text{Washington DC}} = [9, 3]\)</span>.</p>
</div>
<p>Having words represented in a vector space allows us to capture relative meaning of words and find patterns in text. This is the <strong>basis</strong> for many advanced <abbr title="Natural Language Processing">NLP</abbr> tasks.</p>
<p>As you can imagine, similar words will have similar vectors, and thus, will be <strong>close to each other in the vector space</strong>.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Given the word <strong>doctor</strong>, if we look at the words that are close to it in the vector space, we can see that they are probably all related to the medical field. For example: <strong>nurse</strong>, <strong>hospital</strong>, <strong>patient</strong>, <strong>medicine</strong>, etc.</p>
<p>We can also imagine that all countries will be close to each other, and all cities will be close to each other, or all animals will be close to each other, etc.</p>
<p>Also we can expect that sub groups like <strong>water animals</strong>, <strong>land animals</strong>, etc. will be grouped together.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>For a demo, please see the related notebook <code>vector_space_models.ipynb</code>.</p>
</div>
<h2 id="transforming-word-vectors">Transforming Word Vectors<a class="headerlink" href="#transforming-word-vectors" title="Permanent link">&para;</a></h2>
<p>We can make use of vector transformations to build a simple <strong>translation system</strong>.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Let's assume that in the english language, the word <strong>cat</strong> <img alt="🇬🇧" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1ec-1f1e7.svg" title=":gb:" />🐈 is represented by the vector</p>
<div class="arithmatex">\[
\mathbf{v}_{\text{cat}} = [1, 3, 4]
\]</div>
<p>And in the french language, the word <strong>chat</strong> <img alt="🇫🇷" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1eb-1f1f7.svg" title=":fr:" />🐈 is represented by the vector</p>
<div class="arithmatex">\[
\mathbf{v}_{\text{chat}} = [2, -4, -1]
\]</div>
<p>Then we want to find a transformation operation that transforms the english word vector <span class="arithmatex">\(\mathbf{v}_{\text{cat}}\)</span> <img alt="🇬🇧" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1ec-1f1e7.svg" title=":gb:" />🐈 into the french word vector <span class="arithmatex">\(\mathbf{v}_{\text{chat}}\)</span> <img alt="🇫🇷" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1eb-1f1f7.svg" title=":fr:" />🐈.</p>
</div>
<p>The basic idea is that one vector space can be transformed into another vector space using a <strong>rotation matrix</strong>.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>If we have the english word vectors for <strong>cat</strong>, <strong>dog</strong>, and <strong>bird</strong>, and the french word vectors for <strong>chat</strong>, <strong>chien</strong>, and <strong>oiseau</strong>, we can find a rotation matrix that transforms the english word vectors into the french word vectors.</p>
</div>
<p><img alt="Transforming word vectors" src="../../img/vector-space-models-transforming-word-vectors.drawio.svg" /></p>
<p>Mathematically, we want to minimize the distance between the dot product of the two matrices <span class="arithmatex">\(\mathbf{X} \mathbf{R}\)</span> and the matrix <span class="arithmatex">\(\mathbf{Y}\)</span>.</p>
<div class="arithmatex">\[
\mathbf{X} \mathbf{R} \approx \mathbf{Y}
\]</div>
<p>We can find the rotation matrix <span class="arithmatex">\(\mathbf{R}\)</span> by calculating a loss function that measures the difference between the dot product of the two matrices <span class="arithmatex">\(\mathbf{X} \mathbf{R}\)</span> and the matrix <span class="arithmatex">\(\mathbf{Y}\)</span>.</p>
<ol>
<li>Initialize the rotation matrix <span class="arithmatex">\(\mathbf{R}\)</span> with random values.</li>
<li>Calculate the dot product of the two matrices <span class="arithmatex">\(\mathbf{X} \mathbf{R}\)</span>.</li>
<li>Calculate the loss function by comparing the dot product of the two matrices <span class="arithmatex">\(\mathbf{X} \mathbf{R}\)</span> and the matrix <span class="arithmatex">\(\mathbf{Y}\)</span>.</li>
<li>Update the rotation matrix <span class="arithmatex">\(\mathbf{R}\)</span> using gradient descent.</li>
<li>Repeat steps 2-4 until the loss function is minimized.</li>
</ol>
<p>Once we have the rotation matrix <span class="arithmatex">\(\mathbf{R}\)</span>, we can use it to transform the word vectors from one language into another. We will end up somewhere in the vector space of the other language, and then apply a <strong>similarity metric</strong> to find candidates for the translation.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Let's assume we have the following three english word vectors:</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{v}_{\text{cat}} &amp;= [1, 3, 4] \\
\mathbf{v}_{\text{dog}} &amp;= [2, 2, 2] \\
\mathbf{v}_{\text{bird}} &amp;= [3, 1, 0]
\end{align}
\]</div>
<p>And the following three equivalent french word vectors:</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{v}_{\text{chat}} &amp;= [2, -4, -1] \\
\mathbf{v}_{\text{chien}} &amp;= [2, -2, -2] \\
\mathbf{v}_{\text{oiseau}} &amp;= [2, -2, -1]
\end{align}
\]</div>
<p>We can represent the english vectors as a matrix <span class="arithmatex">\(\mathbf{X}\)</span> and the french vectors as matrix <span class="arithmatex">\(\mathbf{Y}\)</span> as follows:</p>
<div class="arithmatex">\[
\mathbf{X} =
\begin{pmatrix}
  1 &amp; 3 &amp; 4 \\
  2 &amp; 2 &amp; 2 \\
  3 &amp; 1 &amp; 0
\end{pmatrix}
\]</div>
<div class="arithmatex">\[
\mathbf{Y} =
\begin{pmatrix}
  2 &amp; -4 &amp; -1 \\
  2 &amp; -2 &amp; -2 \\
  2 &amp; -2 &amp; -1
\end{pmatrix}
\]</div>
<p>Now, we are lookng for a rotation matrix <span class="arithmatex">\(\mathbf{R}\)</span> that transforms the word vectors <span class="arithmatex">\(\mathbf{v}_{\text{cat}}\)</span>, <span class="arithmatex">\(\mathbf{v}_{\text{dog}}\)</span>, and <span class="arithmatex">\(\mathbf{v}_{\text{bird}}\)</span> into the word vectors <span class="arithmatex">\(\mathbf{v}_{\text{chat}}\)</span>, <span class="arithmatex">\(\mathbf{v}_{\text{chien}}\)</span>, and <span class="arithmatex">\(\mathbf{v}_{\text{oiseau}}\)</span> such that:</p>
<div class="arithmatex">\[
\begin{align}
\begin{pmatrix}
  1 &amp; 3 &amp; 4 \\
  2 &amp; 2 &amp; 2 \\
  3 &amp; 1 &amp; 0
\end{pmatrix}
\mathbf{R}
&amp;=
\begin{pmatrix}
  2 &amp; -4 &amp; -1 \\
  2 &amp; -2 &amp; -2 \\
  2 &amp; -2 &amp; -1
\end{pmatrix}
\end{align}
\]</div>
</div>
<p>Here is a NumPy implementation of such a transformation using <code>numpy.dot</code>:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>       <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>       <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>       <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>       <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
</code></pre></div>
<div class="admonition question">
<p class="admonition-title">Question</p>
<p>Why do we want to find a rotation matrix <span class="arithmatex">\(\mathbf{R}\)</span> that transforms the english word vectors into the french word vectors, instead of just using a dictionary to translate the words?</p>
</div>
<!-- Answer: because translation is not always a key-value problem. If there is no direct translation, we can still find a relationship between the words. -->

<h2 id="key-takeaways">Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h2>
<ul>
<li>Vector space models are a way of <strong>representing the meaning of words</strong> in a document. They are a fundamental concept in <abbr title="Natural Language Processing">NLP</abbr>, and are used in many applications such as document classification, information retrieval, and question answering.</li>
<li>Using vector space models, we can capture similarities, differences, dependencies or many other <strong>relationships</strong> between words.</li>
<li>With vector space models, we can capture the <strong>relative meaning</strong> of words by identifying the <strong>context</strong> around each word in the text.</li>
<li>Using the <strong>co-occurrence matrix</strong>, we can extract the word vectors. The vector representation of a word is called a <strong>word embedding</strong>.</li>
<li>In this lecture, we learned two different approaches to create word embeddings: the <strong>word by word design</strong> and the <strong>word by document design</strong>.</li>
<li>When we have word vectors available, we can use a similarity metric like the <strong>Euclidean distance</strong> or the <strong>cosine similarity</strong> to measure the similarity between vectors.</li>
<li>Using simple <strong>vector arithmetics</strong>, we can put word vectors into relation with each other and find interesting relationships between words.</li>
<li><strong>Translation</strong> can be done by finding a <strong>rotation matrix</strong> that transforms the word vectors from one language into another.</li>
<li>Understanding the concept of vector space models is the <strong>basis</strong> for many advanced <abbr title="Natural Language Processing">NLP</abbr> tasks.</li>
</ul>
<!-- markdownlint-disable MD041 -->












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:pascal.keilbach@htwg-konstanz.de" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/pkeilbach/htwg-practical-nlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.tooltips", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.top", "toc.follow"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>